* coordinate v. (使)协调; 搭配 eg. Threads within a block can cooperate by sharing data through some shared memory and by synchronizing their execution to coordinate memory accesses.

* cuda 中同一 block 中的 thread 可以有共享显存（shared memory），这个共享显存通常是 L1 cache。

* cuda 实现矩阵乘法的一个例子

    由 cuda programming guide 里的一个 example 改编而来。

    `main_3.cu`:

    ```cpp
    #include <stdio.h>
    #include <stdlib.h>
    #include <cuda_runtime.h>

    // Matrices are stored in row-major order:
    // M(row, col) = *(M.elements + row * M.width + col)
    typedef struct {
        int width;
        int height;
        float* elements;
    } Matrix;

    #define BLOCK_SIZE 4  // threads per block

    __global__ void MatMulKernel(const Matrix mat_1, const Matrix mat_2, Matrix mat_out);

    void assign_mat_rand_int(float *m, int n_rows, int n_cols)
    {
        for (int i = 0; i < n_rows; ++i)
        {
            for (int j = 0; j < n_cols; ++j)
            {
                m[i * n_cols + j] = rand() % 5;
            }
        }
    }

    void display_mat(float *mat, int n_rows, int n_cols)
    {
        for (int i = 0; i < n_rows; ++i)
        {
            for (int j = 0; j < n_cols; ++j)
            {
                printf("%.1f, ", mat[i * n_cols + j]);
            }
            putchar('\n');
        }
    }

    int main()
    {
        Matrix A, B, C;
        A.width = 8;
        A.height = 8;
        A.elements = (float*) malloc(A.width * A.height * sizeof(float));
        B.width = 8;
        B.height = 8;
        B.elements = (float*) malloc(B.width * B.height * sizeof(float));
        C.width = 8;
        C.height = 8;
        C.elements = (float*) malloc(C.width * C.height * sizeof(float));

        assign_mat_rand_int(A.elements, A.height, A.width);
        assign_mat_rand_int(B.elements, B.height, B.width);

        puts("A:");
        display_mat(A.elements, A.height, A.width);
        puts("B:");
        display_mat(B.elements, B.height, B.width);

        Matrix d_A;
        d_A.width = A.width; d_A.height = A.height;
        size_t size = A.width * A.height * sizeof(float);
        cudaMalloc(&d_A.elements, size);
        cudaMemcpy(d_A.elements, A.elements, size, cudaMemcpyHostToDevice);

        Matrix d_B;
        d_B.width = B.width; d_B.height = B.height;
        size = B.width * B.height * sizeof(float);
        cudaMalloc(&d_B.elements, size);
        cudaMemcpy(d_B.elements, B.elements, size, cudaMemcpyHostToDevice);

        Matrix d_C;
        d_C.width = C.width; d_C.height = C.height;
        size = C.width * C.height * sizeof(float);
        cudaMalloc(&d_C.elements, size);

        dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);
        dim3 dimGrid(B.width / dimBlock.x, A.height / dimBlock.y);
        MatMulKernel<<<dimGrid, dimBlock>>>(d_A, d_B, d_C);

        cudaMemcpy(C.elements, d_C.elements, size,cudaMemcpyDeviceToHost);

        puts("C:");
        display_mat(C.elements, C.height, C.width);

        cudaFree(d_A.elements);
        cudaFree(d_B.elements);
        cudaFree(d_C.elements);
        free(A.elements);
        free(B.elements);
        free(C.elements);

        return 0;
    }

    __global__ void MatMulKernel(Matrix A, Matrix B, Matrix C)
    {
        float Cvalue = 0;
        int row = blockIdx.y * blockDim.y + threadIdx.y;
        int col = blockIdx.x * blockDim.x + threadIdx.x;
        for (int e = 0; e < A.width; ++e)
            Cvalue += A.elements[row * A.width + e] * B.elements[e * B.width + col];
        C.elements[row * C.width + col] = Cvalue;
    }
    ```

    compile: `nvcc -g -G main_3.cu -o main_3`

    run: `./main_3`

    output:

    ```
    A:
    3.0, 1.0, 2.0, 0.0, 3.0, 0.0, 1.0, 2.0, 
    4.0, 1.0, 2.0, 2.0, 0.0, 4.0, 3.0, 1.0, 
    0.0, 1.0, 2.0, 1.0, 1.0, 3.0, 2.0, 4.0, 
    2.0, 0.0, 2.0, 3.0, 2.0, 0.0, 4.0, 2.0, 
    2.0, 3.0, 4.0, 2.0, 3.0, 1.0, 1.0, 2.0, 
    4.0, 3.0, 1.0, 4.0, 4.0, 2.0, 3.0, 4.0, 
    0.0, 0.0, 3.0, 1.0, 1.0, 0.0, 1.0, 3.0, 
    2.0, 0.0, 1.0, 1.0, 0.0, 0.0, 4.0, 2.0, 
    B:
    1.0, 0.0, 1.0, 4.0, 3.0, 2.0, 4.0, 0.0, 
    2.0, 0.0, 4.0, 2.0, 4.0, 4.0, 3.0, 0.0, 
    2.0, 3.0, 1.0, 3.0, 3.0, 4.0, 3.0, 1.0, 
    4.0, 4.0, 2.0, 0.0, 1.0, 3.0, 4.0, 2.0, 
    1.0, 1.0, 4.0, 4.0, 0.0, 0.0, 4.0, 3.0, 
    2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 
    1.0, 0.0, 1.0, 1.0, 0.0, 4.0, 4.0, 4.0, 
    0.0, 4.0, 1.0, 2.0, 2.0, 1.0, 1.0, 0.0, 
    C:
    13.0, 17.0, 24.0, 37.0, 23.0, 24.0, 39.0, 15.0, 
    29.0, 22.0, 26.0, 37.0, 34.0, 47.0, 54.0, 22.0, 
    19.0, 30.0, 24.0, 28.0, 25.0, 33.0, 35.0, 18.0, 
    24.0, 28.0, 24.0, 30.0, 19.0, 39.0, 52.0, 30.0, 
    30.0, 32.0, 39.0, 45.0, 38.0, 46.0, 57.0, 22.0, 
    39.0, 41.0, 52.0, 56.0, 43.0, 56.0, 80.0, 35.0, 
    12.0, 26.0, 13.0, 20.0, 16.0, 22.0, 24.0, 12.0, 
    12.0, 15.0, 11.0, 19.0, 14.0, 29.0, 33.0, 19.0,
    ```

    这个 example 是对 A 矩阵横着切，B 矩阵竖着切，每个 kernel 只计算一行/一列。通过 grid 保证所有的行/列都会被覆盖到。

* cuda shared memory 优化矩阵乘法的一个例子

    `main_4.cu`:

    ```cpp
    #include <stdlib.h>
    #include <stdio.h>
    #include <cuda_runtime.h>

    #define BLOCK_SIZE 4

    typedef struct {
        int width;
        int height;
        int stride;
        float* elements;
    } Matrix;

    __global__ void MatMulKernel(const Matrix, const Matrix, Matrix);

    void assign_mat_rand_int(float *m, int n_rows, int n_cols)
    {
        for (int i = 0; i < n_rows; ++i)
        {
            for (int j = 0; j < n_cols; ++j)
            {
                m[i * n_cols + j] = rand() % 5;
            }
        }
    }

    void display_mat(float *mat, int n_rows, int n_cols)
    {
        for (int i = 0; i < n_rows; ++i)
        {
            for (int j = 0; j < n_cols; ++j)
            {
                printf("%.1f, ", mat[i * n_cols + j]);
            }
            putchar('\n');
        }
    }

    int main()
    {
        Matrix A, B, C;
        A.width = 8;
        A.height = 8;
        A.elements = (float*) malloc(A.width * A.height * sizeof(float));
        B.width = 8;
        B.height = 8;
        B.elements = (float*) malloc(B.width * B.height * sizeof(float));
        C.width = 8;
        C.height = 8;
        C.elements = (float*) malloc(C.width * C.height * sizeof(float));

        assign_mat_rand_int(A.elements, A.height, A.width);
        assign_mat_rand_int(B.elements, B.height, B.width);

        puts("A:");
        display_mat(A.elements, A.height, A.width);
        puts("B:");
        display_mat(B.elements, B.height, B.width);

        Matrix d_A;
        d_A.width = d_A.stride = A.width;
        d_A.height = A.height;
        size_t size = A.width * A.height * sizeof(float);
        cudaMalloc(&d_A.elements, size);
        cudaMemcpy(d_A.elements, A.elements, size, cudaMemcpyHostToDevice);

        Matrix d_B;
        d_B.width = d_B.stride = B.width; d_B.height = B.height;
        size = B.width * B.height * sizeof(float);
        cudaMalloc(&d_B.elements, size);
        cudaMemcpy(d_B.elements, B.elements, size, cudaMemcpyHostToDevice);

        Matrix d_C;
        d_C.width = d_C.stride = C.width;
        d_C.height = C.height;
        size = C.width * C.height * sizeof(float);
        cudaMalloc(&d_C.elements, size);

        dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);
        dim3 dimGrid(B.width / dimBlock.x, A.height / dimBlock.y);
        MatMulKernel<<<dimGrid, dimBlock>>>(d_A, d_B, d_C);

        cudaMemcpy(C.elements, d_C.elements, size, cudaMemcpyDeviceToHost);

        puts("C:");
        display_mat(C.elements, C.height, C.width);

        cudaFree(d_A.elements);
        cudaFree(d_B.elements);
        cudaFree(d_C.elements);
        free(A.elements);
        free(B.elements);
        free(C.elements);

        return 0;
    }

    __device__ float GetElement(const Matrix A, int row, int col)
    {
        return A.elements[row * A.stride + col];
    }

    __device__ void SetElement(Matrix A, int row, int col, float value)
    {
        A.elements[row * A.stride + col] = value;
    }

     __device__ Matrix GetSubMatrix(Matrix A, int row, int col)
    {
        Matrix Asub;
        Asub.width    = BLOCK_SIZE;
        Asub.height   = BLOCK_SIZE;
        Asub.stride   = A.stride;
        Asub.elements = &A.elements[A.stride * BLOCK_SIZE * row + BLOCK_SIZE * col];
        return Asub;
    }

     __global__ void MatMulKernel(Matrix A, Matrix B, Matrix C)
    {
        int blockRow = blockIdx.y;
        int blockCol = blockIdx.x;
        Matrix Csub = GetSubMatrix(C, blockRow, blockCol);
        float Cvalue = 0;
        int row = threadIdx.y;
        int col = threadIdx.x;
        for (int m = 0; m < (A.width / BLOCK_SIZE); ++m)
        {
            Matrix Asub = GetSubMatrix(A, blockRow, m);
            Matrix Bsub = GetSubMatrix(B, m, blockCol);
            __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];
            __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];
            As[row][col] = GetElement(Asub, row, col);
            Bs[row][col] = GetElement(Bsub, row, col);
            __syncthreads();
            for (int e = 0; e < BLOCK_SIZE; ++e)
                Cvalue += As[row][e] * Bs[e][col];
            __syncthreads();
        }
        SetElement(Csub, row, col, Cvalue);
    }
    ```

    compile: `nvcc -g -G main_4.cu -o main_4`

    run: `./main_4`

    output:

    ```
    A:
    3.0, 1.0, 2.0, 0.0, 3.0, 0.0, 1.0, 2.0, 
    4.0, 1.0, 2.0, 2.0, 0.0, 4.0, 3.0, 1.0, 
    0.0, 1.0, 2.0, 1.0, 1.0, 3.0, 2.0, 4.0, 
    2.0, 0.0, 2.0, 3.0, 2.0, 0.0, 4.0, 2.0, 
    2.0, 3.0, 4.0, 2.0, 3.0, 1.0, 1.0, 2.0, 
    4.0, 3.0, 1.0, 4.0, 4.0, 2.0, 3.0, 4.0, 
    0.0, 0.0, 3.0, 1.0, 1.0, 0.0, 1.0, 3.0, 
    2.0, 0.0, 1.0, 1.0, 0.0, 0.0, 4.0, 2.0, 
    B:
    1.0, 0.0, 1.0, 4.0, 3.0, 2.0, 4.0, 0.0, 
    2.0, 0.0, 4.0, 2.0, 4.0, 4.0, 3.0, 0.0, 
    2.0, 3.0, 1.0, 3.0, 3.0, 4.0, 3.0, 1.0, 
    4.0, 4.0, 2.0, 0.0, 1.0, 3.0, 4.0, 2.0, 
    1.0, 1.0, 4.0, 4.0, 0.0, 0.0, 4.0, 3.0, 
    2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 
    1.0, 0.0, 1.0, 1.0, 0.0, 4.0, 4.0, 4.0, 
    0.0, 4.0, 1.0, 2.0, 2.0, 1.0, 1.0, 0.0, 
    C:
    13.0, 17.0, 24.0, 37.0, 23.0, 24.0, 39.0, 15.0, 
    29.0, 22.0, 26.0, 37.0, 34.0, 47.0, 54.0, 22.0, 
    19.0, 30.0, 24.0, 28.0, 25.0, 33.0, 35.0, 18.0, 
    24.0, 28.0, 24.0, 30.0, 19.0, 39.0, 52.0, 30.0, 
    30.0, 32.0, 39.0, 45.0, 38.0, 46.0, 57.0, 22.0, 
    39.0, 41.0, 52.0, 56.0, 43.0, 56.0, 80.0, 35.0, 
    12.0, 26.0, 13.0, 20.0, 16.0, 22.0, 24.0, 12.0, 
    12.0, 15.0, 11.0, 19.0, 14.0, 29.0, 33.0, 19.0,
    ```

    这个例子先按行切第一个矩阵，按列切第二个矩阵，切完后，再将第一个子矩阵竖着切成一小块一小块，每一个小块都是边长为`BLOCK_SIZE`的小正方形。用`m`来标记当前进行到了第几个小正方形。

    `__shared__`关键字来申请 shared memory，用来存放小正方形。`GetElement()`用于往小正方形中填充数据，每个线程填充一个元素。

    `e`用于标记小正方形单行/单列的各个元素序号。因为直接使用的二维坐标来标记线程，所以相当于脱掉了矩阵乘法三层循环的外面两层。

    `__syncthreads();`用于做线程同步，每次等矩阵数据加载完成，或矩阵乘法计算完成后，翥需要同步一下。

* `printf()`使用`"%[-][N]s"`可以指定补全空格

    ```c
    #include <stdio.h>
    #include <stdlib.h>

    int main()
    {
        printf("%8s\n", "hel");
        printf("%-8s\n", "hel");
        printf("%8s\n", "hello, world");
        printf("%-8s\n", "hello, world");
        return 0;
    }
    ```

    output:

    ```
         hel
    hel     
    hello, world
    hello, world
    ```

    `%8s`会在字符串的左侧补全空格，使得空格 + 字符串的总长度为 8 个字符。`%-8s`则会在字符串的右侧补全空格。`-`表示左对齐。

    如果字符串的长度超过 8 个字符，则会忽视对齐要求，按照字符串的实际长度输出。

* `printf()`格式化打印小数

    ```c
    #include <stdio.h>
    #include <stdlib.h>

    int main()
    {
        double val = 12.345;
        printf("%f\n", val);
        printf("%10f\n", val);
        printf("%-10f\n", val);
        printf("%.2f\n", val);
        printf("%10.2f\n", val);
        return 0;
    }
    
    output:

    ```
    12.345000
     12.345000
    12.345000 
    12.35
         12.35
    ```

    * `%f`: 整数部分不删减，小数部分保留 6 位，如果小数位数不够则补 0，如果多于 6 位则按下面的方法舍入：

        从小数部分的第 7 位开始往后截取，设截取的数字为`x`，若`x <= 500...`，则舍去；若`x > 5000...`，则进位。

        比如`0.12345645`，会被转换为`0.123456`;`0.1234565`，会被转换为`0.123456`；`0.12345651`，会被转换为`0.123457`。

        这个也有可能是用二进制数做截断的，目前还不清楚原理。

    * `%10f`: 首先将小数部分保留 6 位，然后判断整数部分 + 小数点 + 小数部分如果小于 10 个字符，则在最左侧补 0，补够 10 个字符。如果大于等于 10 个字符，则按实际的小数输出。

    * `%-10f`: 行为同`%10f`，但是往右侧补空格。

    * `%.2f`: 把小数部分保留两位，整数部分不限制。

    * `%10.2f`: 把小数部分保留 2 位，整体（整数部分 + 小数点 + 小数部分）一共凑够 10 位，如果小数不够 10 位，则在左侧添 0。如果大于等于 10 位，则按实际小数输出。

* `printf()`格式化打印整数

    `%-8d`: 在整数的右侧补空格，直到凑够 8 个字符。如果整数的字符数大于等于 8，则如实输出。

* `printf()`使用`*`替代格式中的常数

    ```c
    #include <stdio.h>
    #include <stdlib.h>

    int main()
    {
        int val = 123;
        for (int i = 0; i < 8; ++i)
            printf("%*d\n", i, val);

        float fval = 12.3;
        for (int i = 4; i < 7; ++i)
        {
            for (int j = 0; j < 4; ++j)
            {
                printf("i = %d, j = %d: %*.*f\n", i, j, i, j, fval);
            }
        }
        return 0;
    }
    ```

    output:

    ```
    123
    123
    123
    123
     123
      123
       123
        123
    i = 4, j = 0:   12
    i = 4, j = 1: 12.3
    i = 4, j = 2: 12.30
    i = 4, j = 3: 12.300
    i = 5, j = 0:    12
    i = 5, j = 1:  12.3
    i = 5, j = 2: 12.30
    i = 5, j = 3: 12.300
    i = 6, j = 0:     12
    i = 6, j = 1:   12.3
    i = 6, j = 2:  12.30
    i = 6, j = 3: 12.300
    ```