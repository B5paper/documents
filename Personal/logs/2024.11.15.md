* nccl 声称实现的通信原语

    * AllReduce

    * Broadcast

    * Reduce

    * AllGather

    * ReduceScatter

    * send/receive

    * scatter, gather

    * all-to-all

* 创建 communicator 时，nccl 会创建 rank，这个 rank 代表一个 device，不代表一个进程

    > Using the same CUDA device multiple times as different ranks of the same NCCL communicator is not supported and may lead to hangs.

* nccl app 中需要调研的点

    * `cudaSetDevice()`

    * `cudaStreamCreate()`

* 如果 vscode 中在编辑 makefile 时，tab 键总是插入 4 个空格而不是 tab，可以在 vscode setting 里把 Detect indentation 选项关了再打开一次就好了

* cuda stream 在创建时，靠`cudaSetDevice()`来指定具体的 device

    这个操作看起来比较像 opengl，提前指定好上下文环境。
