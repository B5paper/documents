* routine 不可能用 cache task 存在，必须以 routine task 的方式存在

    假如 routine 以 cache task 的方式存在，那么 routine 的所有代码都将写到 cache entry 里，但是这是不可能的，因为 routine 主打的是庞大、复杂、断点续传，cache entry 主打的是小巧、灵活、专一、多组合。将未完成的的 routine 放到 cache entry 里，显得格格不入。

* 矩阵论

    矩阵论可以理解为线性代数的深化和扩展。如果说本科的线性代数主要研究有限维向量空间和线性映射的基本性质（如行列式、矩阵运算、特征值、二次型等），那么矩阵论则在此基础上，向更深、更广、更实用的方向拓展。

    核心内容:

    * 矩阵分解：这是矩阵论的核心内容。将复杂的矩阵分解成几个结构简单、性质清晰的矩阵的乘积，以便于分析和计算。

        LU分解：用于求解线性方程组。

        QR分解：用于求解最小二乘问题和特征值计算。

        特征值分解：将方阵对角化，用于分析系统的稳定性和振动模式。

        奇异值分解（SVD）：极其重要，适用于任意矩阵。是数据降维（如PCA）、推荐系统、图像压缩、自然语言处理等领域的数学基础。

    * 矩阵范数：衡量矩阵的“大小”，类似于向量的模。用于分析线性方程组的误差稳定性、数值计算的收敛性等。

    * 矩阵函数：如何定义和计算矩阵的函数，如 $e^A$（矩阵指数，用于求解微分方程组），$\sin (A)$，$\sqrt(A)$ 等。

    * 广义逆矩阵（Moore-Penrose伪逆）：对于非方阵或奇异矩阵，定义其“逆”，用于求解无解或有多解的矛盾线性方程组。是最小二乘法的理论基础。

    * 特殊矩阵类：深入研究具有特殊结构的矩阵，如对称矩阵、Hermite矩阵、正定矩阵、酉矩阵、Toeplitz矩阵等，它们具有更好的性质和更高效的计算方法。

    * 矩阵的扰动理论：研究当矩阵元素发生微小变化时，其特征值、特征向量等性质如何变化，对数值分析至关重要。

    * 非负矩阵：元素均为非负的矩阵，在经济学、概率论（马尔可夫链）和网络科学中有广泛应用。

    应用领域：

    * 数值计算：是各种计算算法的核心。

    * 优化理论：最小二乘、二次规划等。

    * 信号与图像处理：滤波、压缩、去噪。

    * 机器学习与数据科学：主成分分析（PCA）、线性判别分析（LDA）、推荐系统、神经网络（本质是层层的矩阵运算）。

    * 量子力学：算符可以用矩阵表示。

    * 网络科学：图的邻接矩阵、拉普拉斯矩阵。

    教材：

    * 《矩阵论》- 程云鹏，张凯院等：国内经典教材，内容全面，适合工科研究生。

    * 《Matrix Analysis》- Roger A. Horn, Charles R. Johnson：矩阵领域的“圣经”，理论深度很高，适合数学基础好的读者。

    * 《Linear Algebra and Its Applications》- Gilbert Strang：MIT经典教材，直观易懂，特别注重矩阵的应用和几何解释，非常适合入门和建立直觉。他的公开课在MIT OCW上可以找到。

    在线课程：

    * MIT OpenCourseWare - 18.065 Linear Algebra and Learning from Data：Gilbert Strang教授的新课，紧密结合了矩阵论和机器学习。

    * 3Blue1Brown的“线性代数的本质”系列视频：用极其出色的动画直观解释线性代数的核心概念，必看。

    Python中的相关库与函数:

    * NumPy

        * 创建矩阵

            ```py
            import numpy as np
            A = np.array([[1, 2], [3, 4]]) # 从列表创建
            B = np.mat('1 2; 3 4')         # 从字符串创建矩阵（更符合数学习惯）
            I = np.eye(3)                  # 3x3 单位矩阵
            Z = np.zeros((2, 2))           # 2x2 零矩阵
            ```

        * 基础运算

            ```py
            A + B    # 加法
            A - B    # 减法
            A * B    # **注意：这是逐元素相乘，不是矩阵乘法！**
            A @ B    # Python3.5+ 的矩阵乘法运算符
            np.dot(A, B) # 矩阵乘法
            np.linalg.inv(A) # 矩阵求逆（如果可逆）
            A.T      # 矩阵转置
            ```

        * 分解与高级运算

            ```py
            # 特征值分解
            eigenvalues, eigenvectors = np.linalg.eig(A)

            # 奇异值分解（SVD）
            U, S, Vt = np.linalg.svd(A)

            # QR分解
            Q, R = np.linalg.qr(A)

            # 计算范数
            norm_l2 = np.linalg.norm(A, ord=2) # 2-范数
            norm_fro = np.linalg.norm(A, 'fro') # Frobenius范数

            # 求解线性方程组 Ax = b
            x = np.linalg.solve(A, b)

            # 计算伪逆（广义逆）
            A_pinv = np.linalg.pinv(A)
            ```

    * SciPy

        * 更特殊的分解

            ```py
            import scipy.linalg

            # Schur分解
            T, Z = scipy.linalg.schur(A)

            # LU分解（带 pivoting）
            P, L, U = scipy.linalg.lu(A)

            # 矩阵函数
            exp_A = scipy.linalg.expm(A)   # 矩阵指数 e^A
            sin_A = scipy.linalg.sinm(A)   # 矩阵正弦
            ```

    * 其他相关库

        `Scikit-learn`：机器学习库，内部大量使用SVD、QR等矩阵分解进行建模（如decomposition.PCA, decomposition.TruncatedSVD）。

* 谱分析

    谱分析是矩阵论从“静态”描述走向“动态”分析的关键桥梁，它研究的核心是矩阵的特征值集合（称为“谱”）及其相关的特征向量如何决定和揭示矩阵（以及它所代表的线性算子或系统）的深层性质和行为。

    核心概念:

    * 谱：一个矩阵的所有特征值构成的集合，记作 $\sigma(A)$。

    * 谱半径：谱中特征值绝对值的最大值，记作 $\rho(A) = \max \{ \lvert \lambda \rvert : \lambda \in \sigma(A) \}$。它决定了与矩阵相关的迭代过程（如求解线性方程组）的收敛速度。

    * 谱定理：这是谱分析的基石。它指出，厄米特矩阵（在实数域中即对称矩阵 $A^T = A$）和酉矩阵（在实数域中即正交矩阵 $A^T A=I$）都可以被单位正交基对角化。这意味着：

        * 它们有完整的正交特征向量集。

        * 矩阵的 action 可以分解为在相互垂直的特征方向上进行简单的伸缩变换。

    **谱分析的意义与应用**:

    * 系统稳定性分析：

        * 在线性微分方程组 $\frac{dx}{dt} = Ax$ 中，系统的长期行为完全由 $A$ 的谱决定。

        * 如果所有特征值的实部都小于零，系统是稳定的（解会衰减到零）。

        * 如果存在特征值的实部大于零，系统是不稳定的（解会指数增长）。

        * 示例：在结构力学中，特征值代表结构的固有振动频率，特征向量代表相应的振型。

    * 矩阵函数的计算：

        * 如果矩阵 $A$ 可对角化，即 $A = PDP^{−1}$，其中 $D$ 是对角矩阵（对角线为特征值），那么矩阵函数（如指数函数 $e^A$）的计算变得非常简单：$f(A) = P f(D) P^{−1}$。

        * 示例：$e^A = P e^D P^{−1}$，而 $e^D$ 就是对角线上每个元素取指数，这极大地简化了计算。

    * 数据降维与主成分分析（PCA）：

        * PCA 的本质就是数据的协方差矩阵的谱分解。

        * 协方差矩阵是实对称矩阵，其特征向量（主成分）指向数据方差最大的方向，对应的特征值表示该方向上的方差大小。

        * 通过保留对应最大特征值的几个特征向量，就能实现数据降维，同时保留最主要的信息。

    * 图论与网络分析：

        * 图的拉普拉斯矩阵的谱（谱图理论）揭示了图的许多重要性质，如连通性、分割方式、 robustness 等。

        * 例如，拉普拉斯矩阵的第二小特征值（称为代数连通度）的大小反映了图的连通强度。

    * 量子力学：

        * 系统的哈密顿算符对应的矩阵的特征值代表系统可能存在的能级。

    **代码**：

    谱分析的核心工具就是特征值分解和奇异值分解（SVD）。SVD 可以看作是任意矩阵的谱分析的推广。

    * numpy

        ```py
        import numpy as np
        import matplotlib.pyplot as plt

        # 生成一个对称矩阵（其谱是实数）
        A = np.random.randn(5, 5)
        A = A + A.T # 使其对称

        # 1. 特征值分解 (Spectural Decomposition)
        eigenvalues, eigenvectors = np.linalg.eig(A)
        # eigenvalues 就是谱
        # eigenvectors 的每一列是对应的特征向量

        print("谱（特征值）:", eigenvalues)
        print("谱半径:", np.max(np.abs(eigenvalues)))

        # 验证谱定理：对于对称矩阵，特征向量是正交的
        print("特征向量矩阵是否正交（近似单位矩阵）:")
        print(eigenvectors.T @ eigenvectors) # 应近似于单位矩阵

        # 2. 奇异值分解 (SVD) - 适用于任何矩阵
        B = np.random.randn(4, 6)
        U, S, Vt = np.linalg.svd(B)
        # S 是奇异值向量（非负），奇异值的平方就是 B^T B 或 B B^T 的特征值。
        print("奇异值谱:", S)
        ```

    * SciPy

        ```py
        from scipy.linalg import eigh, schur

        # eigh 是专门用于厄米特/对称矩阵的，更高效且能确保特征值按序返回
        eigvals_symmetric, eigvecs_symmetric = eigh(A)
        print("按升序排列的特征值:", eigvals_symmetric)

        # Schur 分解可以得到（拟）三角矩阵，对角线上的元素就是特征值，数值上比直接求特征值更稳定。
        T, Z = schur(A)
        print("Schur 形式矩阵 T（上三角），对角线是特征值:")
        print(np.diag(T))
        ```

    谱分析是矩阵论的精华所在，它将抽象的矩阵运算转化为对特征值（谱）和特征方向（特征向量）的直观理解。通过“谱”这个透镜，我们可以洞察线性系统的稳定性、数据的内在结构、网络的拓扑特性等深远问题。它无疑是连接矩阵理论与众多科学工程应用的核心桥梁。

* `/proc/iomem`

    /proc/iomem 是 Linux 内核提供的一个只读的虚拟文件，它直观地展示了整个系统物理内存地址空间的映射和分配情况。

    它不仅仅显示普通的 RAM（系统内存）, 还显示了其他所有被映射到物理地址空间的设备，例如：

    * 显卡的显存 (Video RAM)

    * BIOS 的 ROM

    * PCI/PCIe 设备的内存映射寄存器 (MMIO) - 这是非常重要的一部分，操作系统通过读写这些地址来与硬件（如网卡、显卡、硬盘控制器）通信。

    * 系统保留的内存区域。

    每一行代表一个地址范围，格式通常为：起始地址-结束地址 : 资源描述

    /proc/iomem 描述的就是 CPU 的总线地址空间（或叫物理地址空间）的映射图。，总线上面可以挂多个设备，包含 dram, pcie 等，每个设备又分别占用总线地址空间的一小段范围

    linux kernel 中的 va 指的并不是这里的总线地址。

* 对于cpu的dma，cpu内部的内存控制器里有dma引擎，dma引擎设定一个计数器，要求dram把指定范围内的数据发送到总线上，如果目的地是网卡，那么网卡需要配合着读总线上的数据，并发送ack使计数器加一，直到数据全部传输完成。对于gpu和rdma网卡，则是网卡内部有dma引擎，向pcie发起read/write事务，读取或写入gpu显存的数据，gpu内则有专门的硬件单元来响应这个事务
