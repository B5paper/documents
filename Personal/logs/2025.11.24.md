* `read`, `read_iter`, `splice_read`

    它们都是 VFS（虚拟文件系统）层定义的文件操作函数指针，由具体的文件系统（如 ext4）或设备驱动来实现，用于从文件或设备中读取数据。

    1. read (传统读取)

        作用：这是最经典、最通用的文件读取方法。它的任务是将数据从内核空间的文件（或设备）缓冲区拷贝到用户空间提供的缓冲区中。

        数据流：文件/设备 -> 内核缓冲区 -> 用户缓冲区

        特点：

            涉及两次数据拷贝：一次从文件到内核页缓存（如果使用了缓存），另一次从内核页缓存到用户缓冲区。

            通用性强，几乎所有文件类型都支持。

            对于大量数据的读写，这种拷贝会成为性能瓶颈。

        简单来说：read 是标准的“读取-拷贝”操作，稳定但效率不是最高。

    2. read_iter (迭代读取)

        作用：read 操作的进化版。它使用 struct iov_iter 来描述一个分散/聚集的缓冲区列表，可以一次性将数据读取到多个非连续的用户空间缓冲区，或者从一个非连续的内核缓冲区中读取数据。

        数据流：与 read 类似，也是拷贝，但源和目的可以是多个不连续的块。

        特点：

            支持向量化I/O（如 readv 系统调用），更灵活。

            仍然是数据拷贝的方式。

            在现代内核中，read 系统调用在底层通常会被转换成 read_iter 操作。可以认为 read_iter 是新的、更通用的读取接口。

        简单来说：read_iter 是更现代、更灵活的“读取-拷贝”操作，是 read 的替代者。

    3. splice_read (零拷贝读取)

        作用：实现“零拷贝”读取的高级操作。它的目标是将数据从一个文件描述符（比如一个真实文件）直接移动到到一个管道 中，而不需要经过用户空间。数据始终在内核空间内流动。

        数据流：文件/设备 -> 管道缓冲区 -> 另一个文件描述符（如socket）

        特点：

            零拷贝：避免了数据在内核和用户空间之间的昂贵拷贝，极大提升了性能。

            专用场景：通常用于将数据从文件“输送”到网络套接字（例如，一个文件下载服务器）。

            依赖管道：必须使用管道作为数据的中间载体。

            不是所有文件系统都支持。

        简单来说：splice_read 是高效的“数据搬运工”，它在内核里直接把数据从一个地方“倒”到另一个地方，省去了拷贝到用户层的步骤。

    总结与类比
    操作	数据流向	关键特点	适用场景
    read	内核 -> 用户空间	传统，两次数据拷贝	通用文件读取
    read_iter	内核 -> 用户空间	现代，支持分散/聚集IO	高效、灵活的文件读取（readv）
    splice_read	文件 -> 管道	零拷贝，高性能	高性能网络服务（如文件发送）

* 混淆矩阵（Confusion Matrix）

    有时也被称为 Error Matrix（错误矩阵），它是一个2x2的表格，总结了分类模型对二分类问题的预测结果。


    | | 实际为正例 | 实际为负例 |
    | - | :-: | :-: |
    | 预测为正例 | TP (True Positive) | FP (False Positive) |
    | 预测为负例 | FN (False Negative) | TN (True Negative) |

    * TP（真阳性）：模型预测为正，实际也是正。预测正确。

    * FP（假阳性）：模型预测为正，但实际是负。误报。

    * FN（假阴性）：模型预测为负，但实际是正。漏报。

    * TN（真阴性）：模型预测为负，实际也是负。预测正确。

* Accuracy（准确率）, Precision（精确率/查准率）, Recall（召回率/查全率）

    * accuracy

        含义：所有预测结果中，预测正确的比例。

        公式：

        `Accuracy = (TP + TN) / (TP + TN + FP + FN)`

        意义：衡量模型整体的正确率。它是一个非常直观的指标。

        优缺点：

        * 优点：容易理解。

        * 缺点：在数据不平衡的数据集上，准确率会严重失真。

            例子：在一个有1000个样本的数据集中，有990个负样本（0），只有10个正样本（1）。如果一个模型简单地将所有样本都预测为负，那么它的准确率是 (0 + 990) / 1000 = 99%。虽然准确率很高，但这个模型完全没有识别正例的能力，是一个无用的模型。

    * precision

        含义：在所有被模型预测为正例的样本中，真正的正例有多少。

        公式：

        `Precision = TP / (TP + FP)`

        意义：衡量模型的“精准度”或“宁缺毋滥”的程度。它关注的是预测结果。

        核心问题：当模型说某个东西是“正例”时，它有多可信？

        应用场景：注重减少误报（FP）的场景。

        * 垃圾邮件检测：我们非常不希望把正常邮件误判为垃圾邮件（FP）。宁可放过一些垃圾邮件（FN），也不能误杀正常邮件。因此，我们需要高精确率。

        * 推荐系统：给用户推送的内容，希望尽量都是他感兴趣的。如果推送了不感兴趣的内容（FP），会影响用户体验。

    * recall

        含义：在所有实际为正例的样本中，模型成功预测出来的有多少。

        公式：

        `Recall = TP / (TP + FN)`

        意义：衡量模型的“覆盖率”或“宁错杀不漏放”的程度。它关注的是真实情况。

        核心问题：在所有真正的正例中，模型找出了多少？

        应用场景：注重减少漏报（FN）的场景。

        * 疾病检测：我们非常不希望把一个患病的人误判为健康（FN）。宁可让一些健康的人做进一步检查（FP），也不能漏掉一个病人。因此，我们需要高召回率。

        * 逃犯识别：在安检系统中，绝对不能漏掉一个逃犯（FN）。即使需要误警一些普通人（FP）进行二次检查，也要确保高召回率。

    * Precision和Recall的“跷跷板”关系

        在大多数情况下，精确率（Precision）和召回率（Recall）是相互矛盾的。提高一个，通常会导致另一个的降低。

        * 如果你想提高Precision（减少FP）：

            你需要提高预测正例的门槛。例如，只有模型有99%的把握时才预测为正。这样，被预测为正的样本确实很可能是正的（Precision高），但很多“没那么确定”的正例会被判为负例，从而导致漏报增加（FN增加），Recall降低。

        * 如果你想提高Recall（减少FN）：

            你需要降低预测正例的门槛。例如，只要模型有50%的把握就预测为正。这样，你能抓住几乎所有的正例（Recall高），但也会混入很多其实是负例的样本，导致误报增加（FP增加），Precision降低。

    * 与Accuracy的关系

        Accuracy提供了一个宏观的、整体的性能视图。

        Precision和Recall提供了更细粒度的、针对特定类别（正例）的性能视图。

        在数据平衡且FP和FN的成本相似的问题中，Accuracy是一个不错的指标。

        在数据不平衡或FP与FN的成本明显不同的问题中，必须结合Precision和Recall（以及F1-Score）来分析。

* F1-Score：调和平均数

    为了同时考虑Precision和Recall，我们引入了 F1-Score。

    公式：

    `F1-Score = 2 * (Precision * Recall) / (Precision + Recall)`

    意义：F1-Score是Precision和Recall的调和平均数。它只有在Precision和Recall都较高时才会高。因此，它是一个综合性的指标，特别适用于不平衡数据集的评价。

    * 为什么取调和平均数，而不是代数平均数，或者几何平均数？

        因为调和平均数对较低值施加了更严厉的惩罚。

        三种平均数：

        假设我们有 Precision (P) 和 Recall (R) 两个值。

        * 算术平均数：(P + R) / 2

            特点：对所有值一视同仁，是普通的“平均值”。

        * 几何平均数：sqrt(P * R)

            特点：受极端值影响较小，更适合衡量比例或增长率。

        * 调和平均数：2 * P * R / (P + R)

            特点：强烈惩罚不平衡的数值。当P和R中有一个非常低时，调和平均数会接近这个低值。

        我们希望一个模型在Precision和Recall上都表现良好，而不是用其中一个的高分来“掩盖”另一个的低分。

        example:

        场景：我们有一个疾病检测模型。

            模型A： Precision = 1.0， Recall = 0.1

                它预测有病的人，100%确实有病（非常准，绝不误诊）。

                但实际有病的人，它只找出了10%（漏掉了90%的病人，非常危险）。

            模型B： Precision = 0.5， Recall = 0.5

                它预测有病的人，一半确实有病。

                实际有病的人，它找出了一半。

        问题：哪个模型更好？

        计算它们的平均数：

        | 模型 | A | B |
        | - | - | - |
        |　Precision (P) | 1.0 | 0.5 |
        | Recall (R) | 0.1 | 0.5 |
        | 算术平均 | (1.0 + 0.1) / 2 = 0.55 | (0.5+0.5)/2 = 0.50 |
        | 几何平均 sqrt(1.0 * 0.1) ≈ 0.32 | sqrt(0.5 * 0.5) = 0.50 |
        | 调和平均 | (F1) 2(1.0 * 0.1)/(1.0+0.1) ≈ 0.18 | 2(0.5* 0.5)/(0.5+0.5) = 0.50 |

        分析结果：

        * 从算术平均数看：模型A (0.55) > 模型B (0.50)。这显然是不合理的。模型A是一个“懒惰”的模型，它为了保持100%的准确率，只敢对极少数非常有把握的病例做出阳性预测，导致大量病人被漏诊。在医学上，这是一个灾难性的模型。然而，算术平均数却被它极高的Precision所“欺骗”，给出了更高的分数。

        * 从几何平均数看：模型A (0.32) < 模型B (0.50)。这个结果已经比算术平均数合理了，它识别出了模型A的不平衡性。

        * 从调和平均数 (F1-Score) 看：模型A (0.18) << 模型B (0.50)。调和平均数对模型A的“偏科”行为施加了最严厉的惩罚，给出了一个极低的分数，清晰地表明模型B的综合性能远优于模型A。

        结论与总结:

        平均数类型	对不平衡的惩罚力度	在评估模型中的适用性
        算术平均	最弱	不适用。容易被一个高指标和另一个低指标的模型所误导。
        几何平均	中等	比算术平均好，在某些场景下（如Fβ-Score的变体）有应用。
        调和平均 (F1)	最强	最常用。能有效惩罚“偏科”的模型，确保模型在P和R之间取得有意义的平衡。

    F1-Score特别适合于类别不平衡的数据集，以及那些没有明确倾向是更需要Precision还是Recall的场景，它提供了一个稳健的、单一的综合性评估指标。

    当你有明确倾向时，可以使用Fβ-Score。

    Fβ = (1 + β²) * (Precision * Recall) / (β² * Precision + Recall)

    当β=1时，就是F1。

    当β>1时，Recall的权重更高（更看重查全）。

    当β<1时，Precision的权重更高（更看重查准）。

* 实盘经验

    * 股票的申购日并不是上市时间，没必须要申购日前一周埋伏，在申购日前后埋伏也不迟

    * 开盘是否要买入，可以参考 15 分钟线 / 30 分钟线的 DMI，如果没有重大事件，通常不会突然变化

* torchmetrics

    `acc.update(pred, gt)`

    在`.update()`函数中，第一个参数必须是 pred，第二个参数必须是 gt。

    pred 和 gt 必须是 torch 的 tensor 类型，不能是 numpy 的 ndarray。

    如果 pred 是一维的，那么其编码方式为标签编码，即预测的类别的索引，而不是概率。
    
    如果 pred 是二维的，那么 pred 的类型必须是 float，不能是 int，其代表的含义为输出的概率。问题：是否需要经过 softmax？问题：如果使用 max() 取概率最大值，那么 threshold = 0.5 有什么意义？

* precision 的三种模式 micro, macro 与 none

    * `'micro'`： 全局视角。先汇总所有类别（或所有样本）的 TP, FP, FN，再用汇总后的总数计算一个全局指标。

    * `'macro'`： 平均视角。先独立计算每个类别的指标，然后对所有类别的指标值求算术平均。

    * `'none'`: 计算每个类别的 precision，不进行平均

    example:

    ```py
    from torchmetrics import Precision

    pred = t.tensor([0, 1, 2, 3])
    gt = t.tensor([0, 1, 0, 0])

    pre = Precision('multiclass', num_classes=10, average='micro')
    pre.update(pred, gt)
    pre_score = pre.compute()
    print('micro pre: {}'.format(pre_score))

    pre = Precision('multiclass', num_classes=10, average='macro')
    pre.update(pred, gt)
    pre_score = pre.compute()
    print('macro pre: {}'.format(pre_score))

    pre = Precision('multiclass', num_classes=10, average='none')
    pre.update(pred, gt)
    pre_score = pre.compute()
    print('none pre: {}'.format(pre_score))
    ``` 

    output:

    ```
    micro pre: 0.5
    macro pre: 0.5
    none pre: tensor([1., 1., 0., 0., 0., 0., 0., 0., 0., 0.])
    ```

    * `'micro'`模式详解

        假设我们有一个多类分类问题，有 C 个类别。

        1. 逐类统计：

            对于每个类别 i，计算其真正例（TP_i）和假正例（FP_i）。

            * 真正例（TP_i）： 真实标签为 i 且被预测为 i 的样本数。

            * 假正例（FP_i）： 真实标签不是 i 但被预测为 i 的样本数。

        2. 全局汇总：

            * 计算所有类别的 TP 之和： total_TP = TP_1 + TP_2 + ... + TP_C

            * 计算所有类别的 FP 之和： total_FP = FP_1 + FP_2 + ... + FP_C

        3. 计算 Micro Precision：

            使用汇总后的 total_TP 和 total_FP 来计算 Precision，公式和标准的二分类 Precision 一模一样。

            $\mathrm{Precision_{micro}} = \frac{\mathrm{total\_TP}}{\mathrm{total\_TP + total\_FP}}$

        重要特性与注意事项

        * 与 Accuracy 的关系： 在多类分类中，Micro Precision 的值等于 准确率（Accuracy）。这是因为：

            * total_TP 就是所有被正确分类的样本总数。

            * total_TP + total_FP 就是所有被预测为正例的样本总数，在多类分类中，这等于总样本数（因为每个样本必须被分到一个类别）。

            * 所以，Micro Precision = total_TP / N = Accuracy。

        * 样本不平衡： Micro 平均对每个样本“一视同仁”，因此它更适合样本不平衡的数据集，因为大类的性能会主导最终结果。如果你关心小类的性能，应该使用 'macro' 平均。

        * 多标签任务： 在多标签任务中（一个样本可以有多个标签），Micro Precision 的计算逻辑完全相同（汇总所有标签的 TP 和 FP），但此时它不等于 Accuracy，因为一个样本可以有多个预测和多个真实标签。

    * `'micro'`与`'macro'`模式的对比

        * 对类别不平衡的敏感性

            * `micro`

                不敏感（默认偏向大类）
                大类的性能主导了最终结果。因为大类的 TP/FP 数量远多于小类，在汇总时贡献最大。在我们的例子中，Micro Precision (0.786) 更接近大类 A 的 Precision (0.947)。

            * `macro`

                敏感（平等对待每个类）
                将所有类别视为同等重要，无论其样本多少。小类的差劲性能会直接拉低平均值。在我们的例子中，Macro Precision (0.831) 被小类 C 的 Precision (0.714) 拉低了。
        * 优点

            * micro

                1. 综合性能： 很好地衡量了模型在整体数据集上的性能。
                2. 等于Accuracy： 在多类分类中，Micro-Precision/Recall/F1 等于准确率，易于理解。
                3. 适用于样本不平衡但关心整体性能的场景。

            * macro

                1. 公平性： 给予所有类别同等权重，能揭示模型在小类上的短板。
                2. 稳定性： 不受类别分布影响，适合比较不同数据集或不同采样策略下的模型。
                3. 适用于需要关注小类的场景（如医疗诊断、故障检测）。

        * 缺点

            * micro

                1. 掩盖小类问题： 如果模型完全忽略小类，但只要大类表现好，Micro指标依然会很高，从而误导你认为模型很好。
                2. 对数据分布敏感： 结果严重依赖于数据集的类别分布。

            * macro

                1. 可能低估性能： 如果一个模型在大类上表现极好，但在一个样本极少的小类上表现稍差，Macro指标可能会给出一个相对较低的评价，这可能不完全符合业务直觉。
                2. 对噪声敏感： 一个在某个小类上的极端差值（如 Precision=0）会严重拉低整体平均值。

* 如何选择 micro 与 macro

    选择哪种平均方式完全取决于你的业务目标和数据集特点。

    选择 'micro' 当：

        你关心模型的整体性能，并且每个样本的错误代价是相同的。

        数据存在不平衡，但大类的性能更重要。例如，在电商产品分类中，热销商品的准确率远比冷门商品重要。

        你希望得到一个单一的、概括性的性能指标，并且这个指标与准确率等价。

    选择 'macro' 当：

        所有类别都同等重要，无论其样本数量多少。

        你特别关心模型在小类/稀有类上的表现。这在很多关键领域至关重要：

            医疗： 诊断一个稀有病。

            金融风控： 检测极少数但危害巨大的欺诈交易。

            工业： 预测罕见的设备故障。

        你的数据集类别相对平衡。

        你想评估模型的稳健性和泛化能力，看它是否在所有类别上都“学得不错”。


    最佳实践

    不要只看一个数字！ 一个负责任的实践是：

        同时报告 Micro 和 Macro 值，以提供更全面的视图。

        查看每个类别的单独指标（即不平均），这能最直接地发现问题所在。

        分析混淆矩阵，直观地看到哪些类别被混淆了。

* 为什么 macro 不使用调和平均值？

    F1-score 已经是考虑过类别平衡的数据了，直接对 F1-socre 使用 macro 就可以。F1-socre 对 Precision 和 Recall 使用调和平均是因为 Precision 和 Recall 是同一类别不同维度的指标。而 macro 是对同一个维度的指标进行调和平均，没有必要。

    对已经平衡过的指标（F1）再进行一次平衡，这可能导致过度惩罚。

    如果我们考虑到不同类别的平衡，可以使用

    1. 加权F1（Weighted-F1）

        `Weighted-F1 = Σ(weight_i × F1_i)`

        其中 weight_i 通常是该类别的样本比例

    2. 几何平均（Geometric Mean）

        对极端值比算术平均更敏感，但比调和平均温和：

        `G-Mean = (F1_1 × F1_2 × ... × F1_N)^(1/N)`

    3. 使用专门的不平衡学习指标

        如 G-Mean（几何平均）或 Balanced Accuracy。

    如果特别关注最差类别，考虑报告 最小F1（Min-F1）