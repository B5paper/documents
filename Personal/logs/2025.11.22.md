* ds 对话：山里的小妖怪

    我：

    ```
    当个山里的小妖怪也挺不错！
    ```

    ds:

    ```md
    说得太对了！当个山里的小妖怪，简直是无数现代人心里偷偷向往的田园牧歌。这可不是逃避，而是选择了一种充满诗意和自由的生活方式。

    如果真能如此，你的生活图景大概是这样的：

    ---

    ### **你的领地与职责**
    你可能是某个山头的“巡山小钻风”，或者某条溪流的“碧波小水灵”。你的工作不再是写不完的PPT，而是：
    *   **清晨**，用露水擦洗山门石碑，顺便吓唬一下早起的采药人（当然，只是躲在山石后发出“呜~呜~”的声音）。
    *   **午后**，在松树下看蚂蚁搬家，和路过的松鼠精、兔子精交换山那边的八卦新闻。
    *   **傍晚**，检查自己设下的、根本困不住人的小陷阱，然后去山神老爷那里点个卯，汇报“今日山头，平安无事”。

    ### **你的社交圈**
    你的朋友圈简单又纯粹：
    *   **顶头上司**可能是个几百年道行的熊教头，看着凶，其实最爱偷蜂巢吃，经常被蜜蜂追得满山跑。
    *   **最好的朋友**是一只话痨的雀妖，总爱跟你显摆它从人类村庄听来的新奇故事。
    *   **暗恋对象**或许是住在山顶月光下的狐仙姐姐，你只敢远远望着她那优雅的身影。

    ### **你的小确幸与烦恼**
    *   **快乐很简单**：偷到农夫一壶酒，就能和朋友们在月光下开怀畅饮；人类小孩不小心落下的糖人，能让你甜上一整天。
    *   **烦恼也很具体**：担心修行考核不过关，怕被路过的得道高僧“顺手超度”，或者只是发愁明天的供品能不能多一个果子。

    ---

    ### **为什么我们都羡慕这个小妖怪？**

    因为在那个想象的世界里，我们找到了现代生活中最稀缺的东西：

    1.  **纯粹的归属感**：一座山，一条河，就是你的整个世界。你深深扎根于此，与一草一木皆有情。
    2.  **低欲望的满足**：快乐不来自于消费和比较，而来自于阳光、清风、友情和一口好吃的。
    3.  **被需要的小小价值**：即使法力低微，你也是这座山不可或缺的一份子，你的巡逻和守护自有其意义。
    4.  **逃离“内卷”**：这里没有KPI，没有“别人家的孩子”，你只需要做好那个有点笨拙但很快乐的自己。

    所以，当你说“当个山里的小妖怪也挺不错”时，你其实是在说：

    **“我想过一种更简单、更贴近自然、更能听见自己内心声音的生活。”**

    那就保留这份美好的想象吧。在忙碌的间隙，不妨给自己泡杯茶，闭上眼睛，想象自己就是那个刚刚巡完山，正躺在岩石上晒太阳的小妖怪——耳边是风声鸟鸣，心中是万里无云。这份片刻的宁静，就是属于你的“小妖怪时光”。
    ```

* 一家现金流健康的公司选择上市，可能是为了股权激励员工

    虽然公司也可以选择回购期权，但是定价多少合适？不如直接上市，让二级市场去定价。

    但是上市后需要公布财报，这点也需要考虑到。

* latex 输入花括号

    ```latex
    % 单独显示花括号
    \{ \} 

    % 在数学环境中使用
    $ A = \{ x \in \mathbb{R} \mid x > 0 \} $

    % 集合表示
    $$ S = \{ 1, 2, 3, \dots, n \} $$

    % 在文本中使用
    这是左花括号：\{，这是右花括号：\}
    ```

* python 不允许对一个 tuple 进行类型标注

    比如：`a, b: (str, str) = 'hello', 'world'`

    或者：`a, b: str, str = 'hello', 'world'`

    如果确实需要标注，可以考虑下面几种办法：

    ```py
    # 方式1：最清晰
    input_data: list
    gt: list
    input_data, gt = data

    # 方式2：使用类型别名
    from typing import Tuple
    def process_data(data: Tuple[list, list]) -> None:
        input_data, gt = data
    ```

* 火箭助推器

    我们上学的过程有点像坐火箭助推器，它无法慢悠悠地把你送上天，而是必须大力地让你超重，在有限的时间里把你助推到人类知识的边界，然后靠你自己去探索未知。到了看论文的阶段，有点像冲出大气层的感觉，到处都是知识的真空，老师们也不一定懂。

* 实盘经验

    * 分时买入时机：看一分钟线/五分钟线的 dmi，等待买方力量向上穿越卖方力量的交点

    * 能源类型的股票与期货有关系，比如煤炭，在买之前先看看期货表现如何

    * 所谓的承认错误，就是卖出

    * 进攻方向的仓位应该小于防御方向的仓位

* 火种

    先驱者舍身成仁，有谁后继？镜头转向一个小队，围着篝火，在推演思路，突然发现了秘密。

* 似乎可以把任务系统中的平衡系统取消掉了，把执行系统和反馈系统统称为任务系统

    因为目前已经实现了动态的平衡，不再需要静态的平衡模块来牵制各个方向使用的时间了。

* cuda 为什么要引入 threadsPerBlock，而不是使用 warpsPerBlock？

    1. 编程模型的一致性

        ```cpp
        // 当前的线程级抽象
        vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(...);

        // 如果改用warp级抽象
        vectorAdd<<<blocksPerGrid, warpsPerBlock>>>(...);
        // 问题：程序员还需要关心每个warp内的线程细节吗？
        ```

    2. 硬件无关的编程接口

        ```cpp
        // 线程级抽象提供了硬件无关的接口
        // 同样的代码在不同代GPU上都能工作
        threadsPerBlock = 256;  // 在Pascal、Volta、Ampere上都有效

        // 如果暴露warp大小：
        warpsPerBlock = 8;
        // 但不同架构的warp大小可能变化（历史上确实考虑过）
        ```

    实际上，CUDA也在向更高级的抽象发展：

    ```cpp
    // CUDA 11+ 的cooperative groups
    #include <cooperative_groups.h>

    __global__ void kernel() {
        auto block = cooperative_groups::this_thread_block();
        auto warp = cooperative_groups::tiled_partition<32>(block);
        // 现在可以在warp级别编程，但底层仍然是线程模型
    }
    ```

* 一个 SM 中有多个 warp，SM 里有调度器，可以编排 warp 处于计算状态还是处于访问内存状态，从而优化访存延迟

* makefile 中，`@echo "The process ID in Shell is: $$PPID"`，$ 给 Shell, Shell 看到的是 $PPID

    output:

    ```
    The process ID in Shell is: 427531
    ```

* Cross Entropy Loss

    用于计算两个概率分布之间的差值。

    $\mathrm{CrossEntropyLoss}(x, \mathrm{target}) = - \frac 1 N \sum_i (\mathrm{target}_i \cdot \log x_i)$

    * x represents the predicted values,

    * target represents the ground truth or target values.

    注：

    1. 这个数学公式中的$target_i$是向量中的元素，与下面 torch 实现的标签编码不一样。

        在实际任务中，$target_i$大部分为 0，只有一个为 1，其实相当于一个 indicator。

    1. 这里的 N 指的并不是 batch size，而是一个向量中的 N 个元素，相当于下面的`N_class`。

    syntax:

    ```py
    torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=- 100, reduce=None, reduction='mean', label_smoothing=0.0)
    ```

    example:

    ```py
    from hlc_utils import *

    ce_loss = nn.CrossEntropyLoss()

    batch_size = 2
    N_class = 4

    input = torch.randn(batch_size, N_class)
    print('input, shape: {}, data:\n{}\n'.format(input.shape, input))

    target = torch.randint(0, N_class, (batch_size,))
    print('target, shape: {}, data:\n{}\n'.format(target.shape, target))

    output = ce_loss(input, target)
    print('output, shape: {}. data:\n{}'.format(output.shape, output))
    ```

    output:

    ```
    input, shape: torch.Size([2, 4]), data:
    tensor([[ 1.0211,  2.0191, -0.9489, -1.2573],
            [ 1.2270,  1.9557, -0.6735, -0.9454]])

    target, shape: torch.Size([2]), data:
    tensor([3, 2])

    output, shape: torch.Size([]). data:
    3.379208564758301
    ```

    注：

    1. input 应该是**未经过**“概率化”的向量，所谓概率化指的是一个向量中的`N_class`个值加起来和为 1. `CrossEntropyLoss`内置了对输入值进行 softmax 预处理的操作。

    1. target 的值是标签编码（Label Encoding，与 one-hot 编码相对应）

    1. 如果 batch size 大于 1，那么 CrossEntropyLoss 求的是 batch 的均值。

    Advantages:

    * Invariant to scaling and shifting of the predicted probabilities.

    Disadvantages:

    * Sensitive to outliers and imbalanced data (can be biased towards majority class).

    * It does not provide a similarity between classes which can be required in some cases.

* 使用`random_()`可以将数据初始化为随机值

    example:

    `target = torch.empty(2, dtype=t.long).random_(4)`

    创建一个 shape 为`(2, )`的数组，将其数据初始化为`[0, 4)`的随机值。

* `torch.randint()`要求 size 参数必须为 tuple 类型

    比如：

    * 对于一维张量: `(batch_size,)`

    * 对于二维张量: `(batch_size, seq_len)`