* [v] reorg: documents 30 mins

    16:08 ~ 16:32

    feedback:

    1. [ ] 构建数据集以及数据集的代码

        先从 sin, cos 入手，再做鸢尾花，葡萄洒等传统机器学习数据集，再做 mnist, minist-fashion，再然后是自然语言数据集，最后是综合数据集，比如视频，imagenet 等。

* [v] qa: 4 units

    正确率：2 / 4

    feedback:

    1. 修改 opengl note qa 中的`请给一个三角形加上纹理贴图。`，在 glsl 代码前加上

        `#version 330 core`

        否则跑不通。

    2. [ ] fix bug: 保存最新 qa record 时，不能删除旧的

    3. 假如一个集合有 10 个 0.1，现在只允许每个元素对自身除以 2，再平均到 1，这个集合构造出的数是有限的还是无限的？这些数的取值的概率密度是怎样的？

* [ ] qa 增加`python main.py --review`功能，复习当天的 units

* [v] 调研 cuda gdb，hit nccl kernel 中的断点

    可能尝试的方向：

    1. b address

    2. 在 nccl 中自己写 kernel；显式写 comm kernel，不使用 nccl 中的 template

    feedback:

    1. 使用 b address `(cuda-gdb) b *0x7ffe85823040`会导致直接 cuda-gdb 直接报错退出。

        ```
        (cuda-gdb) b *0x7ffe85823040
        Breakpoint 3 at 0x7ffe85823040
        (cuda-gdb) c
        Continuing.
        warning: Cuda API error detected: cuLaunchKernelEx returned (0x190)


        zjxj:55959:55959 [1] enqueue.cc:1451 NCCL WARN Cuda failure 400 'invalid resource handle'
        zjxj:55959:55959 [1] NCCL INFO group.cc:231 -> 1
        zjxj:55959:55959 [1] NCCL INFO group.cc:453 -> 1
        zjxj:55959:55959 [1] NCCL INFO group.cc:546 -> 1
        zjxj:55959:55959 [1] NCCL INFO group.cc:101 -> 1
        fail to group end
        [Thread 0x7fff590fd000 (LWP 55998) exited]
        [Thread 0x7fff598fe000 (LWP 55997) exited]
        [Thread 0x7fff615ac000 (LWP 55994) exited]
        [Thread 0x7fff61dad000 (LWP 55993) exited]
        [Thread 0x7fffc0dff000 (LWP 55992) exited]
        [Thread 0x7fffc924a000 (LWP 55991) exited]
        [Thread 0x7fffc2a4f000 (LWP 55990) exited]
        [Thread 0x7fffc8a49000 (LWP 55985) exited]
        [Thread 0x7fffc9b3d000 (LWP 55983) exited]
        [Thread 0x7fffd4909000 (LWP 55963) exited]
        [Inferior 1 (process 55959) exited with code 0377]
        ```

    2. 在模板函数`ncclKernelMain()`入口处下断点，等 cuda-gdb 运行半个小时才可以 hit 断点。

        之后每 step 一步都需要几分钟到十几分钟不等的时间。

    3. 调研`cuLaunchKernelEx()`，为自己在 nccl 里写 kernel 做准备。

    4. 调研：cuda-gdb 当进入 cuda kernel 代码中后，是否还能查看 host code 的变量

    5. 调研 224 机器上的 nccl cuda-gdb 情况，是否需要等很长时间

* [v] 调研`pathlib`

    feedback:

    1. resources

        * pathlib --- 面向对象的文件系统路径
        
            <https://docs.python.org/zh-cn/3.13/library/pathlib.html>

        * A Comprehensive Guide to Using pathlib in Python For File System Manipulation
        
            <https://www.datacamp.com/tutorial/comprehensive-tutorial-on-using-pathlib-in-python-for-file-system-manipulation>

* [ ] 在做检测时，写出 unit 出自哪里