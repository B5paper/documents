* 想了想，目前处理大型 project 的方法就是将 project 分解成模块 A, B, C, ... 然后锻炼自己快速组合模块 A， B， C, ... 的能力，直到可以快速快速解构、复现整个项目。如果想做对比实验，那就把模块改成 A, B1, C，或 A1, B, C 等等。

* 每日任务模板

    ```md
    * [ ] reorg: documents 30 mins 09.17

        10:13 ~ 10:25

    * [ ] reorg: projects 30 mins 09.17

    * [ ] qa: 2 units 30 mins 09.17

    * [ ] cache tabs 30 mins 09.17

    * [ ] process tabs 30 mins 09.17

    * [ ] process 1 tab 09.17

    * [ ] task 1 xx mins

    * [ ] task 2 xx mins
    ```

    说明：

    * `reorg: documents`之类的任务，由于每次的任务名称一样，所以需要记录日期以区分。

    * `10:13 ~ 10:25`所有任务都需要记录开始和结束时间，以记录有效工作时间

    * 调研任务，reorg, cache tab 任务需要预计总时间。比如`30 mins`。

        这些任务没有明确的任务量，所以用时间来约束。

    * qa 任务需要预计需求量，比如`qa: 2 units`。由于 qa 任务是精心编排的，不存在未知的问题，所以也指定了约束时间，比如`30 mins`。

* `objdump`

    主要用于反汇编和分析目标文件及可执行文件。

    常用功能：

    1. 反汇编

        将二进制可执行文件或目标文件（.o, .exe, .so, .dll 等）中的机器代码转换回汇编语言代码。

        `objdump -d ./my_program`

        * `-d`选项表示反汇编包含指令的节（section）。

    2. 查看目标文件结构

        显示文件的头部信息和各个节（Section）的详细信息。包括文件的格式（如ELF、PE）、入口地址、节的大小和位置等。

        `objdump -h ./my_program`
        
        * `-h`选项显示节的头部摘要。

    3. 查看符号表

        列出文件中定义和引用的所有符号（如函数名、全局变量名）。

        `objdump -t ./my_program`

        * `-t`选项显示符号表。

    4. 查看文件头信息

        显示二进制文件的元数据，例如目标架构（如x86-64、ARM）、操作系统ABI、文件类型（可执行、共享库等）和入口点地址。

        `objdump -f ./my_program`

        * `-f`选项显示文件头信息。

    5. 以十六进制格式查看文件内容

        除了反汇编，objdump 还可以直接显示文件的十六进制和ASCII表示，类似于 hexdump 或 xxd 命令。

        `objdump -s -j .text ./my_program`

        * `-s` 显示所有节的内容。

        * `-j` 指定只显示某个节（如 .text 节）的内容。

    6. 查看动态链接信息

        对于动态链接的可执行文件或共享库，可以显示其依赖的共享库（如Linux下的 .so 文件）以及动态符号表。

        `objdump -p ./my_program`

        * `-p`显示与动态链接相关的信息（在 ELF 文件中，这类似于`readelf -d`命令）。

* `od -t x<N>`

    按`<N>`字节一组，打印十六进制数据。

    其中`<N>`可以取值 1, 2, 4, 8，如果不指定`<N>`，则默认取`2`。

    注意多字节显示时，输出受字节序的影响，比如单字节显示的`01 02`，使用小端序 + `-x2`显示时可能变成`0201`。

* 使用 permute 导致 tensor 变成 continuous 的例子

    ```py
    import torch as t

    a = t.rand(3, 4)
    print('a shape: {}'.format(a.shape))
    a = a.permute(1, 0)
    print('after permute, a shape: {}'.format(a.shape))
    print('is continuous: {}'.format(a.is_contiguous()))
    a = a.view(2, 6)
    ```

    output:

    ```
    a shape: torch.Size([3, 4])
    after permute, a shape: torch.Size([4, 3])
    is continuous: False
    Traceback (most recent call last):
      File "/home/hlc/Documents/Projects/torch_test/main.py", line 8, in <module>
        a = a.view(2, 6)
    RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.
    ```

* torch 拟合 xor 函数

    ```py
    import torch
    import torch.nn as nn
    from torch import optim

    class SimpleNN(nn.Module):
        def __init__(self):
            super(SimpleNN, self).__init__()
            self.fc1 = nn.Linear(2, 4)
            self.fc2 = nn.Linear(4, 1)

        def forward(self, x):
            x = torch.relu(self.fc1(x))
            x = self.fc2(x)
            return x
        
    X_train = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])
    y_train = torch.tensor([[0.0], [1.0], [1.0], [0.0]])

    # Instantiate the Model, Define Loss Function and Optimizer
    model = SimpleNN()
    criterion = nn.MSELoss()
    optimizer = optim.SGD(model.parameters(), lr=0.1)

    for epoch in range(100):
        model.train()

        # Forward pass
        outputs = model(X_train)
        loss = criterion(outputs, y_train)
        
        # Backward pass and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if (epoch + 1) % 10 == 0:
            print(f'Epoch [{epoch + 1}/100], Loss: {loss.item():.4f}')

    model.eval()
    with torch.no_grad():
        test_data = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])
        predictions = model(test_data)
        print(f'Predictions:\n{predictions}')
    ```

    output:

    ```
    Epoch [10/100], Loss: 0.2205
    Epoch [20/100], Loss: 0.1844
    Epoch [30/100], Loss: 0.1600
    Epoch [40/100], Loss: 0.1357
    Epoch [50/100], Loss: 0.1115
    Epoch [60/100], Loss: 0.0890
    Epoch [70/100], Loss: 0.0671
    Epoch [80/100], Loss: 0.0481
    Epoch [90/100], Loss: 0.0320
    Epoch [100/100], Loss: 0.0199
    Predictions:
    tensor([[0.1897],
            [0.9428],
            [0.8315],
            [0.0905]])
    ```

    说明：

    1. `super(SimpleNN, self).__init__()`与`super().__init__()`是等价的

    1. `model.train()`将模型切换为训练模式，不需要写成`model = model.train()`

        特点：

        * Dropout层会随机丢弃神经元

        * BatchNorm层使用当前批次的统计量（均值和方差）

        * 启用梯度计算（autograd）

        * 适合训练阶段使用

    1. `model.eval()`将模型切换为评估模式

        * Dropout层不会丢弃神经元（所有神经元都参与计算）

        * BatchNorm层使用训练阶段学到的运行统计量

        * 通常与torch.no_grad()一起使用来禁用梯度计算

        * 适合测试、验证和推理阶段使用

* criterion prn. [kraɪˈtɪriən] n. 标准，准则