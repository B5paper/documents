* 以前的科学家、艺术家受到疾病、战争、经济、政治等因素的影响比较多，似乎如果有经济保障和稳定的科研环境，那么就可以有无限的重大成果和产出。现代的人受到功名、声誉、学术成果、论文、金钱、权利、学术帮派的诱惑比较多，导致了即使有稳定的科研环境，也很难有创新和成果。虽然时代变了，但是我们身上的枷锁仍然没有减少。

* Suspend（挂起）函数

    在 suspend 函数中，驱动通常需要做以下几件事：

    1. 保存设备状态：将设备当前关键的运行时状态（例如寄存器配置）保存起来，以便在 resume 时能恢复原样。

    2. 降低设备功耗：

        * 关闭设备时钟。

        * 将设备置于其硬件支持的睡眠模式。

        * 切断设备不需要的电源域（如果软件可控）。

    3. 处理未完成的操作：确保所有挂起的I/O操作被妥善处理，避免数据丢失。

    4. 释放共享资源：可能会释放一些系统级的共享资源，如中断、DMA通道等，以便整个系统能更彻底地休眠。

* Resume（恢复）函数

    在 resume 函数中，驱动通常需要做以下几件事：

    1. 恢复电源和时钟：重新开启设备的时钟和电源。

    2. 重新初始化设备：

        * 将从 suspend 中保存的设备状态重新写入设备的寄存器。

        * 执行一系列初始化序列，让设备回到它“睡着”之前的工作状态。

    3. 重新申请资源：重新申请中断、DMA通道等系统资源。

    4. 恢复正常操作：告知设备可以开始正常处理数据了。

* suspend 和 resume 的 example

    在现代 Linux 内核中，suspend 和 resume 函数通常被赋值给一个 struct dev_pm_ops 结构体的成员，或者更简单地，通过 DEFINE_SIMPLE_DEV_PM_OPS 宏来定义。

    ```c
    #include <linux/pm.h>

    static int my_driver_suspend(struct device *dev)
    {
        struct my_device_data *data = dev_get_drvdata(dev);

        /* 1. 保存设备状态到 data->saved_reg */
        /* 2. 将设备设置为睡眠模式 */
        /* 3. 关闭设备时钟 */
        return 0;
    }

    static int my_driver_resume(struct device *dev)
    {
        struct my_device_data *data = dev_get_drvdata(dev);

        /* 1. 打开设备时钟 */
        /* 2. 将设备从睡眠模式唤醒 */
        /* 3. 从 data->saved_reg 恢复设备状态 */
        return 0;
    }

    /* 定义电源操作结构 */
    static const struct dev_pm_ops my_driver_pm_ops = {
        .suspend = my_driver_suspend,
        .resume = my_driver_resume,
        /* 可能还有 .freeze, .thaw, .poweroff 等更细粒度的回调 */
    };

    /* 在驱动注册时，将这个 pm_ops 关联到设备 */
    static struct platform_driver my_driver = {
        .driver = {
            .name = "my_device",
            .pm = &my_driver_pm_ops, // 这里是关键
        },
        .probe = my_probe,
        .remove = my_remove,
    };
    ```

* S3（Standby/挂起到内存）和S4（Hibernate/挂起到硬盘）

    struct dev_pm_ops 的结构体中，主要包含以下几对函数:

    1. `.suspend` / `.resume`

        主要用于： 运行时电源管理（Runtime PM）和（在较老的内核中）系统睡眠的一部分。但在现代内核中，对于系统睡眠，它们通常被更具体的回调所取代。

    1. `.freeze` / `.thaw`

        主要用于： 为休眠（Hibernate）做准备。

        作用： 在创建休眠镜像之前，.freeze 会被调用来让设备进入一个安静、静止的状态，但它不一定要让设备进入低功耗模式。重点是停止所有I/O操作，并让设备处于一个已知的、稳定的状态，以便内核能安全地创建一个完整的内存镜像并保存到硬盘（S4状态）。

    1. `.poweroff` / `.restore` (或 `.suspend_noirq` / `.resume_noirq` 的特定阶段)

        主要用于： S3睡眠（挂起到内存） 和 S4睡眠（休眠）的最后阶段。

        作用： 这是真正让设备进入低功耗状态的函数。对于S3，它会切断设备电源，只保留内存供电；对于S4，在内存镜像保存完毕后，它会最终关闭设备电源。

    1. `.suspend_late` / `.resume_early`

        这些是在进程和中断被禁用/启用前后执行的，用于执行一些非常关键、需要原子上下文的操作。

* S3睡眠（挂起到内存）流程

    1. 冻结用户空间和进程。

    2. 调用驱动的`.suspend`或`.freeze`回调（让设备安静下来）。

    3. 执行系统核心的 suspend_late 操作。

    4. 禁用设备中断。

    5. 调用驱动的 .suspend_noirq 或 .poweroff 回调（这是最关键的一步，在这里驱动会真正切断设备时钟/电源，将其置于S3要求的低功耗状态）。

    6. 系统进入S3状态，仅内存保持刷新。

    恢复时，顺序相反。

* S4睡眠（休眠到硬盘）流程

    1. 冻结用户空间和进程。

    2. 调用驱动的`.freeze`回调（让设备进入静止状态，以便创建内存镜像）。

    3. 内核将整个内存内容写入硬盘的交换区（创建休眠镜像）。

    4. 调用驱动的`.poweroff`回调（既然内存镜像已保存，现在可以安全地关闭设备电源了）。

    5. 系统完全断电（S4状态）。

    恢复时，这是一个“重启”过程：BIOS/UEFI启动 -> 内核从休眠镜像重新加载 -> 调用驱动的`.restore`回调重新初始化设备 -> 解冻进程和用户空间。

* `grep -z`

    grep -z 将输入数据中的 空字符（NUL, \0） 视为行分隔符，而不是默认的换行符。这使得它能够处理包含多行文本的“记录”，甚至处理二进制文件。

    行为对比：

    1. 默认行为 (grep 不加 -z)

        * 记录分隔符： 换行符 (\n)

        * 工作方式： grep 一次读取一行（以 \n 分隔）进行模式匹配。

        * 问题： 如果一段文本跨越多行，并且你希望将这多行作为一个整体来搜索，默认的 grep 就无法直接做到。

    2. 使用 -z 或 --null-data 的行为

        * 记录分隔符： 空字符 (\0)

        * 工作方式： grep 会读取数据，直到遇到一个 NUL 字符，然后将这整个数据块（可能包含很多换行符）作为一个单一的“记录”进行模式匹配。

    * 匹配一个跨越多行的模式：

        example:

        `test.txt`:

        ```
        Start of block
        This is a pattern we want
        End of block
        Another line
        ```

        run:
        
        `grep -z 'block.*pattern' test.txt`

        output:

        ```
        Start of block
        This is a pattern we want
        End of block
        Another line
        ```

        其中第一行的`block`和第二行的`This is a pattern`被标红。

        可以看到，整个文本会被全部输出。看来这个功能只能用于跨行的标红。

        `.`本身不匹配`\n`，但是在`grep -z`中可以匹配。

* nasm

    安装:
    
    `sudo apt install nasm`

    验证安装：

    `nasm -v`

    hello, world example:

    `hello.asm`:

    ```nasm
    section .data
        hello db 'Hello, World!', 0xa    ; 字符串和换行符
        hello_len equ $ - hello          ; 计算字符串长度

    section .text
        global _start

    _start:
        ; 写入系统调用
        mov eax, 4          ; 系统调用号 (sys_write)
        mov ebx, 1          ; 文件描述符 (stdout)
        mov ecx, hello      ; 字符串地址
        mov edx, hello_len  ; 字符串长度
        int 0x80            ; 调用内核

        ; 退出系统调用
        mov eax, 1          ; 系统调用号 (sys_exit)
        mov ebx, 0          ; 退出状态
        int 0x80            ; 调用内核
    ```

    compile:

    `nasm -f elf64 hello.asm -o hello.o`

    link:

    `ld hello.o -o hello`

    run:

    `./hello`

    output:

    ```
    Hello, World!
    ```

* 打开文件时`a+`的行为分析

    使用`a+`打开时，`seek()`只对读取有效，对写入无效，写入总是发生在文件末尾。

    example:

    ```py
    with open('msg.txt', 'a+') as f:
        f.write('hello\n')
        f.seek(0)
        f.write('world\n')

        print('first read:')
        content = f.read()
        print(content)
        print('')

        print('second read:')
        f.seek(0)
        content = f.read()
        print(content)
    ```

    output:

    ```
    first read:


    second read:
    hello
    world

    ```

    可以看到，虽然在`f.write('world\n')`之前调用了`f.seek(0)`，但是写入的`world`仍然在`hello`后面。

    另外，调用完`f.write()`后，当前 pos 位置又变到文件末尾，所以第一次`f.read()`没有读到内容。

    `a+`模式下，虽然`seek()`不影响`write()`的行为，但是影响`read()`的行为，可以看到第二次 read 读到了文件的内容。

* awk

    命令基本结构：
    
    `awk '模式 {动作}' 文件名`

    * 模式：可选，用于筛选行（如 `/正则/`、`条件表达式`）。

    * 动作：对匹配的行执行的操作（如 `print`、计算）。

    常用内置变量:

    * `$0`：整行内容。

    * `$1, $2...`：第1、2...列字段。

    * `NF`：当前行的字段数。

    * `NR`：当前行号。

    * `FS`：输入字段分隔符（默认为空格/制表符）。

    * `OFS`：输出字段分隔符（默认为空格）。

    常见用法:

    * 打印指定列

        `msg.txt`:

        ```
        hello world nihao zaijian
        haha hehe haihai huaihuai
        1 2 3 4
        ```

        ```bash
        awk '{print $1, $3}' msg.txt      # 打印第1列和第3列
        ```

        output:

        ```
        hello nihao
        haha haihai
        1 3
        ```

    * 条件过滤

        `data.txt`:

        ```
        1 2 3
        4 5 6
        7 8 9
        10 11hehe 12
        ```

        ```bash
        awk '$2 >= 5 {print $0}' data.txt # 打印第 2 列大于等于 5 的行
        ```

        output:

        ```
        4 5 6
        7 8 9
        ```

        如果在比较时，发现有非数字项，那么会被当作`0`处理。所以`11hehe`没有被输出。数据中的小数也可以被正确处理。

        如果需要做“等于”比较，那么可以使用两个等号`==`。

        ```bash
        awk '/error/ {print NR, $0}' log.txt # 打印包含"error"的行及其行号
        ```

* Long Short-Term Memory (LSTM) 

    * Hidden State (h_n)

        The hidden state in an LSTM represents the short-term memory of the network.

        Shape: The hidden state h_n has the shape (num_layers * num_directions, batch, hidden_size). This shape indicates that the hidden state is maintained for each layer and direction in the LSTM.

    * Output (output)

        The output of an LSTM is the sequence of hidden states from the last layer for each time step. 

* 一个可以跑通的 lstm example

    ```py
    import torch
    import torch.nn as nn
    import numpy as np
    import matplotlib.pyplot as plt

    # ==== 超参数 ====
    seq_len = 20       # 每个输入序列长度
    hidden_size = 64   # LSTM 隐层维度
    num_layers = 1
    num_epochs = 200
    lr = 0.01
    torch.manual_seed(0)
    np.random.seed(0)

    # ==== 生成数据 ====
    x = np.linspace(0, 100, 1000)
    y = np.sin(x)
    y = (y - y.min()) / (y.max() - y.min())

    # 构造序列
    def create_dataset(data, seq_len):
        xs, ys = [], []
        for i in range(len(data) - seq_len):
            xs.append(data[i:i+seq_len])
            ys.append(data[i+seq_len])
        return np.array(xs), np.array(ys)

    X, Y = create_dataset(y, seq_len)
    X = torch.tensor(X, dtype=torch.float32).unsqueeze(-1)  # [batch, seq_len, 1]
    Y = torch.tensor(Y, dtype=torch.float32).unsqueeze(-1)  # [batch, 1]

    train_size = int(0.8 * len(X))
    X_train, X_test = X[:train_size], X[train_size:]
    Y_train, Y_test = Y[:train_size], Y[train_size:]

    # ==== 模型定义 ====
    class LSTMModel(nn.Module):
        def __init__(self, input_size=1, hidden_size=64, num_layers=1, output_size=1):
            super(LSTMModel, self).__init__()
            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
            self.fc = nn.Linear(hidden_size, output_size)

        def forward(self, x):
            out, _ = self.lstm(x)
            out = out[:, -1, :]  # 取最后时刻输出
            out = self.fc(out)
            return out

    model = LSTMModel(hidden_size=hidden_size, num_layers=num_layers)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    # ==== 训练 ====
    for epoch in range(num_epochs):
        model.train()
        optimizer.zero_grad()
        output = model(X_train)
        loss = criterion(output, Y_train)
        loss.backward()
        optimizer.step()
        if (epoch + 1) % 20 == 0:
            print(f"Epoch [{epoch+1}/{num_epochs}]  Loss: {loss.item():.6f}")

    # ==== 测试 ====
    model.eval()
    with torch.no_grad():
        pred = model(X_test)   # shape [test_len, 1]
        loss = criterion(pred, Y_test)
        print(f"Test MSE: {loss.item():.6f}")

    # ==== 未来趋势预测（修正后的循环） ====
    future_steps = 50  # 预测未来 50 个点
    future_preds = []

    # 取测试集最后一个序列作为起点（注意是最后一个 X_test）
    last_seq = X_test[-1].clone().detach().unsqueeze(0)  # shape [1, seq_len, 1]

    model.eval()
    with torch.no_grad():
        for _ in range(future_steps):
            # next_pred: shape [1, 1]
            next_pred = model(last_seq)
            future_preds.append(next_pred.item())
            # 把 next_pred 扩成 [1, 1, 1]，然后滑动窗口拼接成新的 last_seq
            next_pred_expanded = next_pred.unsqueeze(-1)     # [1, 1, 1]
            last_seq = torch.cat((last_seq[:, 1:, :], next_pred_expanded), dim=1)  # [1, seq_len, 1]

    # ==== 可视化（用解析 sin 生成 future GT 并只标 10 个 x） ====
    plt.figure(figsize=(12,4))

    # 背景完整 sin 曲线（淡化）
    plt.plot(y, label='True sin (background)', alpha=0.15, zorder=1)

    # 测试区间预测（实线）
    test_pred_idx_start = train_size + seq_len
    test_pred_idx_end = test_pred_idx_start + len(pred)
    plt.plot(range(test_pred_idx_start, test_pred_idx_end),
             pred.squeeze().cpu().numpy(), label='Predicted (test)', zorder=2)

    # 未来预测（红线）
    future_idx_start = test_pred_idx_end
    future_idx_end = future_idx_start + future_steps
    plt.plot(range(future_idx_start, future_idx_end),
             np.array(future_preds), label='Future Prediction', color='red', linewidth=2, zorder=3)

    # --- 用解析式继续生成 future x 与 GT（并用和训练时相同的归一化） ---
    # 原始 x 数组名为 x；我们假设它还在作用域内
    step = x[1] - x[0]
    future_x = x[-1] + step * np.arange(1, future_steps + 1)  # length future_steps
    future_y_raw = np.sin(future_x)

    # 使用训练时的归一化参数（与之前 y 的归一化保持一致）
    # 注意：在你的脚本里 y = np.sin(x); 然后 y = (y - y.min()) / (y.max() - y.min())
    # 所以我们用同样的 min/max 来归一化 future_y_raw
    orig_y_raw = np.sin(x)  # 原始未归一化的 y（基于原 x）
    y_min, y_max = orig_y_raw.min(), orig_y_raw.max()
    future_y = (future_y_raw - y_min) / (y_max - y_min)

    # 在未来段均匀选取 10 个点标出 'x'
    n_marks = 10
    if future_steps >= n_marks:
        mark_indices = np.linspace(0, future_steps - 1, n_marks, dtype=int)
    else:
        # 如果 future_steps 少于 10，标全部点
        mark_indices = np.arange(future_steps, dtype=int)

    x_gt_marks = np.array(range(future_idx_start, future_idx_end))[mark_indices]
    y_gt_marks = future_y[mark_indices]

    plt.scatter(x_gt_marks, y_gt_marks, marker='x', color='darkred',
                s=80, linewidths=2.5, label='Ground Truth (sampled x)', zorder=4)

    # 计算并打印未来预测与解析 GT 的误差（全量比较）
    pred_array = np.array(future_preds)
    mse_future = np.mean((pred_array - future_y) ** 2)
    print(f"Future MSE against analytic sin (future {future_idx_start}..{future_idx_end-1}): {mse_future:.6f}")

    plt.legend()
    plt.title("LSTM: sin(x) prediction + future trend (10 sampled GT 'x' marks)")
    plt.xlabel("Time step")
    plt.ylabel("Normalized sin(x)")
    plt.tight_layout()
    plt.show()
    ```

    output:

    ```
    Epoch [20/200]  Loss: 0.030417
    Epoch [40/200]  Loss: 0.002496
    Epoch [60/200]  Loss: 0.000462
    Epoch [80/200]  Loss: 0.000082
    Epoch [100/200]  Loss: 0.000028
    Epoch [120/200]  Loss: 0.000012
    Epoch [140/200]  Loss: 0.000006
    Epoch [160/200]  Loss: 0.000003
    Epoch [180/200]  Loss: 0.000002
    Epoch [200/200]  Loss: 0.000002
    Test MSE: 0.000002
    ```

    还会输出一个 sin 曲线的图像。