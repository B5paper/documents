* cuda peer access

    `main.cu`:

    ```cpp
    #include <cuda_runtime.h>
    #include <stdio.h>

    __global__ void arr_inc_1(float *arr)
    {
        int i = threadIdx.x;
        arr[i] += 1;
    }

    int main()
    {    
        float* p0;
        cudaSetDevice(0);
        cudaMalloc(&p0, 4 * sizeof(float));
        cudaMemset(&p0, 0, 4 * sizeof(float));
        // vec_add<<<1, 4>>>(p0);

        cudaSetDevice(1);
        int canAccessPeer;
        cudaDeviceCanAccessPeer(&canAccessPeer, 0, 1);
        if (!canAccessPeer)
        {
            printf("fail to access peer 0\n");
            return -1;
        }
        cudaDeviceEnablePeerAccess(0, 0);
        arr_inc_1<<<1, 4>>>(p0);

        cudaSetDevice(0);
        float buf[4] = {0};
        cudaMemcpy(buf, p0, 4 * sizeof(float), cudaMemcpyDeviceToHost);

        for (int i = 0; i < 4; ++i)
        {
            printf("%.1f, ", buf[i]);
        }
        putchar('\n');

        cudaFree(p0);
        return 0;
    }
    ```

    compile: `nvcc -g main.cu -o main`

    run: `./main`

    output:

    ```
    1.0, 1.0, 1.0, 1.0,
    ```

    我们在 dev 0 上申请显存，然后在 dev 1 上 enable dev 0 的 peer access，再在 dev 1 上 launch kernel，用的数据是 dev 0 上的数据，最后把 dev 0 里的数据拿出来，可以看到是正确的结果。

    说明：

    1. `cudaDeviceEnablePeerAccess(0, 0);`表示从当前 device （由`cudaSetDevice(1);`设定）可以获取 remote device (dev 0) 上的数据，是单向链路。而不是 dev 0 的数据可以由任何其他 dev 获取。

    2. 根据官网资料，peer access 可能走的是 pcie 或 nvlink

        > Depending on the system properties, specifically the PCIe and/or NVLINK topology, devices are able to address each other’s memory

        是否可以走网络或者 host 中转？目前不清楚。

        这里的 peer access 似乎更关注虚拟地址的处理，而不是底层通路。

    3. 根据官网资料，一个 dev 似乎最多能 peer access 8 个其他 dev

        > On non-NVSwitch enabled systems, each device can support a system-wide maximum of eight peer connections.