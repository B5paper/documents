* 对于当前节点，必须反复提问自己：上一个节点在哪里？我是从哪里来的？

* c/cpp 似乎没法直接把一个 val 转换成一个 class/union 的对象，但是可以通过指针转换 + 解引用来完成

    ```cpp
    #include <stdio.h>

    union MyUnion
    {
        float val;
        char spices[4];
    };

    int main()
    {
        float val = 123;
        // MyUnion val_union = (MyUnion) val;  // Error
        MyUnion val_union = *(MyUnion*) &val;
        printf("val_union.val: %f\n", val_union.val);
        return 0;
    }
    ```

    output:

    ```
    val_union.val: 123.000000
    ```

* dfs 停止的条件

    阅读材料时，每次开始认为是一个 block，每遇到一个无法解释的点，则记 uncomprehensive point +1，记录够 N 个后，则认为这个 block 到此已经无法再理解了。可以尝试从下面开始，或者往下跳几段/几章，开始一个新的 block。如果新的 block 也无法理解，则记录 uncomprehensive block +1，攒够 N_2 个后，则认为整个 material 是无法理解的。

* ptx tmp

    * Cooperative thread arrays (CTAs) implement CUDA thread blocks

    * The Parallel Thread Execution (PTX) programming model is explicitly parallel:

    * The thread identifier is a three-element vector tid, (with elements tid.x, tid.y, and tid.z) 

    * Each CTA has a 1D, 2D, or 3D shape specified by a three-element vector ntid (with elements ntid.x, ntid.y, and ntid.z).

    * Threads within a CTA execute in SIMT (single-instruction, multiple-thread) fashion in groups called warps.

    * A warp is a maximal subset of threads from a single CTA

        不懂这句什么意思

    * PTX includes a run-time immediate constant, WARP_SZ

        warp size

    * Cluster is a group of CTAs that run concurrently or in parallel and can synchronize and communicate with each other via shared memory. 

        一个 cluster 由多个 warp 组成，多个 warp 之间可以共享内存。

    * Cluster-wide barriers can be used to synchronize all the threads within the cluster. 

        cluster-wide 并不是拿来同步 cluster 的，而是拿来同步 cluster 中的 thread 的

        不同 cluster 之间的 thread 无法同步，也无法通信。

    * Each CTA in a cluster has a unique CTA identifier within its cluster (cluster_ctaid).

        cta 在 cluster 中有独立的 id 来标识。

    * Each cluster of CTAs has 1D, 2D or 3D shape specified by the parameter cluster_nctaid

        cluster 也可以按 1d, 2d, 3d 组合。这里只提供了一个`cluster_nctaid`，不同维度的 id 该如何获取？

    * Each CTA in the cluster also has a unique CTA identifier (cluster_ctarank) across all dimensions. total number of CTAs across all the dimensions in the cluster is specified by cluster_nctarank.

        这个应该是把 dimension flatten 后得到的 id

    * Threads may read and use these values through predefined, read-only special registers %cluster_ctaid, %cluster_nctaid, %cluster_ctarank, %cluster_nctarank.

    * Each cluster has a unique cluster identifier (clusterid) within a grid of clusters. Each grid of clusters has a 1D, 2D , or 3D shape specified by the parameter nclusterid. Each grid also has a unique temporal grid identifier (gridid). Threads may read and use these values through predefined, read-only special registers %tid, %ntid, %clusterid, %nclusterid, and %gridid.

        cluster id 的标识，没啥可说的，和 thread id 差不多。

    * Each CTA has a unique identifier (ctaid) within a grid. Each grid of CTAs has 1D, 2D, or 3D shape specified by the parameter nctaid. Thread may use and read these values through predefined, read-only special registers %ctaid and %nctaid.

         cta 在 grid 中的 id 标识，注意这个是区别于 cluster 的。

    * Grids may be launched with dependencies between one another

        不同 grid 之间可以指定依赖关系后串行执行。这个依赖关系和 cuda graph 相关。

    * ach thread has a private local memory. Each thread block (CTA) has a shared memory visible to all threads of the block and to all active blocks in the cluster and with the same lifetime as the block. Finally, all threads have access to the same global memory.

        其他的都比较清楚，唯一不清楚的是，shared memory 是每个 cta 有一份，还是一个 cluster 中只有一份？

    * Constant and texture memory are read-only; surface memory is readable and writable.

    * The global, constant, param, texture, and surface state spaces are optimized for different memory usages.

    * texture and surface memory is cached, and within the same kernel call, the cache is not kept coherent with respect to global memory writes and surface memory writes,

        texture 和 surface 总是读缓存里的数据，因此如果在一个 kernel 调用内先存入新的值，再立即去读取值，很有可能读出的值并不是存入的值。

    * The global, constant, and texture state spaces are persistent across kernel launches by the same application.

        不懂这个是啥意思。

        前面一直没提到过 param memory 是干什么用的，有什么特性。