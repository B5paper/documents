* reorg 任务中，默认的 feedback 任务应该放到湔任务之上，`[asso]`的 feedback 任务还按原来的方式处理，放到后面

* gnome 的图片查看器叫 eog

* 贪婪匹配与非贪婪匹配

    非贪婪匹配（也称为惰性匹配或最小匹配）是正则表达式中一种匹配模式，它会尽可能少地匹配字符。通常使用`*?`

    贪婪匹配会尽可能多地匹配字符。通常使用`*`。

    example:

    ```py
    import re

    s = 'abbbabba'

    # 贪婪匹配
    for m in re.finditer(r'a.*a', s):
        start_pos = m.start()
        end_pos = m.end()
        selected_str = s[start_pos:end_pos]
        print(selected_str)  # abbbabba

    # 非贪婪匹配
    for m in re.finditer(r'a.*?a', s):
        start_pos = m.start()
        end_pos = m.end()
        selected_str = s[start_pos:end_pos]
        print(selected_str)  # abbba
    ```

* torch dataset and dataloader

    ```py
    import torch
    from torch.utils.data import Dataset, DataLoader

    class MyDataset(Dataset):
        def __init__(self):
            self.data = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
            self.labels = torch.tensor([0, 1, 0])

        def __len__(self):
            return len(self.data)

        def __getitem__(self, idx):
            return self.data[idx], self.labels[idx]

    dataset = MyDataset()
    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)

    for batch in dataloader:
        print("Batch Data:", batch[0])  
        print("Batch Labels:", batch[1])
    ```

    output:

    ```
    Batch Data: tensor([[1., 2.],
            [3., 4.]])
    Batch Labels: tensor([0, 1])
    Batch Data: tensor([[5., 6.]])
    Batch Labels: tensor([0])
    ```

* image augmentation

    ```py
    import torchvision.transforms as transforms
    from PIL import Image

    image = Image.open('example.jpg')

    transform = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor()
    ])

    augmented_image = transform(image)
    print("Augmented Image Shape:", augmented_image.shape)
    ```

    output:

    ```
    Augmented Image Shape: torch.Size([3, 500, 500])
    ```

* 倒三角符号的 latex 是$\nabla$

* 随机梯度下降（SGD）

    沿着损失函数梯度的反方向更新参数，从而最小化损失函数。

    基本SGD（无动量）:

    对于一组可学习的参数（权重）$\theta$，损失函数为 $J(\theta)$，学习率为 $\eta$。

    在每一步（每个batch）$t$，基本的SGD更新规则为：

    $\theta_{t+1} = \theta_t - \eta \cdot \nabla_\theta J_t (\theta_t)$

    其中：

    * $\theta_t$ 是第 `t` 步（迭代）时的参数值。

    * $\nabla_\theta J_t(\theta_t)$ 是第 `t `步损失函数 $J_t$ 关于参数 $\theta$ 的梯度（在当前 batch 上计算得出）。

    * $\eta$ 是学习率（learning rate），控制每次更新的步长。

* RNN (循环神经网络) 

    RNN是一种专门用于处理序列数据的神经网络。它的核心思想是：网络能对序列中的元素进行循环操作，且能够通过内部状态（隐藏状态）记住之前的信息，并利用这些信息来影响后续的输出。

    核心特征：

    * “循环”与“记忆”：RNN单元不仅接收当前的输入（如句子中的一个词），还接收来自上一个时间步的隐藏状态（Hidden State）。这个隐藏状态充当了网络的“记忆”，它包含了之前所有时间步的序列信息。

    * 参数共享：RNN在每个时间步上使用相同的权重参数（U, W, V）。这使得模型可以处理不同长度的序列，并减少需要训练的参数数量。

    * 计算过程：

        * 在任意时间步 $t$：

            * 新的隐藏状态 $h_t$ 由当前输入 $x_t$ 和前一个隐藏状态 $h_{t-1}$ 共同计算得出：$h_t = \tanh(W \cdot h_{t-1} + U \cdot x_t + b)$

            * 输出 $o_t$ 由当前隐藏状态 $h_t$ 计算得出：$o_t = \mathrm{softmax}(V \cdot h_t + c)$

    * 常见问题：

        梯度消失/爆炸（Vanishing/Exploding Gradients）：在处理长序列时，RNN难以学习到远距离时间步之间的依赖关系，因为梯度在反向传播过程中会指数级地减小或增大。

    example:

    ```py
    import torch
    import torch.nn as nn
    import numpy as np
    import matplotlib.pyplot as plt

    # 1. 设置随机种子以确保结果可复现
    torch.manual_seed(42)
    np.random.seed(42)

    # 2. 生成正弦波序列数据
    def generate_sine_wave_data(seq_length=50, num_samples=1000):
        """
        生成训练数据：用前seq_length个点预测第seq_length+1个点
        X: [num_samples, seq_length, 1]
        y: [num_samples, 1]
        """
        time_steps = np.linspace(0, 100, num_samples + seq_length)
        data = np.sin(time_steps)
        data = data.reshape(-1, 1) # 转换为特征维度为1

        X = []
        y = []
        for i in range(num_samples):
            X.append(data[i:i+seq_length])
            y.append(data[i+seq_length])
        
        return np.array(X), np.array(y)

    # 生成数据
    seq_length = 10
    X, y = generate_sine_wave_data(seq_length, 1000)
    X = torch.from_numpy(X).float()
    y = torch.from_numpy(y).float()

    # 划分训练集和测试集
    train_ratio = 0.8
    train_size = int(train_ratio * len(X))
    X_train, y_train = X[:train_size], y[:train_size]
    X_test, y_test = X[train_size:], y[train_size:]

    # 3. 定义简单的RNN模型
    class SinePredictor(nn.Module):
        def __init__(self, input_size=1, hidden_size=50, output_size=1):
            super(SinePredictor, self).__init__()
            self.hidden_size = hidden_size
            # 使用一个RNN层
            self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
            # 全连接层用于输出预测
            self.fc = nn.Linear(hidden_size, output_size)
        
        def forward(self, x):
            # x的形状: (batch_size, seq_length, input_size)
            # out: 所有时间步的隐藏状态 (batch_size, seq_length, hidden_size)
            # hidden: 最后一个时间步的隐藏状态 (1, batch_size, hidden_size)
            out, hidden = self.rnn(x)
            # 我们只使用最后一个时间步的隐藏状态来进行预测
            out = self.fc(out[:, -1, :]) # 取序列的最后一个输出
            return out

    # 初始化模型、损失函数和优化器
    model = SinePredictor()
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # 4. 训练模型
    num_epochs = 100
    train_losses = []

    for epoch in range(num_epochs):
        model.train()
        optimizer.zero_grad()
        
        # 前向传播
        outputs = model(X_train)
        loss = criterion(outputs, y_train)
        
        # 反向传播和优化
        loss.backward()
        optimizer.step()
        
        train_losses.append(loss.item())
        
        if (epoch+1) % 10 == 0:
            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.6f}')

    # 5. 评估模型并可视化
    model.eval()
    with torch.no_grad():
        train_predictions = model(X_train)
        test_predictions = model(X_test)

    # 绘制损失曲线
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(train_losses)
    plt.title('Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('MSE Loss')

    # 绘制一部分测试集上的真实值和预测值
    plt.subplot(1, 2, 2)
    # 取前100个测试点进行绘制
    plt.plot(y_test[:100].numpy(), label='True Value', alpha=0.7)
    plt.plot(test_predictions[:100].numpy(), label='Prediction', alpha=0.7)
    plt.title('Sine Wave Prediction on Test Set')
    plt.legend()

    plt.tight_layout()
    plt.show()

    # 打印最终训练损失和测试损失
    with torch.no_grad():
        test_loss = criterion(test_predictions, y_test)
    print(f'Final Training Loss: {train_losses[-1]:.6f}')
    print(f'Final Test Loss: {test_loss.item():.6f}')
    ```

    output:

    ```
    Epoch [10/100], Loss: 0.302159
    Epoch [20/100], Loss: 0.080454
    Epoch [30/100], Loss: 0.047464
    Epoch [40/100], Loss: 0.025511
    Epoch [50/100], Loss: 0.005334
    Epoch [60/100], Loss: 0.002867
    Epoch [70/100], Loss: 0.001352
    Epoch [80/100], Loss: 0.001240
    Epoch [90/100], Loss: 0.000831
    Epoch [100/100], Loss: 0.000813
    Final Training Loss: 0.000813
    Final Test Loss: 0.000799
    ```

    代码说明：

    * 数据生成：我们生成了一个正弦波，并创建了输入-输出对。每个输入是一个长度为seq_length的序列，输出是序列后的下一个值。

    * 模型定义：

        * `nn.RNN`层是核心，它处理输入序列并返回所有时间步的输出和最后一个隐藏状态。

        * 我们只使用了最后一个时间步的隐藏状态（out[:, -1, :]）并通过一个全连接层(nn.Linear)来生成最终的预测值。这是一种常见的做法，适用于“多对一”的序列任务。

    * 训练：使用均方误差（MSE）作为损失函数，Adam作为优化器。

    * 评估：模型在测试集上进行预测，并绘制结果图。你会看到预测曲线（橙色）能够很好地跟随真实正弦曲线（蓝色）。

    注：

    1. 画出来的图是 [0, 100]，实际上给出的是 y_test 和 y_pred 的最后 100 个数据，并不是 x 数据的 0 到 100，所以 sin 图像只有 1 个半的波长

    1. `time_steps = np.linspace(0, 100, num_samples + seq_length)`，其中的`num_samples + seq_length`表示，x 一共有`num_samples`个，但`x_i`并不是标量，而是一个长度为`seq_length`的向量，`y_i`则为`x_i[0]`后的第`seq_length + 1`个数，是个标量。

        因此为了 x 起始位置共有`num_sample`个，而 y 的最大值则需要比 y 再多`seq_length`个。这就是所有需要用到的数据。

* IMDb 电影评论数据集

    IMDb 数据集是一个用于二元情感分类的经典基准数据集。它包含来自互联网电影数据库（IMDb）的 50,000 条高度极化的电影评论。

    内容： 每条评论都被标记为 正面（positive） 或 负面（negative）。

    规模： 数据集通常被分为 25,000 条带标签的训练评论和 25,000 条测试评论。此外，还有 50,000 条无标签的额外评论（在此任务中通常不使用）。

    任务： 根据评论文本预测其情感极性（正面/负面）。这是一个典型的文本分类任务。

    explore example:

    ```py
    from datasets import load_dataset
    import numpy as np

    # 1. 加载 IMDb 数据集
    imdb_dataset = load_dataset("imdb")

    # 2. 探索数据集结构
    print("数据集结构:", imdb_dataset)
    print("\n训练集特征:", imdb_dataset["train"].features)
    print("\n测试集第一条样本:", imdb_dataset["test"][0])

    # 3. 查看一些基本统计信息
    # 查看训练集和测试集的大小
    print(f"\n训练集大小: {len(imdb_dataset['train'])}")
    print(f"测试集大小: {len(imdb_dataset['test'])}")

    # 查看标签分布
    train_labels = imdb_dataset["train"]["label"]
    test_labels = imdb_dataset["test"]["label"]

    print(f"\n训练集 - 正面评论: {np.sum(train_labels)}, 负面评论: {len(train_labels) - np.sum(train_labels)}")
    print(f"测试集 - 正面评论: {np.sum(test_labels)}, 负面评论: {len(test_labels) - np.sum(test_labels)}")

    # 4. 随机查看几条样本
    def show_samples(dataset, split="train", num_samples=3):
        sampled_data = dataset[split].shuffle(seed=42).select(range(num_samples))
        for i in range(num_samples):
            print(f"\n--- 样本 {i+1} ---")
            print(f"文本预览: {sampled_data[i]['text'][:200]}...") # 只打印前200个字符
            print(f"标签: {sampled_data[i]['label']} ({'正面' if sampled_data[i]['label'] == 1 else '负面'})")

    show_samples(imdb_dataset, "train")
    ```