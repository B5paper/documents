* nvshmem 指定 transport 为 ucx 后不报错，但是看起来跑的结果也不对：

    ```
    huliucheng@zjxj:~/Documents/Projects/nvshmem_src_3.0.6-4/build/examples$ LD_LIBRARY_PATH=/usr/local/nvshmem/lib /home/huliucheng/Documents/Projects/openmpi-5.0.5/build/bin/oshrun -np 4 -x NVSHMEM_REMOTE_TRANSPORT=ucx ./on-stream 
    [0 of 1] run complete 
    [0 of 1] run complete 
    [0 of 1] run complete 
    [0 of 1] run complete
    ```

    ```
    huliucheng@zjxj:~/Documents/Projects/nvshmem_src_3.0.6-4/build/examples$ LD_LIBRARY_PATH=/usr/local/nvshmem/lib /home/huliucheng/Documents/Projects/openmpi-5.0.5/build/bin/mpirun -np 4 -x NVSHMEM_REMOTE_TRANSPORT=ucx ./dev-guide-ring
    0: received message 0
    0: received message 0
    0: received message 0
    0: received message 0
    ```

    正常的情况应该是 pe 编号分别为 0, 1, 2, 3，但是这个里面是全 0。第二个 example 里 recv msg 也全是 0，显然是有问题的。

* openshmem 矩阵乘法版本 2

    `main.c`:

    ```c
    #include <openmpi/shmem.h>
    #include <stdlib.h>
    #include <stdio.h>
    #include "../shmem_matmul/timeit.h"
    #include "../shmem_matmul/matmul.h"

    int main()
    {
        shmem_init();
        int my_pe = shmem_my_pe();
        int n_pes = shmem_n_pes();

        int mat_N = 2048;
        if (mat_N % n_pes != 0)
        {
            printf("mat_N %% n_pes != 0\n");
            return -1;
        }

        int A_pe_nrow = mat_N / n_pes;

        int *A = (int*) shmem_malloc(mat_N * mat_N * sizeof(int));
        int *B = (int*) shmem_malloc(mat_N * mat_N * sizeof(int));
        int *C = (int*) shmem_malloc(mat_N * mat_N * sizeof(int));
        int *C_ref = (int*) shmem_malloc(mat_N * mat_N * sizeof(int));

        if (my_pe == 0)
        {
            for (int i = 0; i < mat_N * mat_N; ++i)
            {
                A[i] = rand() % 5;
                B[i] = rand() % 5;
            }
        }

        int *A_pe = (int*) shmem_malloc(A_pe_nrow * mat_N * sizeof(int));
        int *B_pe = (int*) shmem_malloc(mat_N * mat_N * sizeof(int));
        int *C_pe = (int*) shmem_malloc(A_pe_nrow * mat_N * sizeof(int));

        timeit(TIMEIT_START, NULL);
        shmem_get32(A_pe, A + my_pe * A_pe_nrow * mat_N, A_pe_nrow * mat_N, 0);
        shmem_get32(B_pe, B, mat_N * mat_N, 0);
        // shmem_barrier_all();

        matmul_i32(A_pe, B_pe, C_pe, A_pe_nrow, mat_N, mat_N);
        // shmem_barrier_all();

        shmem_put32(C + my_pe * A_pe_nrow * mat_N, C_pe, A_pe_nrow * mat_N, 0);
        // shmem_barrier_all();
        timeit(TIMEIT_END, NULL);
        float fsecs;
        timeit(TIMEIT_GET_SEC, &fsecs);
        if (my_pe == 0)
            printf("shmem 4 pe, calc time consumption: %.2f secs\n", fsecs);

        if (my_pe == 0)
        {
            timeit(TIMEIT_START, NULL);
            matmul_i32(A, B, C_ref, mat_N, mat_N, mat_N);
            timeit(TIMEIT_END, NULL);
            timeit(TIMEIT_GET_SEC, &fsecs);
            printf("shmem 1 pe, calc time consumption: %.2f secs\n", fsecs);
        }
        
        if (my_pe == 0)
        {
            timeit(TIMEIT_START, NULL);
            int ret = compare_arr_i32(C, C_ref, mat_N * mat_N);
            if (ret != 0)
                return -1;
            timeit(TIMEIT_END, NULL);
            timeit(TIMEIT_GET_SEC, &fsecs);
            printf("shmem 1 pe, check result time consumption: %.2f secs\n", fsecs);
            printf("all results are correct.\n");
        }

        shmem_free(A_pe);
        shmem_free(B_pe);
        shmem_free(C_pe);
        shmem_free(A);
        shmem_free(B);
        shmem_free(C);
        shmem_free(C_ref);
        shmem_finalize();
        return 0;
    }
    ```

    `Makefile`:

    ```makefile
    main: main.c
        /home/hlc/Documents/Projects/openmpi-5.0.5/bin/bin/oshcc -g main.c -o main

    clean:
        rm -f main
    ```

    run: `/home/hlc/Documents/Projects/openmpi-5.0.5/bin/bin/oshrun -np 4 ./main`

    output:

    ```
    shmem 4 pe, calc time consumption: 11.19 secs
    shmem 1 pe, calc time consumption: 139.43 secs
    shmem 1 pe, check result time consumption: 0.01 secs
    all results are correct.
    ```

    ```
    shmem 4 pe, calc time consumption: 28.12 secs
    shmem 1 pe, calc time consumption: 73.77 secs
    shmem 1 pe, check result time consumption: 0.01 secs
    all results are correct.
    ```

    ```
    shmem 4 pe, calc time consumption: 12.44 secs
    shmem 1 pe, calc time consumption: 53.05 secs
    shmem 1 pe, check result time consumption: 0.00 secs
    all results are correct.
    ```

    这个 example 把所有的内存都分配在 symmetric 空间，不再有单 pe 分配数据，以及单 pe 分配数据时，其他 pe 抢占内存 IO 的情况，从而不再有 app 代码的性能瓶颈。代价是申请的内存变大，会有用不到的情况。

    可以看到 11.19 秒基本已经是 4 进程优化的性能极限了，说明 shmem 确实比较有潜力。

    多次运行程序后，输出并不稳定，可能是笔记本电脑降频的影响。

    猜想：

    1. shmem 的程序会优先把 mem io 占满，如果 pe 没事可做，会不停地抢占式地 poll 状态

* mpi 实现的矩阵乘法

    `main.c`:

    ```c
    #include <stdio.h>
    #include <openmpi/mpi.h>
    #include <stdlib.h>
    #include "../shmem_matmul/matmul.h"
    #include "../shmem_matmul/timeit.h"
    #include <memory.h>

    int main()
    {
        MPI_Init(NULL, NULL);

        int ret;

        int world_size;
        int rank;
        MPI_Comm_size(MPI_COMM_WORLD, &world_size);
        MPI_Comm_rank(MPI_COMM_WORLD, &rank);

        int mat_N = 2048;
        if (mat_N % world_size != 0)
        {
            printf("mat_N %% world_size != 0\n");
            return -1;
        }

        int *A = malloc(mat_N * mat_N * sizeof(int));
        int *B = malloc(mat_N * mat_N * sizeof(int));
        int *C = malloc(mat_N * mat_N * sizeof(int));
        int *C_ref = malloc(mat_N * mat_N * sizeof(int));

        for (int i = 0; i < mat_N; ++i)
        {
            for (int j = 0; j < mat_N; ++j)
            {
                A[i * mat_N + j] = rand() % 5;
                B[i * mat_N + j] = rand() % 5;
            }
        }

        // if (rank == 0)
        // {
        //     printf("rank 0, A:\n");
        //     for (int i = 0; i < mat_N; ++i)
        //     {
        //         for (int j = 0; j < mat_N; ++j)
        //         {
        //             printf("%d, ", A[i * mat_N + j]);
        //         }
        //         printf("\n");
        //     }
        // }
        // MPI_Barrier(MPI_COMM_WORLD);

        int A_rank_nrows = mat_N / world_size;

        int *A_rank = malloc(A_rank_nrows * mat_N * sizeof(int));
        int *B_rank = malloc(mat_N * mat_N * sizeof(int));
        int *C_rank = malloc(A_rank_nrows * mat_N * sizeof(int));

        timeit(TIMEIT_START, NULL);
        MPI_Scatter(A, A_rank_nrows * mat_N, MPI_INT, A_rank, A_rank_nrows * mat_N, MPI_INT, 0, MPI_COMM_WORLD);
        // if (rank == 0)
        //     memcpy(A_rank, A, A_rank_nrows * mat_N * sizeof(int));

        if (rank == 0)
            MPI_Bcast(B, mat_N * mat_N, MPI_INT, 0, MPI_COMM_WORLD);
        else
            MPI_Bcast(B_rank, mat_N * mat_N, MPI_INT, 0, MPI_COMM_WORLD);
        if (rank == 0)
            memcpy(B_rank, B, mat_N * mat_N * sizeof(int));

        // printf("rank %d, A_rank: %d, %d, %d, %d, %d, %d, %d, %d\n", rank, A_rank[0], A_rank[1], A_rank[2], A_rank[3], A_rank[4], A_rank[5], A_rank[6], A_rank[7]);

        matmul_i32(A_rank, B_rank, C_rank, A_rank_nrows, mat_N, mat_N);

        // MPI_Gather(A_rank, A_rank_nrows * mat_N, MPI_INT, A, A_rank_nrows * mat_N, MPI_INT, 0, MPI_COMM_WORLD);
        // MPI_Gather(B_rank, mat_N * mat_N, MPI_INT, B, mat_N * mat_N, MPI_INT, 0, MPI_COMM_WORLD);
        MPI_Gather(C_rank, A_rank_nrows * mat_N, MPI_INT, C, A_rank_nrows * mat_N, MPI_INT, 0, MPI_COMM_WORLD);
        // if (rank == 0)
        // {
        //     memcpy(C, C_rank, A_rank_nrows * mat_N * sizeof(int));
        // }
        timeit(TIMEIT_END, NULL);
        float fsecs;
        timeit(TIMEIT_GET_SEC, &fsecs);
        if (rank == 0)
            printf("mpi 4 np, calc time consumption: %.2f secs\n", fsecs);

        if (rank == 0)
        {
            timeit(TIMEIT_START, NULL);
            matmul_i32(A, B, C_ref, mat_N, mat_N, mat_N);
            // printf("C_ref:\n");
            // print_mat(C_ref, mat_N, mat_N);
            // printf("C:\n");
            // print_mat(C, mat_N, mat_N);
            timeit(TIMEIT_END, NULL);
            timeit(TIMEIT_GET_SEC, &fsecs);
            printf("mpi 1 np, calc time consumption: %.2f secs\n", fsecs);
        }
        
        if (rank == 0)
        {
            ret = compare_arr_i32(C, C_ref, mat_N * mat_N);
            if (ret != 0)
                return -1;
            printf("all results are correct.\n");
        }

        free(A_rank);
        free(B_rank);
        free(C_rank);
        free(A);
        free(B);
        free(C);
        free(C_ref);
        MPI_Finalize();
        return 0;
    }
    ```

    compile: `mpicc -g main.c -o main`

    run: `mpirun -np 4 ./main`

    output:

    ```
    mpi 4 np, calc time consumption: 29.01 secs
    mpi 1 np, calc time consumption: 74.96 secs
    all results are correct.
    ```

    ```
    mpi 4 np, calc time consumption: 20.19 secs
    mpi 1 np, calc time consumption: 108.67 secs
    all results are correct.
    ```

    ```
    mpi 4 np, calc time consumption: 28.73 secs
    mpi 1 np, calc time consumption: 59.83 secs
    all results are correct.
    ```

    看起来 mpi 也并没有好到哪去。最优数据不如 shmem，同样也不够稳定。

    mpi 在跑单核时，其他核的占用率为 0，说明在阻塞等待。但是跑出来的单核性能仍然高出不使用 mpi 运行的单核程序，说明 shmem 的单核性能也不一定是受 mem io 抢占的影响。

    猜想：

    1. mpi 程序中不应该出现 memcpy 函数，如果能想办法不使用这个，性能应该还会再高一点。
