* 之前之所以没有写很多笔记，可能是因为 python + ai 实在太简单了，代码即其含义。另外很多算法都封装起来了，只要会调用就可以了。

* 以前的模式总是跑通一次 -> 记录过程 -> 下次仿照着跑通的来写。问题是不清楚别的方式为什么跑不通，并且不清楚能跑通的 case 的底层原理。

* 为什么如此多人这么急切地投身 AI，为了证明自己是人上人？为了挣钱？为了证明自己寒窗没有苦读？为了跨越阶级？还是说迫不得已，其他方向找不到工作？还是说为了人类的未来？

* makefile 与 bash 中变量等号左右的空格

    makefile 会自动删除等号左右的空格：

    ```makefile
    var_1 := hello world

    var_2 =  hello world

    var_3=hello world

    var_4:=  hello world

    test:
    	@echo "var_1: [$(var_1)]"
    	@echo "var_2: [$(var_2)]"
    	@echo "var_3: [$(var_3)]"
    	@echo "var_4: [$(var_4)]"
    ```

    run: `make`

    output:

    ```
    var_1: [hello world]
    var_2: [hello world]
    var_3: [hello world]
    var_4: [hello world]
    ```

    但是 makefile 不会删变量最右侧的空格：

    ```makefile
    var_1 := hello world    

    test:
    	@echo "var_1: [$(var_1)]"
    ```

    output:

    ```
    var_1: [hello world    ]
    ```

    bash 要求等号左右不允许有空格：

    ```bash
    var_1=hello
    var_2 = hello
    var_3= hello
    var_4 =hello
    var_5=hello world
    var_6=hello    # 有后缀空格
    var_7=" hello   "

    echo var_1: [${var_1}]
    echo var_2: [${var_2}]
    echo var_3: [${var_3}]
    echo var_4: [${var_4}]
    echo var_5: [${var_5}]
    echo var_6: [${var_6}]
    echo var_7: [${var_7}]
    echo var_7: ["${var_7}"]
    ```

    output:

    ```
    main.sh: line 2: var_2: command not found
    main.sh: line 3: hello: command not found
    main.sh: line 4: var_4: command not found
    main.sh: line 5: world: command not found
    var_1: [hello]
    var_2: []
    var_3: []
    var_4: []
    var_5: []
    var_6: [hello]
    var_7: [ hello ]
    var_7: [ hello   ]
    ```

    解释：

    * `var_1=hello`

        没有问题，正常的赋值变量的方式。

    * `var_2 = hello`

        bash 会将`var_2`作为一个 command，` = hello`作为 command 的第一个和第二个参数。

    * `var_3= hello`

        `var_3`为空字符串，`hello`是一个 command。

        这个模式类似于`LD_LIBRARY_PATH=xxx ./main`

    * `var_4 =hello`

        `var_4`是一个 command，`=hello`是其第一个参数。

    * `var_5=hello world`

        这个与`var_3= hello`同理。

    * `var_6=hello    # 有后缀空格`

        忽略字符串后的空格，认为这些空格是空白分隔符。

    * `var_7=" hello   "`

        使用双引号将空格也算在字符串内。

        但是在打印的时候出了问题。

        对于`echo var_7: [${var_7}]`，bash 会将`${var_7}`替换为` hello   `，因此实际执行的命令为`echo var_7: [ hello   ]`，`echo`会认为`var_7:`, `[`, `hello`, `]`是 4 个独立的字符串，两个字符串之间的的空格都为 1.

        对于`echo var_7: ["${var_7}"]`，经过 bash 替换变量后为`echo var_7: [" hello   "]`，echo 会认为`var_7`是第 1 个字符串，`[`, `" hello   "`, `]`分别是第 2，3，4 个字符串，但是这些字符串紧挨着，中间没有空格。

* `patsubst`

    在一个以空格分隔的单词列表中，查找并替换符合特定“模式”（Pattern）的文本。

    syntax:

    ```makefile
    $(patsubst PATTERN,REPLACEMENT,TEXT)
    ```

    * PATTERN： 需要被替换的文本模式。它可以包含一个通配符 %，代表任意长度的任何字符。

    * REPLACEMENT： 替换后的文本模式。它同样可以使用 % 通配符，这个 % 会代表 PATTERN 中 % 所匹配到的内容。

    * TEXT： 需要进行处理的原始文本（通常是一个由空格分隔的列表）。

    返回值： 函数会返回一个经过替换处理后的新列表。

    example:

    ```makefile
    SRCS = main.c helper.c utils.c
    OBJS = $(patsubst %.c,%.o,$(SRCS))
    ```

    处理过程：

    1. 取出 main.c，模式 %.c 匹配成功，% 代表 main。替换为 %.o，即 main.o。

    2. 取出 helper.c，模式匹配成功，% 代表 helper。替换为 helper.o。

    3. 取出 utils.c，模式匹配成功，% 代表 utils。替换为 utils.o。

    example 2:

    ```makefile
    # 原始列表
    FILES = foo.txt bar.log baz.txt

    # 目标：将所有 .txt 文件名的前缀改为 “output-”，如 output-foo.txt output-baz.txt
    NEW_FILES = $(patsubst %.txt,output-%.txt,$(FILES))

    # 结果：NEW_FILES 的值为 `output-foo.txt bar.log output-baz.txt`
    # 注意：bar.log 不符合 %.txt 模式，所以原样保留。
    ```

    `%`，匹配任意数量（0个或多个）的任意字符。匹配是“贪婪”的：% 会尽可能多地匹配字符。

* `aligned_alloc()`

    动态分配一块内存，并且这块内存的起始地址会按照你指定的字节对齐方式对齐。

    syntax:

    ```c
    void *aligned_alloc(size_t alignment, size_t size);
    ```

    * alignment：指定的对齐要求。

        它必须是 2 的幂次方（例如 2, 4, 8, 16, 32, 64 ...）。

        在很多实现中，它必须大于或等于`sizeof(void*)`。

    * size：要分配的内存块大小，单位是字节。

        这个 size 参数最好是 alignment 的整数倍。虽然不是所有标准都强制要求，但这是一个很好的实践，可以确保你分配的内存块末尾之后也有足够的对齐空间，避免潜在问题。

    返回值：成功时返回指向分配内存的指针；失败时返回 NULL（例如，请求的对齐无效或内存不足）。

    example:

    ```c
    #include <stdio.h>
    #include <stdlib.h>

    int main() {
        // 分配 100 字节的内存，并且起始地址保证是 32 字节的倍数
        size_t alignment = 32;
        size_t size = 100;

        // 最佳实践：使分配的大小为对齐的整数倍
        // size_t size = 128; // 这样更好

        int *ptr = (int*)aligned_alloc(alignment, size);

        if (ptr == NULL) {
            fprintf(stderr, "Memory allocation failed.\n");
            return 1;
        }

        // 检查地址是否真的对齐（地址值 % 对齐值 应该为 0）
        printf("Allocated address: %p\n", (void*)ptr);
        printf("Address mod %zu: %zu\n", alignment, (size_t)ptr % alignment); // 应该输出 0

        // 使用内存...
        // ...

        free(ptr); // 记得释放内存！
        return 0;
    }
    ```

    注意事项

    * 释放内存：使用 aligned_alloc() 分配的内存必须使用 free() 来释放。你不能使用 realloc() 直接对其重新分配。

    * 可移植性：这是一个 C11 标准的函数。较老的编译器或库（如 Microsoft VC++ 的 C 库）可能不支持它。在这些平台上，你可能需要使用平台特定的 API（如 _aligned_malloc on Windows, posix_memalign on POSIX systems）或手动进行对齐。

    * 过度使用：除非有明确的对齐需求（如性能分析证明需要或硬件强制要求），否则应优先使用 malloc()，因为它更具可移植性和通用性。

* 创建`tmpfs`类型的目录

    ```bash
    sudo mkdir /mnt/shm
    sudo mount -t tmpfs -o size=2G tmpfs /mnt/shm
    ```

    解释：

    * `-t tmpfs`: 挂载`tmpfs`类型的文件系统。`tmpfs`表示使用 ram，如果 ram 不够，则使用 swap

    * `-o size=2G`：限制目录最大大小为 2G

    * `tmpfs`: 这是“源设备”参数。对于像`tmpfs`这样的虚拟文件系统，这个位置通常就填写文件系统类型本身。

    * `/mnt/shm`: 挂载点（mount point）

    特点：

    * 存储在内存中：所有存放在 /mnt/shm 目录下的文件和目录都位于高速的 RAM 中，因此读写速度非常快。

    * 临时性：这是一个临时存储。当系统重启、崩溃或你手动卸载（umount /mnt/shm）这个文件系统时，其中的所有数据都会消失。

    * 动态分配：tmpfs 只会实际占用它已存储数据大小的内存。例如，如果你创建了一个 100MB 的文件，tmpfs 就只占用约 100MB 的 RAM（和少量元数据开销），而不是一开始就占满 2GB。它会根据存储内容的增加而动态增长，但最大不会超过 size 参数的限制（2GB）。

    * 可能使用交换空间（Swap）：如果系统内存不足，tmpfs 中不活跃的数据可能会被换出到硬盘的交换分区（swap）上，从而释放物理内存。这意味着它的大小可以超过物理 RAM，但性能会下降。

* Tensor 中的转置（Transpose）

    转置是一种改变张量维度（轴）顺序的操作。

    矩阵（一个 2D 张量），它的转置就是沿着主对角线翻转的操作。将矩阵 A 的行和列互换，就得到了它的转置 Aᵀ。

    如果原矩阵 A 的形状是 (m, n)，那么转置后的矩阵 Aᵀ 的形状就是 (n, m)。

    元素的位置关系为：A[i, j] = Aᵀ[j, i]。

    对于维度大于 2 的张量（例如 3D、4D），转置指任意地重新排列张量的所有维度。

    PyTorch 中转置操作是一种“视图操作”，由于不复制数据，原张量和转置后的张量共享同一块内存。修改其中一个的值，另一个也会随之改变。

    1. 默认转置（`.T` 或 `transpose()`）

        在很多框架中，如果不提供参数，.T 属性会默认反转所有维度的顺序。

        `y = x.T`

        新的维度顺序是原顺序的反转：`(2, 1, 0)`

        因此，转置后的形状为：`(original_shape[2], original_shape[1], original_shape[0]) = (4, 3, 2)`

    2. 自定义转置（指定 perm 参数）

        * example 1: 交换最后两个维度

            ```py
            # 假设 x.shape = (2, 3, 4)
            y = x.transpose(0, 2, 1) # 或者 x.permute(0, 2, 1) in PyTorch
            # 新的维度顺序: (0, 2, 1)
            # 新形状: (original_shape[0], original_shape[2], original_shape[1])
            #        = (2, 4, 3)
            ```

        * example 2: 复杂的重新排列

            ```py
            # 假设 x.shape = (2, 3, 4, 5)
            # 我们想要一个新的顺序：将原来的维度 2 放到最前面，然后是维度 0，维度 3，最后是维度 1。
            perm = (2, 0, 3, 1)
            y = x.transpose(perm)
            # 新形状: (original_shape[2], original_shape[0], original_shape[3], original_shape[1])
            #        = (4, 2, 5, 3)
            ```

    numpy 与 torch 的接口函数：

    * numpy

        ```py
        import numpy as np
        x = np.random.rand(2, 3, 4)
        y = x.transpose(0, 2, 1) # 使用 transpose 函数
        z = x.T # 反转所有维度
        ```

    * torch

        ```py
        import torch
        x = torch.randn(2, 3, 4)
        y = x.permute(0, 2, 1) # 常用 permute 函数
        z = x.transpose(1, 2)  # transpose 通常一次只交换两个指定维度，这里是交换维度1和2
        w = x.T # 反转所有维度
        ```

* 将 tensor 从 cpu 转移到 gpu

    * 推荐接口`.to()`

        ```py
        import torch

        # 假设有一个在 CPU 上的 tensor
        cpu_tensor = torch.tensor([1, 2, 3])
        print(cpu_tensor.device) # 输出：cpu

        # 检查 GPU 是否可用
        if torch.cuda.is_available():
            device = torch.device("cuda") # 指定目标设备为 GPU
            gpu_tensor = cpu_tensor.to(device) # 转移到 GPU
            print(gpu_tensor.device) # 输出：cuda:0

            # 你也可以直接使用字符串
            gpu_tensor_2 = cpu_tensor.to('cuda')
        ```

    * 旧兼容接口`.cuda()`

        ```py
        if torch.cuda.is_available():
            gpu_tensor = cpu_tensor.cuda() # 转移到默认 GPU (cuda:0)
            gpu_tensor = cpu_tensor.cuda(0) # 明确转移到第一个 GPU
        ```

    在创建时指定设备：

    ```py
    # 直接在 GPU 上创建 tensor，省去转移步骤
    gpu_tensor = torch.tensor([1, 2, 3], device='cuda')
    # 或者
    gpu_tensor = torch.tensor([1, 2, 3]).to('cuda')
    ```