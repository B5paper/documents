* [ ] 调研`ssh -f`

* [ ] 调研`crontab`

* [ ] 调研`strace`

* [ ] 调研`strchr()`

* [O] 调研 siccl 是否能在 135 机器 4 卡环境上 work

    feedback:

    1. `LINK_NVL`的 bw 目前都低了一半

    1. cpu 1 -> cpu 0 的 link sys 的 bw 不对，135 机器上是 16，siccl 是 5000

        因此导致的 edge 顺序也不对

        cpu 0 -> cpu 1 同理。

    1. compute path 后，nccl 的每个 gpu 都有连到 nvs 的 path，但是 siccl 没有

        ```
        gpu 565248 --> nvs 0
        gpu 786432 --> nvs 0
        gpu 798720 --> nvs 0
        ```

    1. nccl 在 trim system 后，并没有删除其他 rank 的 gpu，为什么？

    1. 因为 trim system 后无法 print topo system，所以后续的 compute path 以及 generate coll graph 目前还没有测

    1. gpu 1 -->SILINK--> swi 0 -->SILINK--> gpu 2

        未来可能形成这种形式，swi 只是一个虚拟节点，但是可能会有多个 switch，比如 gpu 1 通过两层 switch 才连接到 gpu2，但是在拓扑层中，两层 switch 只表现为一个 swi 拓扑节点，这样一来，如果我们在统计 gpu 1 -> gpu 2 的延迟和带宽时，如果只计算两个 silink 的延迟，肯定有问题。

        目前能想到的只有 3 点：

        1. 如果直接放弃 swi 节点， 使用 gpu 1 --SILINK--> gpu 2，那么倒是可以解决不同物理拓扑下不同连接的不同延迟问题，但是 topo system 的表示比较复杂

        2. 使用多个 gpu 互相印证：

            测试 gpu 1 --> gpu 2 的延迟 d_1，得到 x + y = d_1

            测试 gpu 1 --> gpu 3 的延迟 d_2，得到 x + z = d_2

            此时我们得到 y - z = d_1 - d_2

            测试 gpu 2 --> gpu 3 的延迟 d_3，得到 y + z = d_3

            此时我们得到 2 * y = d_1 - d_2 + d_3，由此可分别解出 x, y, z

            那么 x 就是 gpu 1 到 swi 0 的延迟，y 就是 gpu 2 到 swi 0 的延迟，z 就是 gpu 3 到 swi 0 的延迟。

        3. 测试出 gpu 1 --> gpu 2 的延迟后，直接除以 2，作为 SILINK 1 和 SILINK 2 的延迟。

* [ ] 调研`ncclGetLevel()`

* [ ] 调研`ncclTopoSelectNets()`

* [ ] 调研`std::nullopt`