* vscode 里，可以使用 ctrl + up / down 实现向上／下滚动一行，不改变光标位置

* adam 优化器的学习率应该设置为 sgd 的 1/10，比如 sgd 为 0.01，adam 应该设置为 0.001

* 使用多种优化式的 training 过程

    ```py
    import torch
    import torchvision
    import torchvision.transforms as transforms

    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])

    trainset = torchvision.datasets.MNIST(root='./data', train=True,
                                            download=False, transform=transform)

    trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,
                                              shuffle=True, num_workers=2)

    class Net(torch.nn.Module):
        def __init__(self):
            super(Net, self).__init__()
            self.fc1 = torch.nn.Linear(784, 128)
            self.fc2 = torch.nn.Linear(128, 128)
            self.fc3 = torch.nn.Linear(128, 10)

        def forward(self, x):
            x = x.view(-1, 784)
            x = torch.nn.functional.relu(self.fc1(x))
            x = torch.nn.functional.relu(self.fc2(x))
            x = torch.nn.functional.softmax(self.fc3(x), dim=1)
            return x

    net = Net()

    criterion = torch.nn.CrossEntropyLoss()

    # SGD optimizer
    optimizer_sgd = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)

    # Adam optimizer
    optimizer_adam = torch.optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))
    # optimizer_adam = torch.optim.Adam(net.parameters(), lr=0.001)

    # Adagrad optimizer
    optimizer_adagrad = torch.optim.Adagrad(net.parameters(), lr=0.01)

    # Adadelta optimizer
    optimizer_adadelta = torch.optim.Adadelta(net.parameters(), rho=0.9)

    device = 'cpu'

    # Train the neural network using different optimization algorithms
    for epoch in range(10):
        running_loss = 0.0
        correct = 0
        total = 0
        for i, data in enumerate(trainloader, 0):
            inputs, labels = data
            inputs, labels = inputs.to(device), labels.to(device)
            # optimizer_sgd.zero_grad()
            optimizer_adam.zero_grad()
            # optimizer_adagrad.zero_grad()
            # optimizer_adadelta.zero_grad()
            outputs = net(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            # optimizer_sgd.step()
            optimizer_adam.step()
            # optimizer_adagrad.step()
            # optimizer_adadelta.step()
            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        print('Epoch: %d | Loss: %.3f | Accuracy: %.3f %%' %
              (epoch + 1, running_loss / len(trainloader), 100 * correct / total))
    ```

    output:

    ```
    Epoch: 1 | Loss: 1.618 | Accuracy: 85.717 %
    Epoch: 2 | Loss: 1.545 | Accuracy: 91.893 %
    Epoch: 3 | Loss: 1.526 | Accuracy: 93.702 %
    Epoch: 4 | Loss: 1.517 | Accuracy: 94.523 %
    Epoch: 5 | Loss: 1.511 | Accuracy: 95.153 %
    Epoch: 6 | Loss: 1.506 | Accuracy: 95.550 %
    Epoch: 7 | Loss: 1.503 | Accuracy: 95.872 %
    Epoch: 8 | Loss: 1.501 | Accuracy: 96.028 %
    Epoch: 9 | Loss: 1.500 | Accuracy: 96.173 %
    Epoch: 10 | Loss: 1.497 | Accuracy: 96.412 %
    ```