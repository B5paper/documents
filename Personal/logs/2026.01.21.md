* vscode 的设置中，如何只显示已修改的

    在 VS Code 中，可以通过以下几种方式只显示已修改的设置：

    方法一：使用设置编辑器

    * 打开设置（Ctrl+, 或 Cmd+,）

    * 在搜索框中输入 @modified

    * 这样只会显示你已经修改过的设置项

    方法二：使用配置文件

    * 打开用户设置文件：

        * 命令面板（Ctrl+Shift+P 或 Cmd+Shift+P）

        * 输入 "Preferences: Open Settings (JSON)"

    * 这会直接打开 settings.json 文件，里面只包含你自定义的设置

    方法三：设置界面筛选

    在设置界面顶部，点击右上角的“更多操作”（⋯ 图标），然后选择：

    * 显示修改的设置：只显示已更改的设置

    * 重置设置：可以查看所有已修改的设置

    方法四：使用命令面板

    * 打开命令面板

    * 搜索并执行 "Preferences: Open User Settings"

    * 默认就会显示所有用户自定义设置

    方法五：查看设置差异

    * 打开命令面板

    * 输入 "Preferences: Open Default Settings (JSON)"

    * 这会在一个编辑器中显示默认设置，另一个编辑器中显示你的自定义设置，便于对比

    小贴士

    * 设置标识：已修改的设置前面会有一个蓝色的竖线标记

    * 搜索语法：

        * @modified - 已修改的设置

        * @tag:usesOnlineService - 按标签筛选

        * window.title - 搜索特定设置

    最常用的方法是直接在设置搜索框输入 @modified，这样最简单快捷！

* 鼠标在 vim 界面上滑动时，会向 vim 发送鼠标事件序列

    禁用鼠标序列：`:set mouse=`

    `:set mouse=nv    " 仅允许在普通模式和可视模式下使用鼠标`

* vim 禁用终端响铃（Bell）

    ```vim
    set noerrorbells   " 禁用错误提示音
    set visualbell     " 使用视觉提示代替声音
    set t_vb=          " 清空视觉提示（彻底静音）
    ```

* vim 在 windows 下的所有 ctrl + v 都被默认替换成了 ctrl + q

    因为 windows 下的 ctrl + v 是粘贴功能。

    不清楚 GVim 是什么情况，可能 ctrl + v 仍保持 vim 的语义。

* 还有哪些类似 linux vim 中 ctrl + v 在 windows vim 中被默认替换成 ctrl + q 的快捷键的例子？详细列出

    在 Windows 版的 Vim（或 gVim）中，由于 Windows 系统本身有一套广为人知的全局快捷键（如 Ctrl + C 复制、Ctrl + V 粘贴），为了降低新手的门槛，Vim 官方发行版通常会加载一个名为 mswin.vim 的脚本。

    这个脚本会将许多经典的 Vim 快捷键重映射为 Windows 习惯。以下是受影响最严重的几个核心快捷键对比：

    1. 核心映射对照表

        | Windows 快捷键 | 在 Windows Vim 中的功能 | 对应的原始 Vim 快捷键 | 原始 Vim 功能说明 |
        | - | - | - | - |
        | Ctrl + V | 粘贴 (Paste) | Ctrl + V | 块可视化模式 (Visual Block) |
        | Ctrl + Q | 块可视化模式 (替换方案) | 无 | 这是为了补位被占用的 Ctrl + V |
        | Ctrl + C | 复制 (Copy) | y (yank) | 复制选中的文本到剪贴板 |
        | Ctrl + X | 剪切 (Cut) | d (delete) | 删除选中内容并存入剪贴板 |
        | Ctrl + A | 全选 (Select All) | ggVG | 选中全文 |
        | Ctrl + Z | 撤销 (Undo) | u | 撤销上一步操作 |
        | Ctrl + Y | 重做 (Redo) | Ctrl + R | 恢复被撤销的操作 |
        | Ctrl + S | 保存 (Save) | :w | 写入文件 |
        | Ctrl + F | 查找 (Find) | / | 开启搜索模式 |

    2. 详细分析受影响的逻辑

        块模式切换：Ctrl + V vs Ctrl + Q

        在 Linux 下，Ctrl + V 是进入“块选择模式”的唯一王牌。但在 Windows 中，它变成了粘贴。

        * 后果： 如果你习惯了在 Linux 下用 Ctrl + V 垂直编辑多行代码，在 Windows 下你会发现它直接吐出了一堆剪贴板里的乱码。

        * 解决： 必须改用 Ctrl + Q。

        撤销与重做：Ctrl + Z / Y

        * Vim 原生： u 是撤销，Ctrl + R 是重做。Ctrl + Z 在 Linux 终端里通常是将进程挂起到后台（Suspend）。

        * Windows 映射： 遵循 Office 逻辑，Ctrl + Z 撤销，Ctrl + Y 重做。这会导致习惯 Ctrl + R 的老用户感到困惑。

        复制粘贴：Ctrl + C / X

        * Vim 原生： Ctrl + C 通常用于中断当前的命令或回到 Normal 模式（类似 Esc）。

        * Windows 映射： 变成了标准复制。这意味着如果你想通过 Ctrl + C 强行退出某个插件的搜索或提示界面，它可能会毫无反应，甚至报错。

    3. 为什么会这样？

        这是因为在 Windows 版本的安装目录下，有一个 _vimrc 文件，它通常会自动执行：

        ```Vim Script
        source $VIMRUNTIME/mswin.vim
        behave mswin
        ```

        这两行代码就是“罪魁祸首”。mswin.vim 重新定义了上述所有快捷键。

    4. 如何恢复成 Linux 习惯？

        如果你希望在 Windows 上也拥有和 Linux 一模一样的纯粹 Vim 体验，你只需要编辑你的 _vimrc 文件（通常在 C:\Users\你的用户名\_vimrc）：

        * 删除或注释掉 source $VIMRUNTIME/mswin.vim 这一行。

        * 删除或注释掉 behave mswin。

        * 保存并重启 Vim。

        这样，Ctrl + V 就会变回块选择模式，u 依然是撤销，所有 Windows 的“入侵”映射都会消失。

* 简述 flash attention

    Flash Attention 是一种高效计算注意力机制的算法，由斯坦福大学团队于2022年提出。它通过优化内存访问方式，显著减少了Transformer模型中注意力层的计算时间和内存占用。

    核心问题

    传统注意力计算（如标准的Softmax注意力）需要：

    * 计算所有查询（Q）和键（K）的点积，存储一个庞大的中间矩阵（大小为序列长度×序列长度）。

    * 对矩阵进行Softmax归一化。

    * 与值（V）相乘得到输出。

        这会导致内存复杂度和计算复杂度均为 O(N²)，其中 N 是序列长度。对于长序列（如文档、图像或长视频），内存和计算成本极高。

    **Flash Attention 的核心思想**

    Flash Attention 的关键是避免存储整个注意力矩阵，而是通过以下两种技术优化：

    * 分块计算（Tiling）：将 Q、K、V 分成小块，在 GPU 的 SRAM（高速缓存）中逐步计算注意力，避免将中间结果写入较慢的 GPU HBM（高带宽内存）。

    * 重计算（Recomputation）：在反向传播时，不存储注意力矩阵，而是通过存储的中间状态重新计算，以节省内存。

    主要优点

    * 内存高效：将内存复杂度从 O(N²) 降低到 O(N)，显著减少内存占用。

    * 速度快：通过减少内存读写，提升计算效率，尤其适合长序列任务。

    * 支持长序列：使得 Transformer 模型能够处理数千甚至数万个 token 的序列。

    应用场景

    * 大语言模型（LLM）的长上下文训练与推理。

    * 图像生成模型（如 Stable Diffusion）中的注意力层优化。

    * 科学计算或生物信息学中的长序列建模。

    后续发展

    * Flash Attention 2：进一步优化了算法，减少了非矩阵乘法运算，提升了 GPU 利用率。

    * Flash Attention 3：针对特定硬件（如 NVIDIA H100）做了更深度的优化。

    总结

    Flash Attention 是注意力计算的一项突破性优化，通过硬件感知的算法设计，显著提升了 Transformer 模型的可扩展性和效率，为处理更长序列的模型铺平了道路。

* 详细讲一下 flash attention 算子该怎么写

    详细讲解 Flash Attention 算子的实现是一个复杂的系统工程问题。我将从核心设计思路、关键算法步骤、代码结构框架和实现细节几个层面进行详细拆解。

    核心设计哲学：硬件感知优化

    Flash Attention 不是一个数学上的新注意力公式，而是一个硬件感知的算法重设计。其核心是围绕 GPU 的内存层次结构进行优化：

    * HBM：容量大（几十GB），但速度慢。

    * SRAM：容量小（几百KB到几MB），但速度快。
    算法目标：尽可能在 SRAM 中完成核心计算，减少与 HBM 的数据交换（IO）。

    关键算法：前向传播（FlashAttention V1/V2）

    标准注意力公式：

    O = softmax(QK^T) V，其中 Q, K, V 形状为 [N, d]，中间矩阵 S = QK^T 形状为 [N, N]。

    Flash Attention 将其重写为分块（Tiling）的增量式 Softmax。

    我们维护两个运行统计量：

    * m(x)：当前块为止，每行的最大值（最大值，用于数值稳定）。

    * l(x)：当前块为止，每行的指数和（用于归一化）。

    假设我们将序列长度 N 分成 Tr 块（对 Q）和 Tc 块（对 K, V）。对于 Q 的每一个块 i：

    * 从 HBM 加载 Q_i 到 SRAM。

    * 初始化该块对应的输出累加器 O_i = 0，行最大值 m_i = -inf，行指数和 l_i = 0。

    * 对于 K, V 的每一个块 j：

        a. 从 HBM 加载 K_j, V_j 到 SRAM。

        b. 在 SRAM 中计算小块注意力分数：S_ij = Q_i @ K_j^T。（形状：[block_size, block_size]）

        c. 根据当前的 m_i（旧）和 S_ij 的新最大值，计算校正因子。

        d. 更新运行统计量 m_i 和 l_i。这是一个关键步骤，它融合了新旧块的统计信息。

        e. 重缩放（Rescale）：由于 m_i 更新了，之前累加到 O_i 的“旧权重”和新计算的 P_ij = exp(S_ij - m_i_new) 需要重新调整比例，以确保数值一致性。

        f. 计算 P_ij @ V_j，并合并到 O_i 中。

    * 将最终计算好的 O_i, m_i, l_i 写回 HBM。

        伪代码（前向传播简化版）：

        ```text
        输入: Q, K, V (HBM)
        输出: O (HBM)， l, m (用于反向传播)

        将Q分成Tr块，K,V分成Tc块。
        O = zeros_like(Q)
        l = zeros(N)
        m = -inf * ones(N)

        for i in range(Tr):
            # 加载 Qi
            Qi = load_from_HBM(Q, i)
            Oi = 0
            mi_old = m[i*block_size:(i+1)*block_size]
            li_old = l[i*block_size:(i+1)*block_size]

            for j in range(Tc):
                # 加载 Kj, Vj
                Kj, Vj = load_from_HBM(K, V, j)

                # 计算小块Sij
                Sij = Qi @ Kj.T

                # 1. 计算新的行最大值 mij_new = max(mi_old, rowmax(Sij))
                # 2. 计算新旧统计量的指数差
                #    P_ij_raw = exp(Sij - mij_new)  # 新块的未归一化概率
                #    scale_old = exp(mi_old - mij_new) # 旧累加值的缩放因子
                # 3. 更新行指数和 li_new = scale_old * li_old + rowsum(P_ij_raw)
                # 4. 重缩放输出 Oi = scale_old * Oi + P_ij_raw @ Vj
                # 5. 更新运行统计量 mi_old, li_old = mij_new, li_new

            # 存储最终Oi, mi, li到HBM
            store_to_HBM(O, i, Oi)
            store_to_HBM(m, i, mi_old)
            store_to_HBM(l, i, li_old)
        ```

    **算子实现框架（以CUDA为例）**

    一个高性能的 Flash Attention CUDA 算子实现非常复杂，通常包含以下层次：

    1. 高层次调度 (Kernel Launch)

        ```cpp
        // 确定分块策略，启动内核
        void flash_attention_forward(
            const torch::Tensor& Q,
            const torch::Tensor& K,
            const torch::Tensor& V,
            torch::Tensor& O,
            float dropout_p,
            float softmax_scale) {

            int N = Q.size(0); // 序列长度
            int d = Q.size(1); // 头维度
            int batch_size = Q.size(2); // 批大小
            int num_heads = Q.size(3); // 头数

            // 确定块大小（Block Size）：基于SRAM容量、头维度d等启发式选择
            // 例如，对于 d=64，可能选择 block_size=128
            int block_size_m = 128; // Q的块大小
            int block_size_n = 128; // K的块大小

            // 计算网格（Grid）和线程块（Block）维度
            dim3 grid(num_heads, batch_size, ceil_div(N, block_size_m));
            dim3 block(CUDA_NUM_THREADS);

            // 启动内核
            flash_attention_kernel<<<grid, block>>>(
                Q.data_ptr<float>(), K.data_ptr<float>(), V.data_ptr<float>(),
                O.data_ptr<float>(),
                N, d,
                block_size_m, block_size_n,
                softmax_scale,
                dropout_p);
        }
        ```

    2. 内核函数 (Kernel Function) - 核心

        内核函数是一个 __global__ 函数，每个线程块负责计算 Q 的一个块（block_size_m 行）的最终输出。

        ```cpp
        template<typename scalar_t>
        __global__ void flash_attention_kernel(
            const scalar_t* __restrict__ Q,
            const scalar_t* __restrict__ K,
            const scalar_t* __restrict__ V,
            scalar_t* __restrict__ O,
            const int N, const int d,
            const int block_size_m, const int block_size_n,
            const float softmax_scale,
            const float dropout_p) {

            // 1. 分配共享内存（Shared Memory，模拟SRAM）
            //    用于存储当前的Q块、K块、V块以及中间Sij块
            extern __shared__ char shared_mem[];
            scalar_t* Qi = (scalar_t*)shared_mem;
            scalar_t* Kj = (scalar_t*)(Qi + block_size_m * d);
            scalar_t* Vj = (scalar_t*)(Kj + block_size_n * d);
            scalar_t* Sij = (scalar_t*)(Vj + block_size_n * d); // 形状 [block_size_m, block_size_n]

            // 2. 线程块ID确定自己负责的Q块索引（i）
            int q_block_idx = blockIdx.z;
            int start_m = q_block_idx * block_size_m;
            int end_m = min(start_m + block_size_m, N);

            // 3. 为当前Q块初始化运行统计量（在线程块的寄存器中或共享内存中）
            float mi[BLOCK_SIZE_M]; // 每线程负责一行，存储行最大值
            float li[BLOCK_SIZE_M]; // 存储行指数和
            scalar_t Oi[BLOCK_SIZE_M][HEAD_DIM]; // 累加的输出

            // 4. 从全局内存（HBM）加载当前Q块到共享内存
            load_q_block(Q, Qi, start_m, end_m, d, ...);

            // 5. 循环遍历K/V的块（j）
            for (int k_block_idx = 0; k_block_idx < num_k_blocks; ++k_block_idx) {
                int start_n = k_block_idx * block_size_n;
                int end_n = min(start_n + block_size_n, N);

                // a. 加载Kj, Vj块到共享内存
                load_kv_block(K, V, Kj, Vj, start_n, end_n, d, ...);
                __syncthreads(); // 确保所有线程加载完毕

                // b. 在共享内存中计算Sij = Qi @ Kj^T
                //    使用WMMA（Tensor Core）或循环展开+向量化进行优化
                compute_scores(Sij, Qi, Kj, d, block_size_m, block_size_n, softmax_scale);

                // c. 应用Mask（如果有无注意力）、Dropout（可选）
                // d. **增量式Softmax核心**：
                //     - 计算Sij的局部行最大值
                //     - 更新全局行最大值mi_new = max(mi_old, local_max)
                //     - 计算缩放因子：exp(mi_old - mi_new), exp(Sij - mi_new)
                //     - 更新li_new
                //     - 重缩放Oi: Oi = Oi * exp(mi_old - mi_new) + Pij @ Vj
                //     - 更新mi, li = mi_new, li_new

                __syncthreads(); // 确保下次循环前共享内存数据就绪
            }

            // 6. 将最终计算好的Oi写回全局内存（HBM）
            write_output(O, Oi, start_m, end_m, d, ...);
        }
        ```

    3. 关键子函数优化

        * load_q_block / load_kv_block：使用向量化内存访问（如 float4 或 LDG.128）以最大化内存带宽。

        * compute_scores：

            * 对于 d 较小（<=64），可使用循环展开，每个线程负责计算多个点积。

            * 对于支持 Tensor Core 的 GPU（如 Volta+），使用 WMMA（Warp Matrix Multiply Accumulate）API 进行计算，这是获得极致性能的关键。

        * Softmax 更新：这部分逻辑需要仔细处理数值稳定性，且要尽可能在 Warp 级别进行规约（如使用 warpReduceMax， warpReduceSum）。

    **反向传播实现**

    反向传播遵循同样的分块原则，但更复杂。由于前向传播没有存储完整的 N x N 注意力矩阵 P，反向传播需要：

    * 重新计算：利用存储的 Q, K, V, O, l, m，重新按块计算注意力分数 S_ij 和概率 P_ij。

    * 分块梯度计算：计算 dQ, dK, dV 时，也采用分块策略，将梯度累加到正确的块上。

    核心挑战：反向传播需要访问前向传播中 Q_i 和 K_j 的所有组合，因此同样需要双重循环。但其计算模式是：对于每个 Q_i 的梯度 dQ_i，需要遍历所有 K_j 块；反之，对于每个 K_j 的梯度 dK_j，需要遍历所有 Q_i 块。这被称为“类似外层产品”的访问模式。

    现代实现建议

    除非你是为了深入研究和学习，不建议从零开始编写生产级别的 Flash Attention 算子。原因如下：

    * 极度复杂：需要考虑不同硬件（Ampere, Hopper）、不同数据类型（FP16, BF16, FP8）、不同序列长度和掩码模式。

    * 已有成熟库：

        * 官方实现：FlashAttention <https://github.com/Dao-AILab/flash-attention> 库提供了高度优化的 CUDA 实现，并与 PyTorch 集成。

        * xFormers：Meta 的 xFormers 库也集成了 Flash Attention 及其变体。

        * PyTorch 集成：PyTorch 2.0+ 已将 Flash Attention 作为 torch.nn.functional.scaled_dot_product_attention 的一个后端。

    如果你想学习实现：

    * 从 FlashAttention 官方仓库的代码开始阅读，重点关注 src/flash_attention 目录。

    * 先理解其 CPU 参考实现（Python/NumPy），它清晰地展示了分块和增量 Softmax 的算法逻辑。

    * 再研究 CUDA 内核的实现，重点关注其内存加载、WMMA 使用和 Warp 级规约。

    总结：算子编写要点

    * 以内存IO为中心设计：算法性能瓶颈在于 HBM 访问，而非浮点计算（FLOPS）。

    * 最大化共享内存使用：精心设计共享内存布局，用于缓存输入块和中间结果。

    * 利用硬件特性：如 Tensor Cores（WMMA）、向量化加载/存储、Warp 级操作。

    * 增量更新与重缩放：这是算法正确性的核心，需仔细推导 Softmax 的增量更新公式。

    * 分块与循环重排：将标准的 softmax(QK^T)V 计算，通过分块和双重循环，转化为内存友好的形式。

    实现一个高性能的 Flash Attention 算子是在 CUDA 编程、GPU 架构和数值计算方面的一次终极挑战。建议结合论文、官方代码和详细的 CUDA 文档进行深入学习。

* 简述 matlab gpuarray 的用法

    gpuArray 是 MATLAB 中用于 GPU 并行计算的核心工具，它将数据从 CPU 内存传输到 GPU 显存，从而利用 GPU 进行并行计算加速。以下是其主要用法：

    1. 基本操作

        创建 gpuArray

        ```matlab
        % 将现有数据上传到 GPU
        A = rand(1000, 'single');  % 单精度浮点数更适合 GPU
        gpuA = gpuArray(A);

        % 直接在 GPU 上创建数据
        gpuB = rand(1000, 'gpuArray');          % 双精度
        gpuC = rand(1000, 'single', 'gpuArray'); % 单精度

        % 其他初始化函数
        gpuOnes = ones(1000, 'gpuArray');
        gpuZeros = zeros(1000, 'gpuArray');
        gpuEye = eye(1000, 'gpuArray');
        ```

        检查 GPU 可用性

        ```matlab
        gpuDeviceCount()      % 查看 GPU 数量
        d = gpuDevice()       % 查看当前 GPU 信息
        gpuAvailable = gpuDeviceCount() > 0  % 判断 GPU 是否可用
        ```

    2. 运算规则

        支持的操作

        * 大多数内置函数和运算符自动支持：

            ```matlab
            gpuA = gpuArray(rand(1000));
            gpuB = gpuArray(rand(1000));

            % 自动在 GPU 上执行
            C = gpuA * gpuB;      % 矩阵乘法
            D = sin(gpuA);        % 逐元素运算
            E = fft(gpuA);        % 快速傅里叶变换
            ```

        混合运算

        * 允许：gpuArray 与标量/小矩阵的混合运算（小矩阵会自动上传）

        * 不允许：gpuArray 与大型 CPU 数组的直接运算

    3. 数据传输

        ```matlab
        % CPU → GPU
        cpuData = rand(100);
        gpuData = gpuArray(cpuData);

        % GPU → CPU（显式操作）
        resultGPU = gpuData * 2;
        resultCPU = gather(resultGPU);  % 关键函数：将数据取回 CPU

        % 错误示例
        % cpuResult = double(gpuData);  % 不会自动传输数据！
        ```

    4. 常用函数支持

        数学运算

        ```matlab
        abs, sin, exp, log, sqrt  % 逐元素运算
        sum, mean, max, min       % 规约操作（支持维度参数）
        fft, ifft, filter         % 信号处理
        ```

        线性代数

        ```matlab
        mtimes (*)    % 矩阵乘法
        inv, pinv     % 矩阵求逆
        eig, svd      % 特征值/奇异值分解（部分支持）
        ```

        数组操作

        ```matlab
        reshape, permute, transpose
        cat, repmat, bsxfun
        ```

    5. 编程技巧

        预分配 GPU 内存

        ```matlab
        gpuArray.zeros(1000, 'single');  % 避免多次内存分配
        ```

        使用 arrayfun 自定义函数

        ```matlab
        % 定义标量函数
        func = @(x, y) x.^2 + sin(y);

        % 应用于 gpuArray
        gpuX = gpuArray(rand(1000));
        gpuY = gpuArray(rand(1000));
        gpuZ = arrayfun(func, gpuX, gpuY);  % 在 GPU 上并行执行
        ```

        性能优化

        ```matlab
        % 1. 使用单精度（速度更快，显存占用减半）
        gpuSingle = rand(1000, 'single', 'gpuArray');

        % 2. 避免 CPU-GPU 频繁传输
        % 错误做法：在循环内反复调用 gather()
        % 正确做法：所有计算完成后一次性 gather()

        % 3. 检查内核执行时间
        tic; gpuResult = fft(gpuData); wait(gpuDevice); toc;
        % wait() 确保计时准确
        ```

    6. 限制与注意事项

        * 显存限制：数据大小不能超过 GPU 显存

        * 函数支持：并非所有 MATLAB 函数都支持 gpuArray（查看文档确认）

        * 精度差异：GPU 计算结果可能与 CPU 有微小差异

        * 数据类型：大多数操作需要相同类型的 gpuArray

    7. 简单示例

        ```matlab
        % 示例：矩阵乘法加速比较
        N = 2000;

        % CPU 计算
        A = rand(N);
        B = rand(N);
        tic; C_cpu = A * B; t_cpu = toc;

        % GPU 计算
        gpuA = gpuArray(single(A));  % 转为单精度
        gpuB = gpuArray(single(B));
        tic; gpuC = gpuA * gpuB; wait(gpuDevice); t_gpu = toc;

        % 取回结果并比较
        C_gpu = gather(gpuC);

        fprintf('CPU: %.3f s, GPU: %.3f s, 加速比: %.2f\n', ...
                t_cpu, t_gpu, t_cpu/t_gpu);
        ```

    重要提示

    * 使用前确保安装正确版本的 GPU 驱动和 CUDA Toolkit

    * 小规模计算可能因数据传输开销而变慢

    * 使用 gpuDevice 管理 GPU 内存（reset(gpuDevice) 可清空显存）

    通过合理使用 gpuArray，MATLAB 程序通常可获得数倍到数十倍的加速效果，特别适合大规模数值计算和矩阵运算。
