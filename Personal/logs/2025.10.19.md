* 二元函数

    设$D$是$\mathbf R^2$的一个非空子集，称映射$f: D \to \mathbf R$为定义在$D$上的二元函数，通常记为

    $$z = f(x, y),\ (x, y) \in D$$

    或

    $$z = f(P),\ P \in D$$

    其中点集$D$称为该函数的定义域，$x$, $y$称为自变量，$z$称为因变量。

    example:

    * $z = ax + by + c$是一张平面

    * $z = x^2 + y^2$是旋转抛物面

    * $z = \sin(xy)$

    隐函数方程：

    $F(x, y, z) = 0$

    example:

    * 球面：$x^2 + y^2 + z^2 - 1 = 0$

    * $e^z + xyz = 0$

* 多元函数

    把二元函数定义中的平面点集$D$换成 n 维空间$\mathbf R^n$内的点集$D$，映射$f: D \to \mathbf R$就称为定义在$D$上的 n 元函数，通常记为

    $$u = f(x_1, x_2, \cdots, x_n), \ (x_1, x_2, \cdots, x_n) \in D$$

    或简讯为

    $$u = f(\boldsymbol x), \ \boldsymbol x = (x_1, x_2, \cdots, x_n) \in D$$

* `EXIT_FAILURE`

    EXIT_FAILURE 定义在 C 标准库头文件 <stdlib.h> 中，用于表示程序异常终止的退出状态。

    syntax:

    ```c
    #include <stdlib.h>

    // 典型定义示例
    #define EXIT_FAILURE 1    // 常见的实现值
    #define EXIT_SUCCESS 0    // 成功的退出状态
    ```

    example:

    ```c
    #include <stdlib.h>

    // void exit(int status);  // status 可以是 EXIT_SUCCESS 或 EXIT_FAILURE

    #include <stdlib.h>

    void cleanup(void) {
        printf("执行清理操作...\n");
    }

    int main() {
        atexit(cleanup);  // 注册退出时执行的函数
        
        if (some_error_condition) {
            exit(EXIT_FAILURE);  // 退出时会自动调用 cleanup()
        }
        
        return EXIT_SUCCESS;
    }
    ```

* qemu edu 写入状态寄存器`0x20`

    ```c
    int edu_open(struct inode *, struct file *) {
        pr_info("in edu_open()...\n");
        pr_info("base_addr_bar0: %p\n", base_addr_bar0);
        iowrite32(0x80, base_addr_bar0 + 0x20);
        uint32_t val = ioread32(base_addr_bar0 + 0x20);
        pr_info("val: %d\n", val);
        return 0;
    }
    ```

    output:

    ```
    [ 1275.443425] in edu_open()...
    [ 1275.443437] base_addr_bar0: 00000000a6ddd184
    [ 1275.443538] val: 128
    [ 1275.443577] in edu_read()...
    [ 1275.443614] in edu_release()...
    ```

* MMIO 的缓存问题

    在 MMIO 区域，刚写入就读取不会读到未写入的值，因为在映射 MMIO 区域时，通常会标记为非缓存（Uncached）。另外编译器在处理 iowrite() / ioread() 相关的函数时，会不进行优化，防止乱序执行。

* Natural language processing (NLP) 常见的任务

    * Automatic Text Generation: Deep learning model can learn the corpus of text and new text like summaries, essays can be automatically generated using these trained models.

    * Language translation: Deep learning models can translate text from one language to another, making it possible to communicate with people from different linguistic backgrounds. 

    * Sentiment analysis: Deep learning models can analyze the sentiment of a piece of text, making it possible to determine whether the text is positive, negative or neutral.

    * Speech recognition: Deep learning models can recognize and transcribe spoken words, making it possible to perform tasks such as speech-to-text conversion, voice search and voice-controlled devices. 

* Batch Normalization

    Batch Normalization (BN) is a critical technique in the training of neural networks, designed to address issues like vanishing or exploding gradients during training.

    Batch Normalization(BN) is a popular technique used in deep learning to improve the training of neural networks by normalizing the inputs of each layer.

    How Batch Normalization works?

    1. During each training iteration (epoch), BN takes a mini batch of data and normalizes the activations (outputs) of a hidden layer. This normalization transforms the activations to have a mean of 0 and a standard deviation of 1.

    2. While normalization helps with stability, it can also disrupt the network's learned features. To compensate, BN introduces two learnable parameters: gamma and beta. Gamma rescales the normalized activations, and beta shifts them, allowing the network to recover the information present in the original activations.

    It ensures that each element or component is in the right proportion before distributing the inputs into the layers and each layer is normalized before being passed to the next layer.

    PyTorch provides the nn.BatchNormXd module (where X is 1 for 1D data, 2 for 2D data like images, and 3 for 3D data) for convenient BN implementation.

    example:

    ```py
    # Define your neural network architecture with batch normalization
    class MLP(nn.Module):
        def __init__(self):
            super().__init__()
            self.layers = nn.Sequential(
                nn.Flatten(),                   # Flatten the input image tensor
                nn.Linear(28 * 28, 64),         # Fully connected layer from 28*28 to 64 neurons
                nn.BatchNorm1d(64),             # Batch normalization for stability and faster convergence
                nn.ReLU(),                      # ReLU activation function
                nn.Linear(64, 32),              # Fully connected layer from 64 to 32 neurons
                nn.BatchNorm1d(32),             # Batch normalization for stability and faster convergence
                nn.ReLU(),                      # ReLU activation function
                nn.Linear(32, 10)               # Fully connected layer from 32 to 10 neurons (for MNIST classes)
            )

        def forward(self, x):
            return self.layers(x)
    ```

    BN 放在 ReLU 之前和之后的区别：

    * BN 在 ReLU 之前（更常见的情况）：

        * 数据分布更对称

            ```py
            # BN 先将输入规范化为 ~N(0,1)
            # 这样 ReLU 激活时，约50%的神经元会被激活
            normalized = BN(linear_output)  # ~N(0,1)
            activated = ReLU(normalized)    # 一半为0，一半为正
            ```

        * 避免ReLU的Dead Neuron问题

            如果某些神经元输出总是负值，ReLU会使其完全失活, BN先进行归一化，减少这种情况

        * 与原始论文一致

            Batch Normalization 原始论文推荐放在激活函数之前

    * BN在ReLU之后:

        * 激活值直接归一化

            ```py
            activated = ReLU(linear_output)  # 都是非负数
            normalized = BN(activated)       # 归一化非负分布
            ```

        * 直接对激活后的值进行归一化, 可能在某些情况下更稳定

    两种顺序性能差异通常很小，可能因网络架构、数据集而异。


    对于某些激活函数，比如 Sigmoid/Tanh，顺序可能更重要，BN 在前可以防止饱和。对于 Leaky ReLU：两种顺序差异可能更小

    BN 可能有害的情况:

    1. 小批量大小（Small Batch Size）

        ```py
        # 当 batch_size 很小时
        batch_size = 2  # 或者 4, 8
        nn.BatchNorm1d(64)  # 这时候BN的统计估计不可靠，可能损害性能
        ```

    2. RNN/LSTM 等序列模型

        在RNN中BN很难用，通常用LayerNorm代替, 因为序列长度变化，BN统计不稳定, 数据分布一直在变，BN的running stats跟不上

    3. 噪声敏感的任务

        在一些对噪声敏感的任务中, BN引入的随机性（来自batch统计）可能有害

    4. 某些生成模型

        GANs中BN有时会导致模式崩溃, 很多现代GAN用LayerNorm或InstanceNorm代替

    BN 更好用的情况：

    - 大型数据集（ImageNet等）
    - 足够大的batch_size（32+）
    - 卷积网络/MLP
    - 稳定的数据分布

    Benefits of Batch Normalization

    * Faster Convergence: By stabilizing the gradients, BN allows you to use higher learning rates, which can significantly speed up training.
    
    * Reduced Internal Covariate Shift: As the network trains, the distribution of activations within a layer can change (internal covariate shift). BN helps mitigate this by normalizing activations before subsequent layers, making the training process less sensitive to these shifts.

    * Initialization Insensitivity: BN makes the network less reliant on the initial weight values, allowing for more robust training and potentially better performance.

* `devm_platform_ioremap_resource()`

    将平台设备（platform device）的指定内存资源映射到内核虚拟地址空间，并自动进行资源管理和错误处理。

    syntax:

    ```c
    void __iomem *devm_platform_ioremap_resource(struct platform_device *pdev, unsigned int index);
    ```

    example:

    ```c
    struct resource *res;
    void __iomem *base;

    // 传统方式需要手动管理
    res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
    base = devm_ioremap_resource(&pdev->dev, res);

    // 使用devm_platform_ioremap_resource简化
    base = devm_platform_ioremap_resource(pdev, 0);
    ```

* `grep -E`

    主要特点：

    * 支持扩展正则语法：可以使用 |, +, ?, {} 等元字符而无需转义

    * 等同于 egrep：grep -E 与 egrep 命令功能相同

    * 更强大的模式匹配：相比基本正则表达式，提供更丰富的模式匹配能力

    `grep -E "keyword1|keyword2|keyword3" file.txt`: 在 file.txt 文件中搜索包含 keyword1 或 keyword2 或 keyword3 任意一个关键词的所有行。

    ```bash
    # 使用基本正则表达式（需要转义 |）
    grep "keyword1\|keyword2\|keyword3" file.txt

    # 使用扩展正则表达式（更简洁）
    grep -E "keyword1|keyword2|keyword3" file.txt
    ```

    `|`前后不能有空格，如果有空格，那么空格也会被匹配，是 keyword 的一部分。

* python 中 is 关键字用于身份比较（identity comparison），它检查两个变量是否引用内存中的同一个对象。

    example:

    ```py
    # 比较两个变量是否指向同一个对象
    a = [1, 2, 3]
    b = a  # b 和 a 指向同一个列表对象
    c = [1, 2, 3]  # c 指向一个新的列表对象

    print(a is b)  # True - 同一个对象
    print(a is c)  # False - 值相同但不是同一个对象

    # 与 None 的比较（推荐用法）
    x = None
    print(x is None)  # True
    print(x is not None)  # False
    ```

    * is 与 == 的区别:

        ```py
        # is: 身份比较（是否同一个对象）
        # ==: 值比较（值是否相等）

        a = ''
        b = ''

        print(a == b)  # True - 值相等
        print(a is b)  # 可能为 True 或 False，取决于字符串驻留
        ```

    * 小整数和字符串驻留

        Python 会对小整数和某些字符串进行驻留优化：

        ```py
        # 小整数（-5 到 256）会被缓存
        a = 100
        b = 100
        print(a is b)  # True

        # 空字符串通常也会被驻留
        a = ''
        b = ''
        print(a is b)  # True（在大多数实现中）
        ```

    * 正确的 None 比较方式

        ```py
        # 推荐：使用 is 来比较 None
        if x is None:
            print("x is None")

        # 不推荐：使用 == 来比较 None
        if x == None:  # 能工作，但不推荐
            print("x == None")
        ```

* py 中，open file 时`a+`表示追加并且可读，只有`a`表示追加，但是读取文件时会报错

    example:

    * 只可追加，不可读

        ```py
        with open('test_doc.txt', "a") as f:
            content = f.read()
        print(content)
        ```

        output:

        ```
        Traceback (most recent call last):
          File "/home/hlc/Documents/Projects/python_test/main_2.py", line 2, in <module>
            content = f.read()
                      ^^^^^^^^
        io.UnsupportedOperation: not readable
        ```

    * 既可追加，又可读

        ```py
        with open('test_doc.txt', "a+") as f:
            content = f.read()
        print('first read:')
        print(content)

        with open('test_doc.txt', "a+") as f:
            f.seek(0)
            content = f.read()
        print('second read:')
        print(content)
        ```

        output:

        ```
        first read:

        second read:
        你好
        世界
        nihao
        zaijian
        ```

        可以看到，第一次读文件时，没有内容。因为`a+`模式，默认当前位置在文件末尾。

* 稀疏矩阵乘法

    加速算法简述（以 CSR x CSC 为例）：

    1. 外层循环：遍历矩阵A的每一行 i（利用CSR的 row_ptr）。

    2. 中层循环：对于A的第 i 行，遍历该行的每一个非零元素 A(i,k)（利用CSR的 col_indices 和 values）。这个 k 是A的列号，同时也是B的行号。

    3. 内层循环：对于每个 k，找到矩阵B的第 k 行（即CSC格式下的第 k 列）。遍历B的第 k 行上的每一个非零元素 B(k,j)（利用CSC的 row_indices 和 values）。

    4. 累加：将乘积 A(i,k) * B(k,j) 累加到结果矩阵 C(i,j) 上。

    我们只处理那些可能产生非零结果的计算。