* 二元函数

    设$D$是$\mathbf R^2$的一个非空子集，称映射$f: D \to \mathbf R$为定义在$D$上的二元函数，通常记为

    $$z = f(x, y),\ (x, y) \in D$$

    或

    $$z = f(P),\ P \in D$$

    其中点集$D$称为该函数的定义域，$x$, $y$称为自变量，$z$称为因变量。

    example:

    * $z = ax + by + c$是一张平面

    * $z = x^2 + y^2$是旋转抛物面

    * $z = \sin(xy)$

    隐函数方程：

    $F(x, y, z) = 0$

    example:

    * 球面：$x^2 + y^2 + z^2 - 1 = 0$

    * $e^z + xyz = 0$

* 多元函数

    把二元函数定义中的平面点集$D$换成 n 维空间$\mathbf R^n$内的点集$D$，映射$f: D \to \mathbf R$就称为定义在$D$上的 n 元函数，通常记为

    $$u = f(x_1, x_2, \cdots, x_n), \ (x_1, x_2, \cdots, x_n) \in D$$

    或简讯为

    $$u = f(\boldsymbol x), \ \boldsymbol x = (x_1, x_2, \cdots, x_n) \in D$$

* `EXIT_FAILURE`

    EXIT_FAILURE 定义在 C 标准库头文件 <stdlib.h> 中，用于表示程序异常终止的退出状态。

    syntax:

    ```c
    #include <stdlib.h>

    // 典型定义示例
    #define EXIT_FAILURE 1    // 常见的实现值
    #define EXIT_SUCCESS 0    // 成功的退出状态
    ```

    example:

    ```c
    #include <stdlib.h>

    // void exit(int status);  // status 可以是 EXIT_SUCCESS 或 EXIT_FAILURE

    #include <stdlib.h>

    void cleanup(void) {
        printf("执行清理操作...\n");
    }

    int main() {
        atexit(cleanup);  // 注册退出时执行的函数
        
        if (some_error_condition) {
            exit(EXIT_FAILURE);  // 退出时会自动调用 cleanup()
        }
        
        return EXIT_SUCCESS;
    }
    ```

* qemu edu 写入状态寄存器`0x20`

    ```c
    int edu_open(struct inode *, struct file *) {
        pr_info("in edu_open()...\n");
        pr_info("base_addr_bar0: %p\n", base_addr_bar0);
        iowrite32(0x80, base_addr_bar0 + 0x20);
        uint32_t val = ioread32(base_addr_bar0 + 0x20);
        pr_info("val: %d\n", val);
        return 0;
    }
    ```

    output:

    ```
    [ 1275.443425] in edu_open()...
    [ 1275.443437] base_addr_bar0: 00000000a6ddd184
    [ 1275.443538] val: 128
    [ 1275.443577] in edu_read()...
    [ 1275.443614] in edu_release()...
    ```

* MMIO 的缓存问题

    在 MMIO 区域，刚写入就读取不会读到未写入的值，因为在映射 MMIO 区域时，通常会标记为非缓存（Uncached）。另外编译器在处理 iowrite() / ioread() 相关的函数时，会不进行优化，防止乱序执行。

* Natural language processing (NLP) 常见的任务

    * Automatic Text Generation: Deep learning model can learn the corpus of text and new text like summaries, essays can be automatically generated using these trained models.

    * Language translation: Deep learning models can translate text from one language to another, making it possible to communicate with people from different linguistic backgrounds. 

    * Sentiment analysis: Deep learning models can analyze the sentiment of a piece of text, making it possible to determine whether the text is positive, negative or neutral.

    * Speech recognition: Deep learning models can recognize and transcribe spoken words, making it possible to perform tasks such as speech-to-text conversion, voice search and voice-controlled devices. 

* Batch Normalization

    Batch Normalization (BN) is a critical technique in the training of neural networks, designed to address issues like vanishing or exploding gradients during training.

    Batch Normalization(BN) is a popular technique used in deep learning to improve the training of neural networks by normalizing the inputs of each layer.

    How Batch Normalization works?

    1. During each training iteration (epoch), BN takes a mini batch of data and normalizes the activations (outputs) of a hidden layer. This normalization transforms the activations to have a mean of 0 and a standard deviation of 1.

    2. While normalization helps with stability, it can also disrupt the network's learned features. To compensate, BN introduces two learnable parameters: gamma and beta. Gamma rescales the normalized activations, and beta shifts them, allowing the network to recover the information present in the original activations.

    It ensures that each element or component is in the right proportion before distributing the inputs into the layers and each layer is normalized before being passed to the next layer.

    PyTorch provides the nn.BatchNormXd module (where X is 1 for 1D data, 2 for 2D data like images, and 3 for 3D data) for convenient BN implementation.

    example:

    ```py
    # Define your neural network architecture with batch normalization
    class MLP(nn.Module):
        def __init__(self):
            super().__init__()
            self.layers = nn.Sequential(
                nn.Flatten(),                   # Flatten the input image tensor
                nn.Linear(28 * 28, 64),         # Fully connected layer from 28*28 to 64 neurons
                nn.BatchNorm1d(64),             # Batch normalization for stability and faster convergence
                nn.ReLU(),                      # ReLU activation function
                nn.Linear(64, 32),              # Fully connected layer from 64 to 32 neurons
                nn.BatchNorm1d(32),             # Batch normalization for stability and faster convergence
                nn.ReLU(),                      # ReLU activation function
                nn.Linear(32, 10)               # Fully connected layer from 32 to 10 neurons (for MNIST classes)
            )

        def forward(self, x):
            return self.layers(x)
    ```

    BN 放在 ReLU 之前和之后的区别：

    * BN 在 ReLU 之前（更常见的情况）：

        * 数据分布更对称

            ```py
            # BN 先将输入规范化为 ~N(0,1)
            # 这样 ReLU 激活时，约50%的神经元会被激活
            normalized = BN(linear_output)  # ~N(0,1)
            activated = ReLU(normalized)    # 一半为0，一半为正
            ```

        * 避免ReLU的Dead Neuron问题

            如果某些神经元输出总是负值，ReLU会使其完全失活, BN先进行归一化，减少这种情况

        * 与原始论文一致

            Batch Normalization 原始论文推荐放在激活函数之前

    * BN在ReLU之后:

        * 激活值直接归一化

            ```py
            activated = ReLU(linear_output)  # 都是非负数
            normalized = BN(activated)       # 归一化非负分布
            ```

        * 直接对激活后的值进行归一化, 可能在某些情况下更稳定

    两种顺序性能差异通常很小，可能因网络架构、数据集而异。


    对于某些激活函数，比如 Sigmoid/Tanh，顺序可能更重要，BN 在前可以防止饱和。对于 Leaky ReLU：两种顺序差异可能更小

    BN 可能有害的情况:

    1. 小批量大小（Small Batch Size）

        ```py
        # 当 batch_size 很小时
        batch_size = 2  # 或者 4, 8
        nn.BatchNorm1d(64)  # 这时候BN的统计估计不可靠，可能损害性能
        ```

    2. RNN/LSTM 等序列模型

        在RNN中BN很难用，通常用LayerNorm代替, 因为序列长度变化，BN统计不稳定, 数据分布一直在变，BN的running stats跟不上

    3. 噪声敏感的任务

        在一些对噪声敏感的任务中, BN引入的随机性（来自batch统计）可能有害

    4. 某些生成模型

        GANs中BN有时会导致模式崩溃, 很多现代GAN用LayerNorm或InstanceNorm代替

    BN 更好用的情况：

    - 大型数据集（ImageNet等）
    - 足够大的batch_size（32+）
    - 卷积网络/MLP
    - 稳定的数据分布

    Benefits of Batch Normalization

    * Faster Convergence: By stabilizing the gradients, BN allows you to use higher learning rates, which can significantly speed up training.
    
    * Reduced Internal Covariate Shift: As the network trains, the distribution of activations within a layer can change (internal covariate shift). BN helps mitigate this by normalizing activations before subsequent layers, making the training process less sensitive to these shifts.

    * Initialization Insensitivity: BN makes the network less reliant on the initial weight values, allowing for more robust training and potentially better performance.