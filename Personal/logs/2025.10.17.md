* python 读文件

    `read([size])`: 一次性读取整个文件内容，并将其作为一个字符串返回。

    可选的 size 参数，指定要读取的字符数（文本模式）或字节数（二进制模式）。如果不提供，则读取整个文件。

    `test_doc.txt`:

    ```
    你好
    世界
    nihao
    zaijian
    ```

    ```py
    file = 'test_doc.txt'

    with open(file) as f:
        content = f.read()  # read all characters
    print('------ test 1: read all characters ------')
    print(content)

    # open as text file
    with open(file) as f:
        content = f.read(7)  # read 7 characters
    print('------ test 2: read 7 characters ------')
    print(content)

    # open as binary file
    with open(file, 'rb') as f:
        content = f.read(7)  # read 7 bytes
    print('------ test 3: read 7 bytes ------')
    print(content)
    ```

    output:

    ```
    ------ test 1: read all characters ------
    你好
    世界
    nihao
    zaijian
    ------ test 2: read 7 characters ------
    你好
    世界
    n
    ------ test 3: read 7 bytes ------
    b'\xe4\xbd\xa0\xe5\xa5\xbd\n'
    ```

    * `readline([size])`

        一次只读取文件的一行。

        返回值：一个字符串，包含一行的内容（包括换行符 \n）。如果到达文件末尾，则返回一个空字符串。

        ```py
        file = 'test_doc.txt'

        with open(file) as f:
            line = f.readline()
            while line != '':
                print(line)
                line = f.readline()
        ```

        output:

        ```
        你好

        世界

        nihao

        zaijian
        ```

    * `readlines([hint])`

        读取整个文件，并将其作为一个列表返回，列表中的每个元素是文件中的一行（字符串）。

        可选的 hint 参数。如果指定了 hint，则读取大约 hint 个字节的行，直到读完这些字节所在的行为止，可能不会读取整个文件。

        ```py
        file = 'test_doc.txt'

        with open(file) as f:
            lines = f.readlines()
        print(lines)
        ```

        output:

        ```
        ['你好\n', '世界\n', 'nihao\n', 'zaijian']
        ```

        可以看到`\n`仍然被保留。

    * 文件对象本身是可迭代的

        迭代文件对象本身，这相当于一个“惰性”的 readline()，内存效率最高。

        ```py
        # 这是读取大文件的最佳方式
        with open('example.txt', 'r') as file:
            for line in file: # 直接遍历文件对象
                print(line, end='')
        ```

        对于非常大的文件，read() 和 readlines() 会一次性将整个文件加载到内存中，可能导致内存不足。此时，应使用 readline() 或直接迭代文件对象。

* PIL (Python Imaging Library) 显示图片

    安装：`pip install Pillow`

    ```py
    from PIL import Image

    img = Image.open('/home/hlc/Pictures/SAVE_20250313_134654.jpg')
    img.show()
    ```

    这个方法会将图片保存为一个临时文件（通常是 PNG 格式）, 然后使用操作系统默认的图片查看器打开该文件.

    在 Windows 上通常用"照片"应用，在 macOS 上用"预览"，在 Linux 上用 xdg-open

* 使用 PIL 结合 tk 显示图片

    ```py
    from PIL import Image, ImageTk
    import tkinter as tk

    file = '/home/hlc/Pictures/SAVE_20250313_134654.jpg'

    # 创建主窗口
    root = tk.Tk()
    root.title("PIL Image Display")

    # 打开图片
    img = Image.open(file)

    # 转换为 Tkinter 兼容的格式
    tk_img = ImageTk.PhotoImage(img)

    # 创建标签显示图片
    label = tk.Label(root, image=tk_img)
    label.pack()

    # 运行主循环
    root.mainloop()
    ```

* PIL 结合 matplotlib 显示图片

    ```py
    from PIL import Image
    import matplotlib.pyplot as plt

    file = '/home/hlc/Pictures/SAVE_20250313_134654.jpg'

    img = Image.open(file)
    plt.imshow(img)
    plt.axis('off')  # 不显示坐标轴
    plt.show()
    ```

* 在 jupyter 中显示 PIL 图片

    ```py
    from PIL import Image
    from IPython.display import display

    img = Image.open('example.jpg')
    display(img)  # 在 Jupyter 中直接显示
    ```

* PIL 显示 np.array 的图片

    使用 Image.fromarray()

    ```py
    from PIL import Image
    import numpy as np

    # 创建或加载numpy数组
    # 假设你的数组形状为 (height, width, channels) 或 (height, width)
    array = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)

    # 转换为PIL图像
    img = Image.fromarray(array)

    # 显示图像
    img.show()
    ```

    PIL 只接受 uint8 类型：

    ```py
    # 对于不同数据类型的处理
    # uint8 类型 (0-255)
    array_uint8 = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)
    img1 = Image.fromarray(array_uint8)

    # float 类型 (0.0-1.0)
    array_float = np.random.rand(100, 100, 3)
    # 需要转换为uint8
    array_float_uint8 = (array_float * 255).astype(np.uint8)
    img2 = Image.fromarray(array_float_uint8)
    ```

    处理灰度图像：

    ```py
    # 灰度图像 (2D数组)
    gray_array = np.random.randint(0, 255, (100, 100), dtype=np.uint8)
    gray_img = Image.fromarray(gray_array)
    gray_img.show()
    ```

    注意：

    * 数据类型：确保numpy数组的数据类型是np.uint8

    * 数值范围：RGB值应该在0-255范围内

    * 数组形状：

        * 彩色图像：(height, width, 3) 或 (height, width, 4)

        * 灰度图像：(height, width)

* `pci_alloc_dev()`

    分配并初始化一个 PCI 设备结构体。

    这个函数是内核内部使用的函数，不是给驱动开发者调用的。

* `ls -R`

    递归列出目录及其所有子目录中的内容。

    -R 是 “Recursive”（递归）的缩写。

* `ls -lS`

    以长格式列出文件，并按文件大小降序排序（从大到小）。

    -l 是 “long format” 的缩写，会显示详细信息（权限、所有者、大小、修改时间等）。

    -S 是 “Sort by size” 的缩写（注意是大写S）。

* `ls -lr`

    以反向（逆序） 方式列出文件和目录。

    -r 是 “reverse” 的缩写（注意是小写r）。

    组合使用后，它会将排序结果反转。默认情况下（没有其他排序选项），ls -l 是按文件名升序排序（a-z, 0-9），加上 -r 后就变成了降序（z-a, 9-0）。

    与其他排序选项结合时，用于反转排序顺序。例如，ls -lSr 会按文件大小升序排列（从小到大），因为 -S（大小降序）被 -r 反转了。

* 如果只有 cdev，没有 device 设备文件节点，不可以调用 cdev 绑定的 fops 驱动

* `is_pointer_v`

    `is_pointer_v<T>`检查一个类型 T 是否为指针类型，等价于`is_pointer<T>::value`。

* `SubsetRandomSampler()`

    从一个完整的数据集中，随机地选取一个子集，并且在这个子集上进行无放回地随机采样。

    syntax:

    ```py
    torch.utils.data.SubsetRandomSampler(indices, generator=None)
    ```

    参数详解

    1. indices

        类型: Sequence (序列)

        说明: 这是一个整数索引的序列，用于指定要从原始数据集中抽取哪些样本。

        详细信息:

            它可以是任何 Python 序列类型，如 list, range, numpy.array, torch.Tensor 等。

            索引对应的是原始数据集中的样本位置（从 0 开始）。

            采样器会从这些指定的索引中随机抽取，不会重复抽取同一个索引（无放回抽样）。

            索引的顺序不需要是排序的，也不需要是连续的。

    2. generator

        类型: torch.Generator

        默认值: None

        说明: 用于控制随机数生成的生成器。

        详细信息:

            如果指定了 generator，采样器将使用这个特定的生成器来进行随机打乱。

            如果为 None（默认），采样器将使用默认的随机数生成器。

            这个参数主要用于确保结果的可重现性。当你希望每次运行代码时都能得到相同的随机顺序时，可以传入一个固定种子的生成器。

        example:

        ```py
        # 使用固定种子的生成器以确保可重现性
        generator = torch.Generator().manual_seed(42)
        sampler = SubsetRandomSampler(indices, generator=generator)
        ```

    返回值

        返回一个 SubsetRandomSampler 迭代器对象。

        当在 DataLoader 中迭代时，这个采样器会按照随机顺序逐个返回 indices 中的索引。

        当遍历完所有 indices 后，一个 epoch 就结束了。

    example:

    ```py
    import torch
    from torch.utils.data import DataLoader, TensorDataset, SubsetRandomSampler

    # 1. 创建示例数据集
    data = torch.randn(10, 3)  # 10个样本，每个样本3个特征
    labels = torch.arange(10)  # 10个标签
    dataset = TensorDataset(data, labels)

    # 2. 定义要使用的索引
    indices = [2, 5, 1, 8, 3, 9]  # 只使用这6个样本

    # 3. 创建采样器（带固定生成器以确保可重现性）
    generator = torch.Generator().manual_seed(42)  # 固定随机种子
    sampler = SubsetRandomSampler(indices, generator=generator)

    # 4. 创建 DataLoader
    dataloader = DataLoader(
        dataset, 
        batch_size=2, 
        sampler=sampler,  # 使用自定义采样器
        # shuffle=True    # 注意：这里不能设置 shuffle=True！
    )

    # 5. 测试输出
    for batch_idx, (data, target) in enumerate(dataloader):
        print(f"Batch {batch_idx}:")
        print(f"  Indices: {target.tolist()}")  # 这里target正好是原始索引
        print(f"  Data shape: {data.shape}")
    ```

    使用时要避免在 DataLoader 中设置 shuffle=True。同时，通常也不指定 batch_sampler。

    example: 划分训练集和验证集

    ```py
    import torch
    from torch.utils.data import DataLoader, TensorDataset, SubsetRandomSampler

    # 1. 创建一个示例数据集 (10000个样本)
    dataset = TensorDataset(torch.randn(10000, 10), torch.randint(0, 2, (10000,)))

    # 2. 定义数据集总大小和划分比例
    dataset_size = len(dataset)
    indices = list(range(dataset_size)) # 生成 [0, 1, 2, ..., 9999] 的索引列表
    split = int(0.8 * dataset_size) # 计算划分点：8000

    # 3. 随机打乱索引，以确保划分是随机的
    torch.manual_seed(42) # 设置随机种子以保证结果可复现
    indices.shuffle() # 就地打乱索引列表

    # 4. 创建训练集和验证集的索引子集
    train_indices = indices[:split]   # 前8000个索引作为训练集
    val_indices = indices[split:]     # 后2000个索引作为验证集

    # 5. 创建 SubsetRandomSampler
    train_sampler = SubsetRandomSampler(train_indices)
    val_sampler = SubsetRandomSampler(val_indices)

    # 6. 创建对应的 DataLoader
    train_loader = DataLoader(dataset, batch_size=64, sampler=train_sampler)
    val_loader = DataLoader(dataset, batch_size=64, sampler=val_sampler)

    # 现在就可以在训练循环中使用 train_loader，在验证中使用 val_loader 了
    # for data, target in train_loader:
    #     ...
    ```

    与 Subset 的区别：

    * SubsetRandomSampler 是一个 采样器，它作用于 DataLoader 级别。DataLoader 仍然会遍历整个数据集，但采样器告诉它只从指定的索引中取数据。

    * torch.utils.data.Subset 是一个 数据集，它直接返回原始数据集的一个子集。当你使用 Subset 后，得到的就是一个全新的、更小的数据集对象。

    * 如何选择：如果你需要随机打乱，用 SubsetRandomSampler。如果你只是想静态地获取一个子集（不打乱），可以用 Subset。

* transforms.Compose 能够接收 PIL.Image 类型的对象，是因为它内部组合的各个变换（transform）都实现了对 PIL.Image 的处理逻辑。

    内部的可能实现如下：

    ```py
    # 在 torchvision/transforms/functional.py 中
    def to_tensor(pic):
        """Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.
        
        Args:
            pic (PIL Image or numpy.ndarray): Image to be converted to tensor.
        """
        if not(_is_pil_image(pic) or _is_numpy(pic)):
            raise TypeError('pic should be PIL Image or ndarray. Got {}'.format(type(pic)))
        
        if _is_pil_image(pic):
            # 处理PIL.Image的代码路径
            # ... 将PIL图像转为numpy，再转为tensor
        elif _is_numpy(pic):
            # 处理numpy数组的代码路径
            # ... 直接处理numpy数组
        
        return result
    ```

* Apply a 2D Max Pooling in PyTorch

    There are two main types of pooling used in deep learning: Max Pooling and Average Pooling.

    Max Pooling: Max Pooling selects the maximum value from each set of overlapping filters and passes this maximum value to the next layer. This helps to retain the most important feature information while reducing the size of the representation.

    Average Pooling: Average Pooling computes the average value of each set of overlapping filters, and passes this average value to the next layer. This helps to retain a more general form of the feature information, but with a reduced spatial resolution.

    Pooling is usually applied after a convolution operation and helps to reduce overfitting and improve the generalization performance of the model.