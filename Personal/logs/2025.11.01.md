* 在`fig, axes = subplots()`时，如果是一行或者一列，那么`axes`是一维的，如果是多行多列，`axes`是二维的。

* `unsqueeze()`

    在张量的指定维度上增加一个长度为 1 的维度。这个操作通常也被称为“升维”。

    syntax:

    ```py
    torch.unsqueeze(input, dim) → Tensor
    ```

    * input: 输入张量。

    * dim: 一个整数，指定在哪个位置插入新的维度。这个新维度的长度将为 1。

        dim 的取值范围是 [-input.dim()-1, input.dim()]。

        * 正索引: 从前往后数，0 表示在最前面插入。

        * 负索引: 从后往前数，-1 表示在最后一个维度之后插入。

    这是一个“视图操作”，意味着它通常不会复制底层数据，而只是改变了看待数据的“视角”，因此效率很高。

    例如：

    对于一个 3 维张量 (C, H, W)：

    * dim=0 -> 新形状为 (1, C, H, W)

    * dim=1 -> 新形状为 (C, 1, H, W)

    * dim=-1 -> 新形状为 (C, H, W, 1)

    * dim=-2 -> 新形状为 (C, H, 1, W)

* `squeeze()`

    移除所有长度为 1 的维度（或者只移除指定维度，如果其长度为 1）。

    example:

    ```py
    # 接上面的例子
    x = torch.randn(1, 4, 1, 2)
    print(f"Original shape: {x.shape}") # torch.Size([1, 4, 1, 2])

    y = x.squeeze() # 移除所有长度为1的维度
    print(f"After squeeze(): {y.shape}") # torch.Size([4, 2])

    z = x.squeeze(0) # 只移除第0维，如果其长度为1
    print(f"After squeeze(0): {z.shape}") # torch.Size([4, 1, 2])

    w = x.squeeze(2) # 只移除第2维，如果其长度为1
    print(f"After squeeze(2): {w.shape}") # torch.Size([1, 4, 2])
    ```

* 使用 venv 创建 python 虚拟环境

    ```py
    python3 -m venv myenv      # 创建虚拟环境
    source myenv/bin/activate # 激活虚拟环境
    ```

* apt instlal pyhton3-xx 安装的包的位置

    * Python 模块（纯 Python）:

        /usr/lib/python3/dist-packages/ (Ubuntu/Debian 的标准位置)

        /usr/local/lib/python3.x/dist-packages/ (某些情况)

    * Python 模块（含 C 扩展）:

        /usr/lib/python3.x/site-packages/ (某些情况)

    * 可执行脚本:

        /usr/bin/ (例如 pip, black, flake8 等命令行工具)

    * 配置文件:

        /etc/ (某些 Python 包会安装配置文件在这里)

    * 共享库:

        /usr/share/ (文档、示例文件等)

    查看具体的包：

    `dpkg -L python3-xxx`

    pip install xxx → 默认用户级安装 (~/.local/lib/python3.x/site-packages/)

* `std::is_base_of`

    在编译时检查一个类是否是另一个类的基类。

    syntax:

    ```cpp
    #include <type_traits>

    std::is_base_of<Base, Derived>::value  // 返回 bool 类型的编译时常量
    ```

    example:

    ```cpp
    #include <type_traits>
    #include <iostream>

    class Base {};
    class Derived : public Base {};
    class Other {};

    int main() {
        std::cout << std::boolalpha;
        
        // 检查继承关系
        std::cout << std::is_base_of<Base, Derived>::value << std::endl;  // true
        std::cout << std::is_base_of<Base, Other>::value << std::endl;    // false
        std::cout << std::is_base_of<Derived, Base>::value << std::endl;  // false
        
        // 类总是自己的基类
        std::cout << std::is_base_of<Base, Base>::value << std::endl;     // true
        
        return 0;
    }
    ```

    返回 std::true_type 或 std::false_type

    注意事项

    * 即使继承是 private 或 protected，std::is_base_of 也会返回 true

    * 类总是被认为是自己的基类

    * 内置类型之间没有基类关系

    * 结果在编译时确定，可用于模板元编程

* 科研的难度远远大于线性学习和非线性学习

    科研过程：编织一张知识概念网，在编织时做出猜想和预测，如果没做过实验，那么做实验验证猜想，如果猜想正确，那么继续阅读新材料，获得新概念，做出新猜想。如果猜想错误，那么需要修改或重构知识网，使得其在内部自洽时，又可以合理解释现象（猜想为什么是错的）。

    通常一个新概念会联想出许多猜想和预测，在做实验中又可能冒出新的猜想或看到新的现象。为了解释新现象，我们又需要搜索假设空间，在高维度构造出许多个假设。
    
    整个过程是网状的，节点呈指数形式爆发的。

    线性学习：查资料，跟着做一遍（验证），做笔记，做一些额外的联想或尝试，如果没时间尝试了就先缓存下来。整个过程比较简单。

    非线性学习：前面的概念可能到了后面才解释，或者根本就没有解释，中间需要我们加入大量的猜想、假设来保持思维连贯。如果能拆分成线性学习，我们可能还会把它拆分成线性任务。整个过程稍微有点难度，但是基本还是跟着学习资料走的，而且大部分问题都有标准答案。

    学习的过程有点像把鱼饵（问题）丢进湖里，等一段时间总会有鱼（答案）咬钩，而科研有点像把石头丢进湖里，除了泛起一点点涟漪，剩下的什么都没有。