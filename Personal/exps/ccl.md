* main thread

    * 如果使用 host_1(gpu_1, rdma_1) + host_2(gpu_2, rdma_2) 进行通信，那么 gpu 向 host 写入数据时，使用的是 memcpy 还是 uva？

        如果是 uva，是否可以改成 memcpy？

    * 检测 rdma, 普通网卡设备

    * 解构 transport layer

    * h100 上编译 nccl，先跑通 2 卡、4 卡

    * 写一个 nccl c 语言 app，跑通 2 卡上的 all reduce，要求可以指定卡的索引号（比如 0, 1）和 data buffer 的大小（比如 256K, 4M, 16M 等）

* `vProps`

    ```c
    struct alignas(32) ncclIbNetCommBase {
      ncclNetVDeviceProps_t vProps;
    ```

    ```c
    typedef ncclNetVDeviceProps_v10_t ncclNetVDeviceProps_t;
    ```

    ```c
    typedef struct {
      int ndevs;
      int devs[NCCL_NET_MAX_DEVS_PER_NIC_V10];
    } ncclNetVDeviceProps_v10_t;
    ```

    ```c
    #define NCCL_NET_MAX_DEVS_PER_NIC_V10 4
    ```

    每张网卡最多负责 4 个 gpu，实际负责的 gpu 数由 ndevs 决定？

    `int devs[]`里的 int 有点像 gpu 的 rank 号。

    那这里的`vProps`该如何解释？当前这个网卡负责哪几个 gpu？

* `reqs`

    ```c
    struct alignas(32) ncclIbNetCommBase {
      ncclNetVDeviceProps_t vProps;
      bool isSend;
      struct ncclIbRequest reqs[MAX_REQUESTS];
    ```

    ```c
    #define MAX_REQUESTS (NCCL_NET_MAX_REQUESTS*NCCL_NET_IB_MAX_RECVS)
    // We need to support NCCL_NET_MAX_REQUESTS for each concurrent receive

    // Expands to:

    // (32*8)
    ```

    这里的 recvs 是 4 个 recv 队列吗？这里的 4 刚好可以和前面的每个网卡服务最多 4 个 gpu 对应上。有三种可能：

    1. 4 个 recv 队列分别对应 4 个 gpu

    2. 4 个 recv 队列共享 4 个 gpu

    3. 每个 gpu 分配 4 个 recv 队列

    ```c
    struct ncclIbRequest {
      struct ncclIbNetCommBase* base;
      int type;
      struct ncclSocket* sock;
    ```

    `base`是 parent。`type`不清楚是什么意思。

    ```c
    struct ncclSocket {
      int fd;
      int acceptFd;
      int errorRetries;
      union ncclSocketAddress addr;
      volatile uint32_t* abortFlag;
      int asyncFlag;
      enum ncclSocketState state;
      int salen;
      uint64_t magic;
      enum ncclSocketType type;
      int customRetry;
      int finalizeCounter; // Used to keep track of initial handshake for async sockets.
      char finalizeBuffer[sizeof(uint64_t)]; // Used to keep track of initial handshake for async sockets.
    };
    ```

    ```c
    union ncclSocketAddress {
      struct sockaddr sa;
      struct sockaddr_in sin;
      struct sockaddr_in6 sin6;
    };
    ```

    socket 这里比较熟悉了。`errorRetries`是重试的次数？`addr`是普通的 ipv4, ipv6。`abortFlag`这个不清楚是什么意思，`volatile`指的是这个值可能由 device 填写吗？

    `asyncFlag`不清楚什么意思，异步 socket，可能和 poll / epoll 有关系？如果不使用 epoll，支持同步模式吗，比如使用 recv 阻塞等待数据？

    `customRetry`与`errorRetries`有什么区别？error retries 指的是已经尝试过重连的次数，custom retry 指的是最大重连次数？

    ```c
    enum ncclSocketType {
      ncclSocketTypeUnknown = 0,
      ncclSocketTypeBootstrap = 1,
      ncclSocketTypeProxy = 2,
      ncclSocketTypeNetSocket = 3,
      ncclSocketTypeNetIb = 4,
      ncclSocketTypeRasNetwork = 5
    };
    ```

    看起来不同用途的网络都会起一个 socket。

    ```c
    enum ncclSocketState {
      ncclSocketStateNone = 0,
      ncclSocketStateInitialized = 1,
      ncclSocketStateAccepting = 2,
      ncclSocketStateAccepted = 3,
      ncclSocketStateConnecting = 4,
      ncclSocketStateConnectPolling = 5,
      ncclSocketStateConnected = 6,
      ncclSocketStateReady = 7,
      ncclSocketStateTerminating = 8,
      ncclSocketStateClosed = 9,
      ncclSocketStateError = 10,
      ncclSocketStateNum = 11
    };
    ```

    `ncclSocketStateConnectPolling` socket 也支持 polling 吗？

* `devCommSetup()`

    调用栈：

    ```c
    ncclAsyncJobMain()

    ncclCommInitRankFunc()

    initTransportsRank()

    devCommSetup()
    ```

    ```c
    static ncclResult_t devCommSetup(ncclComm_t comm) {
      ncclResult_t ret = ncclSuccess;
      int nRanks = comm->nRanks;
      struct ncclDevCommAndChannels tmpCommAndChans;
      struct ncclDevCommAndChannels *devCommAndChans = NULL;
      struct ncclNvmlCCStatus ccStatus;
      bool ccEnable;
      cudaStream_t deviceStream;

      memset(&tmpCommAndChans, '\0', sizeof(tmpCommAndChans));
      NCCLCHECKGOTO(ncclStrongStreamAcquire(ncclCudaGraphNone(), &comm->sharedRes->deviceStream, /*concurrent=*/false, &deviceStream), ret, fail);
      NCCLCHECKGOTO(ncclCudaCallocAsync(&devCommAndChans, 1, deviceStream), ret, fail);
      ncclCommPushCudaFree(comm, devCommAndChans);
      NCCLCHECKGOTO(ncclCudaCallocAsync(&tmpCommAndChans.comm.rankToLocalRank, comm->nRanks, deviceStream), ret, fail);
      ncclCommPushCudaFree(comm, tmpCommAndChans.comm.rankToLocalRank);
      NCCLCHECKGOTO(ncclCudaMemcpyAsync(tmpCommAndChans.comm.rankToLocalRank, comm->rankToLocalRank, comm->nRanks, deviceStream), ret, fail);

      comm->devComm = &devCommAndChans->comm;
      tmpCommAndChans.comm.rank = comm->rank;
      tmpCommAndChans.comm.nRanks = nRanks;
      tmpCommAndChans.comm.node = comm->node;
      tmpCommAndChans.comm.nNodes = comm->nNodes;
      tmpCommAndChans.comm.abortFlag = comm->abortFlagDev;
      tmpCommAndChans.comm.isAllNvlink = comm->isAllNvlink;
      for (int p=0; p < NCCL_NUM_PROTOCOLS; p++) {
        tmpCommAndChans.comm.buffSizes[p] = comm->buffSizes[p];
      }
      tmpCommAndChans.comm.p2pChunkSize = comm->p2pChunkSize;
      tmpCommAndChans.comm.channels = &devCommAndChans->channels[0];
    ```

* `tmpCommAndChans.comm.rankToLocalRank`

    这里的 tmp 的目的是先修改、再写回，防止原数据失效。

    确实，看到了

    ```c
    ncclCudaMemcpyAsync(devCommAndChans, &tmpCommAndChans, 1, deviceStream)
    ```

* buffer sizes

    ```
    [0] =
    524288
    [1] =
    4915200
    [2] =
    4194304
    ```

    ```c
    #define NCCL_NUM_PROTOCOLS 3 // Simple/LL/LL128
    #define NCCL_PROTO_UNDEF -1
    #define NCCL_PROTO_LL 0
    #define NCCL_PROTO_LL128 1
    #define NCCL_PROTO_SIMPLE 2
    ```

* `comm->p2pChunkSize`, `524288`

* `ncclCommPushCudaHostFree()`

    管理CUDA主机内存的释放与NCCL通信的同步。确保在释放CUDA主机内存时，不会与正在进行的NCCL通信操作产生竞争条件。

    example:

    ```c
    // 伪代码示例
    void* cudaHostPtr;
    cudaMallocHost(&cudaHostPtr, size);  // 分配CUDA主机内存

    // 在NCCL通信中使用这个内存
    ncclSend(cudaHostPtr, count, datatype, peer, comm, stream);

    // 当需要释放内存时
    ncclCommPushCudaHostFree(comm, cudaHostPtr);  // 注册待释放的内存
    // 内存会在所有相关的NCCL操作完成后自动释放
    ```

    工作原理

    1. 延迟释放机制

        将需要释放的CUDA主机内存指针加入待释放队列

        NCCL确保所有使用该内存的通信操作完成后再实际释放

    2. 防止use-after-free

        ```cpp
        // 危险情况：
        // 操作1: ncclSend(hostPtr, ...)  // 异步发送
        // 操作2: cudaFreeHost(hostPtr)    // 立即释放 → 可能崩溃

        // 安全做法：
        // 操作1: ncclSend(hostPtr, ...)
        // 操作2: ncclCommPushCudaHostFree(comm, hostPtr)  // 安全延迟释放
        ```

    3. 与通信进度协调

        NCCL在推进通信进度的同时，检查待释放内存

        确认内存不再被任何通信操作使用后执行实际释放

* `devCommSetup`

    下面这段可能和拓扑有关系，有时间研究一下。

    ```c
      for (int c=0; c < MAXCHANNELS; c++) {
        tmpCommAndChans.channels[c].peers = comm->channels[c].devPeers;
        tmpCommAndChans.channels[c].ring = comm->channels[c].ring;
        tmpCommAndChans.channels[c].ring.userRanks = comm->channels[c].devRingUserRanks;
        tmpCommAndChans.channels[c].tree = comm->channels[c].tree;
        tmpCommAndChans.channels[c].collnetChain = comm->channels[c].collnetChain;
        tmpCommAndChans.channels[c].collnetDirect = comm->channels[c].collnetDirect;
        tmpCommAndChans.channels[c].nvls = comm->channels[c].nvls;

        if (comm->channels[c].ring.userRanks != nullptr) {
          NCCLCHECKGOTO(ncclCudaMemcpyAsync(tmpCommAndChans.channels[c].ring.userRanks, comm->channels[c].ring.userRanks, nRanks, deviceStream), ret, fail);
        }
      }
    ```

    `#define MAXCHANNELS 64`

* `comm->allocP2pNetLLBuffers`

    ```c
      // Initialize num P2P LL buffers for this communicator
      comm->allocP2pNetLLBuffers = ncclParamAllocP2pNetLLBuffers() == 1;
    ```

    值是 false.

* `allGather3Data`

    allGather3Data是一个跨进程通信的数据结构，用于在所有参与计算的GPU/进程之间收集和交换网络拓扑信息。

    具体作用

        存储网络拓扑信息

            每个rank（进程）将自己的网络拓扑信息存入本地数组

            通过AllGather操作，所有rank都能获取其他rank的拓扑信息

        收集的关键信息包括：

            pattern: 通信模式

            nChannels: 通道数量

            sameChannels: 是否使用相同通道

            bwIntra/bwInter: 节点内/节点间带宽

            typeIntra/typeInter: 节点内/节点间连接类型

            crossNic: 是否跨NIC通信

    ```c
      // AllGather3 - begin
      NCCLCHECKGOTO(ncclCalloc(&allGather3Data, nranks), ret, fail);

      for (int a=0; a<NCCL_NUM_ALGORITHMS; a++) {
        allGather3Data[rank].graphInfo[a].pattern = graphs[a]->pattern;
        allGather3Data[rank].graphInfo[a].nChannels = graphs[a]->nChannels;
        allGather3Data[rank].graphInfo[a].sameChannels = graphs[a]->sameChannels;
        allGather3Data[rank].graphInfo[a].bwIntra = graphs[a]->bwIntra;
        allGather3Data[rank].graphInfo[a].bwInter = graphs[a]->bwInter;
        allGather3Data[rank].graphInfo[a].typeIntra = graphs[a]->typeIntra;
        allGather3Data[rank].graphInfo[a].typeInter = graphs[a]->typeInter;
        allGather3Data[rank].graphInfo[a].crossNic = graphs[a]->crossNic;
      }
    ```

    理论上各个 thread 计算出来的拓扑应该是一样的才对，为什么这里又同步一遍？