# Torch Note

## cache

* ç®€è¿° pytorch ä¸­ï¼Œa.storage().data_ptr() çš„ä½œç”¨

    åœ¨ PyTorch ä¸­ï¼Œ`a.storage().data_ptr()` çš„ä½œç”¨æ˜¯**è¿”å›žå¼ é‡ `a` çš„åº•å±‚æ•°æ®å­˜å‚¨åŒºçš„å†…å­˜åœ°å€**ã€‚

    å…·ä½“è§£é‡Šå¦‚ä¸‹ï¼š

    1. **`a.storage()`**ï¼š

       - è¿”å›žå¼ é‡ `a` çš„åº•å±‚ `Storage` å¯¹è±¡

       - PyTorch çš„å¼ é‡ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼šä¸€ä¸ªå…ƒä¿¡æ¯å¤´ï¼ˆshapeã€strideã€dtype ç­‰ï¼‰å’Œä¸€ä¸ªå®žé™…çš„è¿žç»­æ•°æ®å­˜å‚¨åŒºï¼ˆStorageï¼‰

       - å¤šä¸ªå¼ é‡å¯ä»¥å…±äº«åŒä¸€ä¸ª Storageï¼ˆå¦‚åˆ‡ç‰‡æ“ä½œã€`.view()`ã€`.detach()` ç­‰ï¼‰

    2. **`.data_ptr()`**ï¼š

       - è¿”å›žè¯¥ Storage å¯¹è±¡æ‰€ç®¡ç†çš„å†…å­˜çš„èµ·å§‹åœ°å€ï¼ˆPython æ•´æ•°ï¼‰

       - è¿™æ˜¯å®žé™…å­˜å‚¨å¼ é‡æ•°æ®çš„å†…å­˜ä½ç½®

    3. **ä¸»è¦ç”¨é€”**ï¼š

       - **æ£€æŸ¥å†…å­˜å…±äº«**ï¼šä¸¤ä¸ªå¼ é‡å¦‚æžœ `storage().data_ptr()` ç›¸åŒï¼Œè¯´æ˜Žå®ƒä»¬å…±äº«åŒä¸€ä»½åº•å±‚æ•°æ®

       - **ä¸Žå¤–éƒ¨åº“äº¤äº’**ï¼šå°† PyTorch å¼ é‡çš„å†…å­˜åœ°å€ä¼ é€’ç»™ C/CUDA æ‰©å±•æˆ–å…¶ä»–åº“ï¼ˆå¦‚ NumPyã€OpenCVï¼‰

       - **è°ƒè¯•**ï¼šç¡®è®¤å†…å­˜æ˜¯å¦è¢«é‡æ–°åˆ†é…ï¼Œæˆ–éªŒè¯æ“ä½œæ˜¯åŽŸåœ°æ‰§è¡Œè¿˜æ˜¯æ–°å»ºå†…å­˜

    **ç¤ºä¾‹**ï¼š

    ```python
    import torch

    a = torch.tensor([1, 2, 3])
    print(a.storage().data_ptr())  # è¾“å‡ºå†…å­˜åœ°å€ï¼Œå¦‚ 139823456789012

    b = a[:2]  # åˆ‡ç‰‡æ“ä½œï¼Œå…±äº«å­˜å‚¨
    print(b.storage().data_ptr())  # ä¸Ž a ç›¸åŒ

    c = a.clone()  # æ·±æ‹·è´ï¼Œæ–°åˆ†é…å†…å­˜
    print(c.storage().data_ptr())  # ä¸Ž a ä¸åŒ
    ```

    **æ³¨æ„**ï¼š

    - åœ¨ CPU ä¸Šè¿”å›žçš„æ˜¯ç³»ç»Ÿå†…å­˜åœ°å€ï¼ŒCUDA å¼ é‡è¿”å›žçš„æ˜¯ GPU æ˜¾å­˜åœ°å€
    - 0.4.0 ç‰ˆæœ¬ä¹‹å‰ä¹Ÿå¸¸ç”¨ `.data_ptr()`ï¼ŒçŽ°åœ¨æ›´æŽ¨èé€šè¿‡ `storage()` è®¿é—®


* `torch.allclose()`

    torch.allclose æ˜¯ PyTorch ä¸­ç”¨äºŽåˆ¤æ–­ä¸¤ä¸ªå¼ é‡æ˜¯å¦åœ¨ç»™å®šå®¹å¿åº¦èŒƒå›´å†…æŽ¥è¿‘ç›¸ç­‰çš„å‡½æ•°ã€‚

    åŽŸåž‹

    ```python
    torch.allclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=False)
    ```

    å‚æ•°è¯´æ˜Ž

    * input (Tensor): ç¬¬ä¸€ä¸ªå¼ é‡

    * other (Tensor): ç¬¬äºŒä¸ªå¼ é‡

    * rtol (float): ç›¸å¯¹å®¹å¿åº¦ï¼Œé»˜è®¤ 1e-05

    * atol (float): ç»å¯¹å®¹å¿åº¦ï¼Œé»˜è®¤ 1e-08

    * equal_nan (bool): æ˜¯å¦å°† NaN è§†ä¸ºç›¸ç­‰ï¼Œé»˜è®¤ False

    ä½œç”¨

    åˆ¤æ–­ä¸¤ä¸ªå¼ é‡æ˜¯å¦æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼š

    ```text
    abs(input - other) â‰¤ atol + rtol * abs(other)
    ```

    å¯¹äºŽæ‰€æœ‰å…ƒç´ éƒ½æˆç«‹æ—¶è¿”å›ž Trueï¼Œå¦åˆ™è¿”å›ž Falseã€‚

    ç”¨æ³•ç¤ºä¾‹

    åŸºæœ¬ä½¿ç”¨

    ```python
    import torch

    # åˆ›å»ºä¸¤ä¸ªæŽ¥è¿‘çš„å¼ é‡
    a = torch.tensor([1.0, 2.0, 3.0])
    b = torch.tensor([1.00001, 2.00001, 3.00001])

    # åˆ¤æ–­æ˜¯å¦æŽ¥è¿‘
    result = torch.allclose(a, b, rtol=1e-4)  # True
    print(result)  # True
    ```

    è®¾ç½®å®¹å¿åº¦

    ```python
    x = torch.tensor([100.0, 200.0])
    y = torch.tensor([100.1, 200.1])

    # ç»å¯¹å®¹å¿åº¦
    result1 = torch.allclose(x, y, atol=0.2)  # True
    # ç›¸å¯¹å®¹å¿åº¦
    result2 = torch.allclose(x, y, rtol=1e-3)  # True
    ```

    NaN å¤„ç†

    ```python
    import torch

    a = torch.tensor([1.0, float('nan'), 3.0])
    b = torch.tensor([1.0, float('nan'), 3.0])

    result1 = torch.allclose(a, b)  # False (é»˜è®¤ equal_nan=False)
    result2 = torch.allclose(a, b, equal_nan=True)  # True
    ```

    ä¸Ž torch.equal çš„åŒºåˆ«

    ```python
    a = torch.tensor([1.0, 2.0])
    b = torch.tensor([1.000001, 2.000001])

    print(torch.equal(a, b))      # False (ä¸¥æ ¼ç›¸ç­‰)
    print(torch.allclose(a, b))   # True (åœ¨å®¹å¿åº¦èŒƒå›´å†…)
    ```

    åº”ç”¨åœºæ™¯

    * æ•°å€¼æ¯”è¾ƒï¼šæ¯”è¾ƒæµ®ç‚¹æ•°è®¡ç®—ç»“æžœ

    * æ¨¡åž‹æµ‹è¯•ï¼šéªŒè¯æ¨¡åž‹è¾“å‡ºæ˜¯å¦ç¬¦åˆé¢„æœŸ

    * æ¢¯åº¦æ£€æŸ¥ï¼šæ¯”è¾ƒæ¢¯åº¦å€¼æ˜¯å¦æŽ¥è¿‘

    * å•å…ƒæµ‹è¯•ï¼šéªŒè¯å¼ é‡è¿ç®—çš„æ­£ç¡®æ€§

    æ³¨æ„äº‹é¡¹

    * ä¸¤ä¸ªå¼ é‡å¿…é¡»æœ‰ç›¸åŒçš„å½¢çŠ¶

    * ä¸»è¦ç”¨äºŽæµ®ç‚¹æ•°æ¯”è¾ƒï¼Œæ•´æ•°ç±»åž‹ä¼šæ¯”è¾ƒä¸¥æ ¼

    * rtol å’Œ atol å¯ä»¥åŒæ—¶èµ·ä½œç”¨ï¼Œæ»¡è¶³å…¶ä¸­ä¸€ä¸ªæ¡ä»¶å³å¯

* pytorch ä¸­ dataloader åœ¨æ‰“åŒ… batch æ—¶ï¼Œå¦‚ä½•è¿”å›ž list ç±»åž‹ï¼Œè€Œä¸æ˜¯ tensor ç±»åž‹ï¼Ÿ

    åœ¨PyTorchçš„DataLoaderä¸­ï¼Œé»˜è®¤æƒ…å†µä¸‹ä¼šå°†batchæ•°æ®è½¬æ¢ä¸ºTensorã€‚å¦‚æžœä½ éœ€è¦è¿”å›žlistç±»åž‹è€Œä¸æ˜¯Tensorç±»åž‹ï¼Œæœ‰ä»¥ä¸‹å‡ ç§æ–¹æ³•ï¼š

    æ–¹æ³•1ï¼šä½¿ç”¨è‡ªå®šä¹‰çš„collate_fnï¼ˆæŽ¨èï¼‰

    ```python
    import torch
    from torch.utils.data import Dataset, DataLoader

    # è‡ªå®šä¹‰collate_fnï¼Œç›´æŽ¥è¿”å›žlist
    def list_collate_fn(batch):
        # batchæ˜¯ä¸€ä¸ªåŒ…å«å¤šä¸ªæ ·æœ¬çš„åˆ—è¡¨ï¼Œæ¯ä¸ªæ ·æœ¬æ˜¯__getitem__è¿”å›žçš„å…ƒç»„
        # å°†æ¯ä¸ªå­—æ®µçš„æ•°æ®æ”¶é›†åˆ°listä¸­
        transposed = zip(*batch)
        return [list(samples) for samples in transposed]

    class MyDataset(Dataset):
        def __init__(self, data):
            self.data = data
        
        def __len__(self):
            return len(self.data)
        
        def __getitem__(self, idx):
            # è¿”å›žlistè€Œä¸æ˜¯Tensor
            return [self.data[idx], self.data[idx] * 2]

    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    data = list(range(10))
    dataset = MyDataset(data)
    dataloader = DataLoader(dataset, batch_size=3, collate_fn=list_collate_fn)

    # æµ‹è¯•
    for batch in dataloader:
        print(f"Batch type: {type(batch)}, Content: {batch}")
        # batchå°†æ˜¯listç±»åž‹
    ```

    æ–¹æ³•2ï¼šåœ¨collate_fnä¸­æ›´ç²¾ç»†åœ°æŽ§åˆ¶

    ```python
    def custom_collate_fn(batch):
        """
        å¤„ç†å¤šç§æ•°æ®ç±»åž‹ï¼Œåªå¯¹æŸäº›å­—æ®µè½¬æ¢ä¸ºTensor
        """
        elem = batch[0]
        if isinstance(elem, (list, tuple)):
            # å¦‚æžœæ˜¯åˆ—è¡¨æˆ–å…ƒç»„ï¼Œé€’å½’å¤„ç†æ¯ä¸ªå­—æ®µ
            return [custom_collate_fn([d[i] for d in batch]) for i in range(len(elem))]
        elif isinstance(elem, dict):
            # å¦‚æžœæ˜¯å­—å…¸
            return {key: custom_collate_fn([d[key] for d in batch]) for key in elem}
        else:
            # å¯¹äºŽæ•°å­—ç±»åž‹ï¼Œå¯ä»¥é€‰æ‹©è½¬æ¢ä¸ºTensoræˆ–ä¿æŒåŽŸæ ·
            # return torch.tensor(batch)  # è½¬æ¢ä¸ºTensor
            return list(batch)  # ä¿æŒä¸ºlist

    # ä½¿ç”¨ç¤ºä¾‹
    dataloader = DataLoader(dataset, batch_size=3, collate_fn=custom_collate_fn)
    ```

    æ–¹æ³•3ï¼šåœ¨Datasetçš„__getitem__ä¸­å¤„ç†

    ```python
    class MyDataset2(Dataset):
        def __init__(self, data):
            self.data = data
        
        def __len__(self):
            return len(self.data)
        
        def __getitem__(self, idx):
            # è¿”å›žtupleï¼Œè®©collate_fnåˆ†åˆ«å¤„ç†
            return self.data[idx], str(self.data[idx])  # æ··åˆæ•°æ®ç±»åž‹

    def mixed_collate_fn(batch):
        # å°†batchè§£åŒ…ä¸ºä¸åŒå­—æ®µ
        field1, field2 = zip(*batch)
        
        # å¯¹field1è½¬æ¢ä¸ºTensorï¼Œfield2ä¿æŒä¸ºlist
        return torch.tensor(field1), list(field2)

    # ä½¿ç”¨
    dataloader = DataLoader(MyDataset2(data), batch_size=3, 
                           collate_fn=mixed_collate_fn)
    ```

    æ–¹æ³•4ï¼šæœ€ç®€å•çš„æ–¹æ³• - è¿”å›žPythonåŽŸç”Ÿç±»åž‹

    ```python
    # å¦‚æžœåªéœ€è¦é¿å…è‡ªåŠ¨è½¬æ¢ä¸ºTensorï¼Œæœ€ç®€å•çš„æ–¹æ³•æ˜¯ç¡®ä¿__getitem__è¿”å›žPythonåŽŸç”Ÿç±»åž‹
    class SimpleDataset(Dataset):
        def __getitem__(self, idx):
            # è¿”å›žPythonåŽŸç”Ÿç±»åž‹ï¼Œè€Œä¸æ˜¯Tensor
            return float(self.data[idx])  # è€Œä¸æ˜¯ torch.tensor(self.data[idx])
    ```

    ç¤ºä¾‹ï¼šå¤„ç†å›¾åƒå’Œæ ‡ç­¾

    ```python
    from PIL import Image
    import numpy as np

    class ImageDataset(Dataset):
        def __init__(self, image_paths, labels):
            self.image_paths = image_paths
            self.labels = labels
        
        def __getitem__(self, idx):
            # åŠ è½½å›¾åƒä½†ä¸è½¬æ¢ä¸ºTensor
            img = Image.open(self.image_paths[idx])
            img = np.array(img)  # ä¿æŒä¸ºnumpyæ•°ç»„
            label = self.labels[idx]
            
            return img, label

    def image_collate_fn(batch):
        images, labels = zip(*batch)
        
        # å›¾åƒä¿æŒä¸ºlistï¼Œæ ‡ç­¾è½¬æ¢ä¸ºTensorï¼ˆå¯é€‰ï¼‰
        return list(images), torch.tensor(labels)

    # ä½¿ç”¨
    dataloader = DataLoader(dataset, batch_size=32, 
                           collate_fn=image_collate_fn)
    ```

    æ³¨æ„äº‹é¡¹ï¼š

    * æ€§èƒ½è€ƒè™‘ï¼šä½¿ç”¨listè€Œä¸æ˜¯Tensorå¯èƒ½ä¼šé™ä½Žæ€§èƒ½ï¼Œå› ä¸ºPyTorchçš„Tensoræ“ä½œç»è¿‡äº†ä¼˜åŒ–

    * GPUåŠ é€Ÿï¼šå¦‚æžœè¦åœ¨GPUä¸Šè®­ç»ƒï¼Œæœ€ç»ˆéœ€è¦è½¬æ¢ä¸ºTensor

    * çµæ´»æ€§ï¼šcollate_fnæä¾›äº†æœ€å¤§çš„çµæ´»æ€§ï¼Œå¯ä»¥æ ¹æ®éœ€è¦å¤„ç†ä¸åŒç±»åž‹çš„æ•°æ®

    æŽ¨èä½¿ç”¨æ–¹æ³•1æˆ–æ–¹æ³•2ï¼Œé€šè¿‡è‡ªå®šä¹‰collate_fnå‡½æ•°æ¥æŽ§åˆ¶è¿”å›žç±»åž‹ã€‚è¿™æ˜¯æœ€çµæ´»å’Œæ¸…æ™°çš„æ–¹å¼ã€‚

* `tensor.detach()`

    ä½œç”¨

    * æ–­å¼€è®¡ç®—å›¾ï¼šè¿”å›žä¸€ä¸ªæ–°å¼ é‡ï¼Œä¸ŽåŽŸå§‹å¼ é‡å…±äº«æ•°æ®ä½†ä¸å‚ä¸Žæ¢¯åº¦è®¡ç®—

    * é˜»æ­¢æ¢¯åº¦å›žä¼ ï¼šåœ¨åå‘ä¼ æ’­æ—¶ï¼Œä»Žè¯¥å¼ é‡å¼€å§‹çš„è¿ç®—ä¸ä¼šè¢«è¿½è¸ªæ¢¯åº¦

    * å†…å­˜å…±äº«ï¼šä¸å¤åˆ¶æ•°æ®ï¼Œä»…åˆ›å»ºæ–°çš„å¼•ç”¨

    å…¸åž‹åº”ç”¨åœºæ™¯

    * GANè®­ç»ƒä¸­åˆ†ç¦»ä¸åŒç½‘ç»œçš„æ¢¯åº¦è®¡ç®—

    * åœ¨è®¡ç®—æŸå¤±æ—¶å†»ç»“éƒ¨åˆ†å‚æ•°

    * å°†å¼ é‡è½¬æ¢ä¸ºnumpyæ•°ç»„å‰

    * æ¨¡åž‹æŽ¨ç†æ—¶å‡å°‘å†…å­˜å ç”¨

* torch åˆ†çº§ä¼˜åŒ–å‚æ•°

    ```py
    import torch.nn as nn
    import torch.optim as optim

    model = nn.Sequential(
        nn.Linear(10, 20),
        nn.ReLU(),
        nn.Linear(20, 1)
    )

    # å‡è®¾æˆ‘ä»¬æƒ³å¯¹ç½‘ç»œä¸åŒéƒ¨åˆ†ä½¿ç”¨ä¸åŒçš„å­¦ä¹ çŽ‡
    backbone_params = list(model[0].parameters())  # ç¬¬ä¸€å±‚å‚æ•°
    head_params = list(model[2].parameters())      # ç¬¬ä¸‰å±‚å‚æ•°

    # ä¸åŒå‚æ•°ç»„ä½¿ç”¨ä¸åŒçš„å­¦ä¹ çŽ‡
    optimizer = optim.SGD([
        {'params': backbone_params, 'lr': 1e-4},  # åŸºç¡€å±‚å­¦ä¹ çŽ‡å°
        {'params': head_params, 'lr': 1e-3}       # å¤´éƒ¨å±‚å­¦ä¹ çŽ‡å¤§
    ])

    # æˆ–è€…ï¼šåªä¼˜åŒ–æœ€åŽä¸€å±‚
    optimizer = optim.SGD(model[2].parameters(), lr=1e-3)
    ```

* `optim.SGD([train_param], lr=1e-3)`

    æˆ‘ä»¬å¯ä»¥æŠŠæƒ³ä¼˜åŒ–çš„æ•°æ®åšæˆ listï¼Œsgd å¯ä»¥åªä¼˜åŒ– list ä¸­çš„å¯¹è±¡ã€‚

    example:

    Y = A x + b

    æˆ‘ä»¬å¯ä»¥å†™æˆ`optim.SGD([A[0], A[3]], lr=1e-3)`ï¼Œè¿™æ ·å¯ä»¥åªä¼˜åŒ–`A[0]`, `A[3]`è¿™ä¸¤ä¸ªæ•°å­—ã€‚

    åœ¨è¿›è¡Œ layer A -> layer B çš„åå‘ä¼ æ’­ï¼Œè®¡ç®— A ä¸­å‚æ•°çš„æ¢¯åº¦æ—¶ï¼Œä¸éœ€è¦ç”¨åˆ° layer B ä¸­å‚æ•°çš„**æ¢¯åº¦**ï¼Œä½†æ˜¯æœ‰å¯èƒ½ç”¨åˆ° B ä¸­å‚æ•°çš„**å€¼**ã€‚

    åœ¨æ•´ä»‹ç¥žç»ç½‘ç»œä¸­ï¼Œåªè¦æŸä¸€å±‚çš„è¾“å‡ºå…¨ä¸ºæ’å®šå€¼ï¼ˆæ¯”å¦‚ Y = A x + b ä¸­ï¼ŒA = 0ï¼Œæ­¤æ—¶ Y æ’ç­‰äºŽ bï¼‰ï¼Œé‚£ä¹ˆæ•´ä¸ªç¥žç»ç½‘ç»œçš„è¾“å‡ºä»¥åŠæ¢¯åº¦æµå°±ä¼šåœ¨è¿™é‡Œè¢«æˆªæ–­ï¼Œæ¢¯åº¦ä¼˜åŒ–ä¼šå¤±æ•ˆã€‚

    åªè¦ç¥žç»ç½‘çµ¡çš„æ¯ä¸€å±‚è¾“å‡ºéƒ½è‡³å°‘å­˜åœ¨ä¸€ä¸ªå€¼éš input x çš„å˜åŒ–è€Œå˜åŒ–ï¼Œé‚£ä¹ˆæ¢¯åº¦ä¼˜åŒ–ä»ç„¶å¥æ•ˆã€‚

    è¿™æ„å‘³ç€ freeze ç¥žç»ç½‘ç»œä¸­å…¶ä¸­æŸä¸€å±‚ï¼ˆæˆ–å‡ å±‚ï¼‰çš„æ¢¯åº¦ï¼ˆ`requires_grad = False`ï¼‰ï¼Œæˆ–è€…åœ¨è°ƒç”¨ sgd æ—¶ï¼Œåªä¼ å…¥æŒ‡å®šå‡ å±‚ layer çš„æ¢¯åº¦ï¼ˆ`optim.SGD([layer_1_param, layer_3_param])`ï¼‰ï¼Œæ•´ä½“ä»æ˜¯ work çš„ã€‚

* è™šæ‹Ÿæ‹¼æŽ¥

    ä½ è¯´å¾—éžå¸¸å¯¹ï¼è¿™æ­£æ˜¯çŽ°ä»£æ·±åº¦å­¦ä¹ æ¡†æž¶æ­£åœ¨æŽ¢ç´¢çš„ä¼˜åŒ–æ–¹å‘ã€‚ä½ æè¿°çš„è¿™ç§è®¾è®¡å®žé™…ä¸Šæ¶‰åŠåˆ° "è™šæ‹Ÿæ‹¼æŽ¥" æˆ– "å»¶è¿Ÿæ‹¼æŽ¥" çš„æ¦‚å¿µã€‚

    ç†æƒ³çš„æŒ‡é’ˆå¼ cat è®¾è®¡

    1. æ¦‚å¿µå®žçŽ°ï¼ˆä¼ªä»£ç ï¼‰

        ```python
        class VirtualCatTensor:
            def __init__(self, tensors, dim):
                self.tensors = tensors  # åªå­˜å‚¨å¼•ç”¨
                self.dim = dim
                self._shape = self._compute_shape()
            
            def _compute_shape(self):
                # è®¡ç®—è™šæ‹Ÿå½¢çŠ¶è€Œä¸å®žé™…å¤åˆ¶æ•°æ®
                total_dim = sum(t.shape[self.dim] for t in self.tensors)
                shape = list(self.tensors[0].shape)
                shape[self.dim] = total_dim
                return tuple(shape)
            
            # æƒ°æ€§è®¡ç®—ï¼šåªåœ¨éœ€è¦æ—¶è®¿é—®æ•°æ®
            def __getitem__(self, indices):
                # æ™ºèƒ½ç´¢å¼•ï¼Œå®šä½åˆ°æ­£ç¡®çš„å­å¼ é‡
                pass
        ```

    2. å®žé™…åº”ç”¨åœºæ™¯

        ```python
        # ä¼ ç»Ÿæ–¹å¼ï¼šç«‹å³å¤åˆ¶
        x = torch.randn(1000, 1000).cuda()  # 200MB
        y = torch.randn(1000, 1000).cuda()  # 200MB
        z = torch.cat([x, y], dim=0)  # ç«‹å³åˆ†é…400MBæ–°å†…å­˜å¹¶å¤åˆ¶

        # è™šæ‹Ÿæ–¹å¼ï¼šå»¶è¿Ÿå¤„ç†
        v = VirtualCat([x, y], dim=0)  # åªå­˜å‚¨æŒ‡é’ˆï¼Œå‡ ä¹Žé›¶å¼€é”€

        # éœ€è¦æ—¶å†å®žé™…è®¡ç®—
        result = v @ weight  # æ­¤æ—¶æ‰éœ€è¦å®žé™…æ•°æ®
        ```

    çŽ°å®žä¸­çš„å®žçŽ°æŒ‘æˆ˜

    1. è®¡ç®—å›¾å…¼å®¹æ€§

        ```python
        # åœ¨è‡ªåŠ¨æ±‚å¯¼ä¸­ï¼ŒæŒ‡é’ˆè®¾è®¡å˜å¾—å¤æ‚
        x = torch.randn(3, 4, requires_grad=True)
        y = torch.randn(3, 4, requires_grad=True)
        z = virtual_cat([x, y], dim=0)  # åå‘ä¼ æ’­æ—¶æ¢¯åº¦å¦‚ä½•ä¼ æ’­ï¼Ÿ
        ```

    2. å†…å­˜è®¿é—®æ¨¡å¼

        ```python
        # ä¸è¿žç»­å†…å­˜è®¿é—®å½±å“GPUæ•ˆçŽ‡
        # ä¼ ç»Ÿcat: [xxxxxxxxxx][yyyyyyyyyy] -> [xxxxxxxxxxyyyyyyyyyy] è¿žç»­è®¿é—®
        # è™šæ‹Ÿcat: [xxxxxxxxxx]...[yyyyyyyyyy] è·³è½¬è®¿é—®ï¼Œå½±å“ç¼“å­˜æ•ˆçŽ‡

        # GPUæ›´å–œæ¬¢è¿žç»­çš„å†…å­˜è®¿é—®æ¨¡å¼
        for i in range(n):
            # è¿žç»­è®¿é—®æ›´é«˜æ•ˆ
            output[i] = continuous_data[i] * weight
        ```

    PyTorch ä¸­çš„çŽ°æœ‰è¿‘ä¼¼æ–¹æ¡ˆ

    1. torch.nested

        ```python
        import torch

        # åµŒå¥—å¼ é‡æ”¯æŒä¸è§„åˆ™ç»“æž„
        nt = torch.nested.nested_tensor([
            torch.randn(2, 3),
            torch.randn(3, 3),
            torch.randn(4, 3)
        ])

        # å†…éƒ¨å­˜å‚¨ä¸ºç‹¬ç«‹æ•°æ®å—
        print(nt.shape)  # torch.Size([3, None, 3])
        ```

    2. ä½¿ç”¨ç¨€ç–å¼ é‡

        ```python
        # ç¨€ç–è¡¨ç¤ºå¯ä»¥å‡å°‘æ•°æ®ç§»åŠ¨
        indices = torch.cat([
            torch.arange(0, 1000).unsqueeze(0),
            torch.arange(0, 1000).unsqueeze(0)
        ], dim=0)
        values = torch.cat([x.flatten(), y.flatten()])
        sparse_cat = torch.sparse_coo_tensor(indices, values)
        ```

    3. è‡ªå®šä¹‰æ“ä½œç¬¦

        ```python
        class LazyCatFunction(torch.autograd.Function):
            @staticmethod
            def forward(ctx, *tensors):
                ctx.save_for_backward(*tensors)
                ctx.dim = dim
                # è¿”å›žä¸€ä¸ªåŒ…è£…å™¨è€Œä¸æ˜¯å®žé™…æ•°æ®
                return LazyCatWrapper(tensors, dim)
            
            @staticmethod
            def backward(ctx, grad_output):
                # å¤„ç†æ¢¯åº¦ä¼ æ’­
                pass
        ```

    å®žé™…å¯è¡Œçš„æŠ˜ä¸­æ–¹æ¡ˆ

    1. æ‰¹é‡å»¶è¿Ÿå¤„ç†

        ```python
        class BatchCollector:
            def __init__(self, dim=0, buffer_size=1024):
                self.buffers = []  # å­˜å‚¨æ•°æ®å—
                self.dim = dim
                self.buffer_size = buffer_size
            
            def add(self, tensor):
                self.buffers.append(tensor)
                if self.total_size() >= self.buffer_size:
                    return self.flush()  # è¾¾åˆ°é˜ˆå€¼æ—¶å®žé™…cat
                return None
            
            def flush(self):
                if self.buffers:
                    result = torch.cat(self.buffers, dim=self.dim)
                    self.buffers = []
                    return result
        ```

    2. å†…å­˜æ± æŠ€æœ¯

        ```python
        class TensorPool:
            def __init__(self, size):
                self.pool = torch.empty(size, device='cuda')
                self.offset = 0
            
            def cat_view(self, tensors, dim):
                # åœ¨å†…å­˜æ± ä¸­åˆ†é…ç©ºé—´å¹¶è¿”å›žè§†å›¾
                total_size = sum(t.numel() for t in tensors)
                view = self.pool[self.offset:self.offset+total_size].view(new_shape)
                # æ‰‹åŠ¨å¤åˆ¶æ•°æ®
                current = self.offset
                for t in tensors:
                    self.pool[current:current+t.numel()] = t.flatten()
                    current += t.numel()
                self.offset += total_size
                return view
        ```

    å‰æ²¿ç ”ç©¶æ–¹å‘

    * Unified Virtual Memory (UVM)

        * GPUå¯ä»¥ç›´æŽ¥è®¿é—®CPUå†…å­˜

        * å‡å°‘æ˜¾å­˜é—´çš„æ•°æ®å¤åˆ¶

    * Zero-copy Tensor Operations

        * å¦‚NVIDIAçš„cudaMemcpyAsync

        * å¼‚æ­¥å†…å­˜ä¼ è¾“

    * Compressed Sparse Tensor Formats

        * åªå­˜å‚¨éžé›¶å…ƒç´ å’Œç´¢å¼•

    æ€»ç»“

    ä½ çš„æƒ³æ³•åœ¨ç†è®ºä¸Šæ˜¯ç†æƒ³çš„ï¼Œä½†å®žè·µä¸­é¢ä¸´æŒ‘æˆ˜ï¼š

    * è®¡ç®—æ•ˆçŽ‡ï¼šä¸è¿žç»­è®¿é—®é™ä½ŽGPUåˆ©ç”¨çŽ‡

    * è½¯ä»¶å¤æ‚æ€§ï¼šç ´åçŽ°æœ‰APIå’Œè‡ªåŠ¨æ±‚å¯¼ç³»ç»Ÿ

    * å†…å­˜ç®¡ç†ï¼šç”Ÿå‘½å‘¨æœŸç®¡ç†å¤æ‚

    ç›®å‰çš„æŠ˜ä¸­æ–¹æ¡ˆï¼š

    * å°å¼ é‡ï¼šç›´æŽ¥cat

    * å¤§å¼ é‡ï¼šé¢„åˆ†é…+åˆ†æ‰¹å¤„ç†

    * ç‰¹å®šåœºæ™¯ï¼šä½¿ç”¨ç¨€ç–è¡¨ç¤ºæˆ–è‡ªå®šä¹‰å†…æ ¸

    æœªæ¥éšç€ç¡¬ä»¶å’Œè½¯ä»¶çš„å‘å±•ï¼Œè¿™ç±»"é›¶å¤åˆ¶"æ“ä½œå¯èƒ½ä¼šå˜å¾—æ›´åŠ æ™®éã€‚

* torch.cat()

    torch.cat() æ˜¯ PyTorch ä¸­ç”¨äºŽå¼ é‡æ‹¼æŽ¥çš„æ ¸å¿ƒå‡½æ•°ã€‚

    åŽŸåž‹

    ```python
    torch.cat(tensors, dim=0, *, out=None) â†’ Tensor
    ```

    å‚æ•°ï¼š

    * tensorsï¼šéœ€è¦æ‹¼æŽ¥çš„å¼ é‡åºåˆ—ï¼ˆåˆ—è¡¨æˆ–å…ƒç»„ï¼‰

    * dimï¼šæ²¿ç€å“ªä¸ªç»´åº¦è¿›è¡Œæ‹¼æŽ¥ï¼ˆé»˜è®¤ä¸º0ï¼‰

    * outï¼šè¾“å‡ºå¼ é‡ï¼ˆå¯é€‰ï¼‰

    ä½œç”¨

    å°†å¤šä¸ªå¼ é‡æ²¿ç€æŒ‡å®šç»´åº¦è¿žæŽ¥èµ·æ¥ï¼Œè¦æ±‚éžæ‹¼æŽ¥ç»´åº¦çš„å¤§å°å¿…é¡»ç›¸åŒã€‚

    **ç”¨æ³•ç¤ºä¾‹**

    åŸºç¡€ç”¨æ³•

    ```python
    import torch

    # åˆ›å»ºä¸¤ä¸ªå¼ é‡
    x = torch.tensor([[1, 2], [3, 4]])  # shape: (2, 2)
    y = torch.tensor([[5, 6], [7, 8]])  # shape: (2, 2)

    # æ²¿ç€ç¬¬0ç»´æ‹¼æŽ¥ï¼ˆä¸Šä¸‹å †å ï¼‰
    result = torch.cat([x, y], dim=0)
    # shape: (4, 2)
    # [[1, 2],
    #  [3, 4],
    #  [5, 6],
    #  [7, 8]]

    # æ²¿ç€ç¬¬1ç»´æ‹¼æŽ¥ï¼ˆå·¦å³æ‹¼æŽ¥ï¼‰
    result = torch.cat([x, y], dim=1)
    # shape: (2, 4)
    # [[1, 2, 5, 6],
    #  [3, 4, 7, 8]]
    ```

    ä¸åŒç»´åº¦çš„ç¤ºä¾‹

    ```python
    # ä¸‰ç»´å¼ é‡æ‹¼æŽ¥
    a = torch.randn(2, 3, 4)
    b = torch.randn(2, 3, 4)

    # æ²¿ç€dim=0æ‹¼æŽ¥ï¼šshapeå˜ä¸º (4, 3, 4)
    # æ²¿ç€dim=1æ‹¼æŽ¥ï¼šshapeå˜ä¸º (2, 6, 4)
    # æ²¿ç€dim=2æ‹¼æŽ¥ï¼šshapeå˜ä¸º (2, 3, 8)
    ```

    æ³¨æ„äº‹é¡¹

    ```python
    # é”™è¯¯ç¤ºä¾‹ï¼šéžæ‹¼æŽ¥ç»´åº¦å¤§å°ä¸åŒä¼šæŠ¥é”™
    x = torch.randn(2, 3)
    y = torch.randn(3, 3)  # ç¬¬0ç»´å¤§å°ä¸åŒ
    # torch.cat([x, y], dim=0)  # æŠ¥é”™

    # æ­£ç¡®ï¼šä¿è¯éžæ‹¼æŽ¥ç»´åº¦ä¸€è‡´
    x = torch.randn(2, 3, 4)
    y = torch.randn(2, 3, 5)
    result = torch.cat([x, y], dim=2)  # æ­£ç¡®ï¼šæ²¿ç€ç¬¬2ç»´æ‹¼æŽ¥
    ```

    ä¸Ž stack() çš„åŒºåˆ«

    * cat()ï¼šæ‰©å±•çŽ°æœ‰ç»´åº¦ï¼Œè¦æ±‚å…¶ä»–ç»´åº¦å¤§å°ç›¸åŒ

    * stack()ï¼šåˆ›å»ºæ–°ç»´åº¦ï¼Œè¦æ±‚æ‰€æœ‰ç»´åº¦å¤§å°éƒ½ç›¸åŒ

        ```python
        x = torch.tensor([1, 2, 3])
        y = torch.tensor([4, 5, 6])

        cat_result = torch.cat([x, y], dim=0)  # [1, 2, 3, 4, 5, 6], shape: (6,)
        stack_result = torch.stack([x, y], dim=0)  # [[1, 2, 3], [4, 5, 6]], shape: (2, 3)
        ```

    torch.cat() åœ¨ç¥žç»ç½‘ç»œä¸­å¸¸ç”¨äºŽç‰¹å¾æ‹¼æŽ¥ã€å¤šåˆ†æ”¯ç½‘ç»œç»“æžœçš„åˆå¹¶ç­‰åœºæ™¯ã€‚

* ç®€è¿° torch ä¸­ tensor ä¸Ž numpy çš„è½¬æ¢æ–¹å¼

    PyTorch Tensor ä¸Ž NumPy æ•°ç»„è½¬æ¢

    PyTorch ä¸Ž NumPy ä¹‹é—´çš„è½¬æ¢éžå¸¸æ–¹ä¾¿ï¼Œå› ä¸ºå®ƒä»¬å…±äº«åº•å±‚å†…å­˜ï¼ˆé»˜è®¤æƒ…å†µä¸‹ï¼‰ï¼Œé¿å…äº†æ•°æ®å¤åˆ¶ã€‚

    1. Tensor â†’ NumPy

        ```python
        import torch
        import numpy as np

        # åˆ›å»º PyTorch Tensor
        tensor = torch.tensor([1, 2, 3])
        tensor = torch.randn(3, 4)  # éšæœºå¼ é‡

        # è½¬æ¢ä¸º NumPy æ•°ç»„
        numpy_array = tensor.numpy()
        # æˆ–ä½¿ç”¨æ›´æ˜Žç¡®çš„æ–¹å¼
        numpy_array = tensor.cpu().numpy()  # æŽ¨èï¼šç¡®ä¿åœ¨ CPU ä¸Š
        ```

        æ³¨æ„äº‹é¡¹ï¼š

        * CPU Tensorï¼šå…±äº«å†…å­˜ï¼ˆä¿®æ”¹ä¸€æ–¹ä¼šå½±å“å¦ä¸€æ–¹ï¼‰

        * GPU Tensorï¼šä¸èƒ½ç›´æŽ¥è½¬æ¢ï¼Œéœ€è¦å…ˆç§»åˆ° CPU

        * å½“ Tensor æœ‰æ¢¯åº¦æ—¶ï¼ˆrequires_grad=Trueï¼‰ï¼Œéœ€è¦å…ˆåˆ†ç¦»

            ```python
            # GPU Tensor è½¬æ¢
            if tensor.is_cuda:
                numpy_array = tensor.cpu().numpy()  # å¿…é¡»å…ˆç§»åˆ° CPU

            # æœ‰æ¢¯åº¦çš„ Tensor
            tensor_with_grad = torch.tensor([1., 2., 3.], requires_grad=True)
            numpy_array = tensor_with_grad.detach().numpy()  # å¿…é¡»å…ˆ detach()
            ```

    2. NumPy â†’ Tensor

        ```python
        import numpy as np
        import torch

        # åˆ›å»º NumPy æ•°ç»„
        np_array = np.array([1, 2, 3])
        np_array = np.random.randn(3, 4)

        # è½¬æ¢ä¸º PyTorch Tensor
        tensor = torch.from_numpy(np_array)  # å…±äº«å†…å­˜
        # æˆ–ä½¿ç”¨æž„é€ å‡½æ•°
        tensor = torch.tensor(np_array)      # åˆ›å»ºå‰¯æœ¬ï¼ˆä¸å…±äº«å†…å­˜ï¼‰
        ```

        é‡è¦åŒºåˆ«ï¼š

        * torch.from_numpy()ï¼šå…±äº«å†…å­˜ï¼Œä¿®æ”¹ NumPy æ•°ç»„ä¼šå½±å“ Tensor

        * torch.tensor()ï¼šåˆ›å»ºå‰¯æœ¬ï¼Œä¸¤è€…ç‹¬ç«‹

            ```python
            np_array = np.array([1, 2, 3])
            tensor1 = torch.from_numpy(np_array)  # å…±äº«å†…å­˜
            tensor2 = torch.tensor(np_array)      # åˆ›å»ºå‰¯æœ¬

            np_array[0] = 999
            print(tensor1)  # tensor([999, 2, 3])  æ”¹å˜äº†ï¼
            print(tensor2)  # tensor([1, 2, 3])    æ²¡æ”¹å˜
            ```

    3. æ•°æ®ç±»åž‹å¯¹åº”å…³ç³»

        ```python
        # NumPy ç±»åž‹ â†’ PyTorch ç±»åž‹
        np.float32  â†”  torch.float32 / torch.float
        np.float64  â†”  torch.float64 / torch.double
        np.int32    â†”  torch.int32
        np.int64    â†”  torch.int64 / torch.long
        np.bool_    â†”  torch.bool

        # æŒ‡å®šæ•°æ®ç±»åž‹è½¬æ¢
        tensor = torch.from_numpy(np_array).float()  # è½¬æ¢ä¸º float32
        tensor = torch.from_numpy(np_array).long()   # è½¬æ¢ä¸º int64
        ```

    4. GPU Tensor çš„ç‰¹æ®Šå¤„ç†

        ```python
        # NumPy â†’ GPU Tensor
        np_array = np.array([1, 2, 3])
        gpu_tensor = torch.from_numpy(np_array).cuda()          # å…ˆåˆ›å»º CPU Tensor å†ç§»åˆ° GPU
        gpu_tensor = torch.tensor(np_array, device='cuda')     # ç›´æŽ¥åœ¨ GPU åˆ›å»º

        # GPU Tensor â†’ NumPy
        gpu_tensor = torch.randn(3, 4, device='cuda')
        cpu_tensor = gpu_tensor.cpu()          # å…ˆç§»åˆ° CPU
        numpy_array = cpu_tensor.numpy()       # å†è½¬æ¢ä¸º NumPy
        ```

    5. å®žç”¨å‡½æ•°å°è£…

        ```python
        def numpy_to_tensor(np_array, device='cpu', dtype=None):
            """å®‰å…¨åœ°å°† NumPy æ•°ç»„è½¬æ¢ä¸º Tensor"""
            tensor = torch.from_numpy(np_array)
            if dtype:
                tensor = tensor.to(dtype)
            return tensor.to(device)

        def tensor_to_numpy(tensor):
            """å®‰å…¨åœ°å°† Tensor è½¬æ¢ä¸º NumPy æ•°ç»„"""
            if tensor.requires_grad:
                tensor = tensor.detach()
            if tensor.is_cuda:
                tensor = tensor.cpu()
            return tensor.numpy()
        ```

    6. å†…å­˜å…±äº«ç¤ºä¾‹

        ```python
        # å…±äº«å†…å­˜ï¼ˆé«˜æ•ˆï¼Œä½†éœ€å°å¿ƒï¼‰
        np_array = np.ones((3, 3))
        tensor = torch.from_numpy(np_array)  # å…±äº«å†…å­˜

        tensor[0, 0] = 999
        print(np_array[0, 0])  # è¾“å‡º 999ï¼ŒåŽŸå§‹æ•°ç»„ä¹Ÿè¢«ä¿®æ”¹äº†

        # é¿å…å…±äº«å†…å­˜ï¼ˆåˆ›å»ºå‰¯æœ¬ï¼‰
        tensor = torch.tensor(np_array)      # åˆ›å»ºå‰¯æœ¬
        tensor[0, 0] = 888
        print(np_array[0, 0])  # è¾“å‡º 999ï¼ŒåŽŸå§‹æ•°ç»„ä¸å—å½±å“
        ```

    æ€»ç»“è¦ç‚¹

    * è½¬æ¢æ–¹æ³•ï¼š

        * Tensor â†’ NumPyï¼š.numpy() æˆ– .cpu().numpy()

        * NumPy â†’ Tensorï¼štorch.from_numpy() æˆ– torch.tensor()

    * å†…å­˜å…±äº«ï¼š

        * torch.from_numpy() å…±äº«å†…å­˜

        * torch.tensor() åˆ›å»ºå‰¯æœ¬

        * CPU Tensor ä¸Ž NumPy æ•°ç»„é»˜è®¤å…±äº«å†…å­˜

    * ç‰¹æ®Šå¤„ç†ï¼š

        * GPU Tensor éœ€è¦å…ˆç§»åˆ° CPU

        * æœ‰æ¢¯åº¦çš„ Tensor éœ€è¦å…ˆ detach()

        * æ³¨æ„æ•°æ®ç±»åž‹çš„ä¸€è‡´æ€§

    * æ€§èƒ½å»ºè®®ï¼š

        * è®­ç»ƒæ—¶å°½é‡ä½¿ç”¨ Tensorï¼ˆæ”¯æŒ GPU å’Œè‡ªåŠ¨å¾®åˆ†ï¼‰

        * æ•°æ®é¢„å¤„ç†å¯ä½¿ç”¨ NumPy

        * é¿å…é¢‘ç¹è½¬æ¢ä»¥å‡å°‘å¼€é”€

* torch set_num_threads() ä¸Ž set_num_interop_threads() åŒºåˆ«

    torch.set_num_threads() å’Œ torch.set_num_interop_threads() æŽ§åˆ¶çš„æ˜¯ ä¸åŒå±‚é¢çš„å¹¶è¡ŒåŒ–ï¼Œç†è§£å®ƒä»¬çš„åŒºåˆ«å¯¹äºŽæ€§èƒ½è°ƒä¼˜å¾ˆé‡è¦ã€‚

    ä¸»è¦åŒºåˆ«

    | ç‰¹æ€§ | torch.set_num_threads() | torch.set_num_interop_threads() |
    | - | - | - |
    | ä½œç”¨å¯¹è±¡ | å•ä¸ªæ“ä½œå†…éƒ¨å¹¶è¡Œï¼ˆå¦‚çŸ©é˜µä¹˜æ³•ï¼‰ | å¤šä¸ªç‹¬ç«‹æ“ä½œé—´çš„å¹¶è¡Œï¼ˆå¦‚å¤šä¸ªç‹¬ç«‹çš„çŸ©é˜µä¹˜æ³•ï¼‰ |
    | å¹¶è¡Œå±‚çº§ | æ“ä½œå†…å¹¶è¡Œï¼ˆintra-opï¼‰ | æ“ä½œé—´å¹¶è¡Œï¼ˆinter-opï¼‰ |
    | å…¸åž‹åœºæ™¯ | å¤§åž‹çŸ©é˜µè¿ç®—ã€å·ç§¯ç­‰ | æ•°æ®åŠ è½½ã€å¤šä¸ªå°æ“ä½œçš„å¹¶è¡Œæ‰§è¡Œ |
    | é»˜è®¤å€¼ | CPU æ ¸å¿ƒæ•° | é€šå¸¸ä¸º 1ï¼ˆä¿å®ˆé»˜è®¤ï¼‰ |
    | çº¿ç¨‹æ±  | ä¸åŒçš„çº¿ç¨‹æ±  | ä¸åŒçš„çº¿ç¨‹æ±  |

    è¯¦ç»†è§£é‡Š

    1. torch.set_num_threads() - æ“ä½œå†…å¹¶è¡Œ

        ```python
        import torch
        import time

        # è®¾ç½®æ“ä½œå†…å¹¶è¡Œçº¿ç¨‹æ•°
        torch.set_num_threads(4)  # è¿™ä¸ªæ“ä½œå†…éƒ¨æœ€å¤šç”¨4ä¸ªçº¿ç¨‹

        # å•ä¸€å¤§æ“ä½œä¼šè¢«æ‹†åˆ†æˆå¤šä¸ªå­ä»»åŠ¡å¹¶è¡Œæ‰§è¡Œ
        x = torch.randn(5000, 5000)
        y = torch.randn(5000, 5000)

        # è¿™ä¸ªçŸ©é˜µä¹˜æ³•ä¼šåœ¨å†…éƒ¨ä½¿ç”¨æœ€å¤š4ä¸ªçº¿ç¨‹
        z = x @ y  # æ“ä½œå†…å¹¶è¡Œ
        ```

        å·¥ä½œæ–¹å¼ï¼š

        * å•ä¸ªå¤æ‚æ“ä½œï¼ˆå¦‚å¤§çŸ©é˜µä¹˜æ³•ï¼‰è¢«åˆ†è§£æˆå¤šä¸ªå­ä»»åŠ¡

        * è¿™äº›å­ä»»åŠ¡åœ¨å¤šä¸ªçº¿ç¨‹ä¸Šå¹¶è¡Œæ‰§è¡Œ

        * æ‰€æœ‰çº¿ç¨‹å…±åŒå®Œæˆè¿™ä¸€ä¸ªæ“ä½œ

    2. torch.set_num_interop_threads() - æ“ä½œé—´å¹¶è¡Œ

        ```python
        import torch
        import concurrent.futures

        # è®¾ç½®æ“ä½œé—´å¹¶è¡Œçº¿ç¨‹æ•°
        torch.set_num_interop_threads(2)  # æœ€å¤šåŒæ—¶æ‰§è¡Œ2ä¸ªç‹¬ç«‹æ“ä½œ
        torch.set_num_threads(2)  # æ¯ä¸ªæ“ä½œå†…éƒ¨æœ€å¤šç”¨2ä¸ªçº¿ç¨‹

        def compute(i):
            x = torch.randn(1000, 1000)
            y = torch.randn(1000, 1000)
            return (x @ y).mean()

        # å¤šä¸ªç‹¬ç«‹çš„çŸ©é˜µä¹˜æ³•å¯ä»¥å¹¶è¡Œæ‰§è¡Œ
        # ç”±äºŽè®¾ç½®äº† interop_threads=2ï¼Œæœ€å¤š2ä¸ªæ“ä½œåŒæ—¶è¿›è¡Œ
        results = []
        for i in range(4):
            results.append(compute(i))
        ```

    å®žé™…åº”ç”¨åœºæ™¯å¯¹æ¯”

    åœºæ™¯1ï¼šå•ä¸ªå¤§ä»»åŠ¡

    ```python
    # é€‚åˆç”¨ set_num_threads() ä¼˜åŒ–
    torch.set_num_threads(8)  # è®©å•ä¸ªå¤§æ“ä½œç”¨8ä¸ªæ ¸å¿ƒ
    torch.set_num_interop_threads(1)  # åªæœ‰ä¸€ä¸ªæ“ä½œï¼Œæ— éœ€æ“ä½œé—´å¹¶è¡Œ

    # å•ä¸ªå¤§åž‹çŸ©é˜µè¿ç®—
    large_matrix = torch.randn(10000, 10000)
    result = large_matrix @ large_matrix.T  # è¿™ä¸ªæ“ä½œå†…éƒ¨ä¼šå¹¶è¡Œ
    ```

    åœºæ™¯2ï¼šå¤šä¸ªå°ä»»åŠ¡

    ```python
    # é€‚åˆç”¨ set_num_interop_threads() ä¼˜åŒ–
    torch.set_num_threads(2)  # æ¯ä¸ªå°æ“ä½œç”¨2ä¸ªæ ¸å¿ƒ
    torch.set_num_interop_threads(4)  # åŒæ—¶æ‰§è¡Œ4ä¸ªå°æ“ä½œ

    # å¤šä¸ªç‹¬ç«‹çš„å°è¿ç®—
    def process_batch(batch_data):
        return torch.mm(batch_data, batch_data.T)

    # å¦‚æžœæœ‰4ä¸ªbatchï¼Œå¯ä»¥å¹¶è¡Œå¤„ç†
    batches = [torch.randn(500, 500) for _ in range(4)]
    results = [process_batch(batch) for batch in batches]
    ```

    åœºæ™¯3ï¼šæ··åˆåœºæ™¯

    ```python
    # æ•°æ®åŠ è½½ï¼ˆæ“ä½œé—´å¹¶è¡Œï¼‰ + è®¡ç®—ï¼ˆæ“ä½œå†…å¹¶è¡Œï¼‰
    torch.set_num_interop_threads(2)  # åŒæ—¶åŠ è½½2ä¸ªbatch
    torch.set_num_threads(4)          # æ¯ä¸ªbatchè®¡ç®—ç”¨4ä¸ªæ ¸å¿ƒ

    # å‡è®¾çš„æ•°æ®åŠ è½½å’Œè®¡ç®—æµç¨‹
    def load_and_compute(i):
        # æ¨¡æ‹Ÿæ•°æ®åŠ è½½
        data = torch.randn(1000, 1000)
        # è®¡ç®—
        return data @ data.T

    # å¯ä»¥åŒæ—¶åŠ è½½å’Œè®¡ç®—å¤šä¸ªbatch
    ```

    æ€§èƒ½å½±å“ç¤ºä¾‹

    ```python
    import torch
    import time

    # æµ‹è¯•ä¸åŒé…ç½®çš„æ€§èƒ½
    def benchmark():
        # åˆ›å»ºå¤šä¸ªå°çŸ©é˜µ
        matrices = [torch.randn(500, 500) for _ in range(8)]
        
        # é…ç½®1ï¼šåªæœ‰æ“ä½œå†…å¹¶è¡Œ
        torch.set_num_threads(4)
        torch.set_num_interop_threads(1)
        
        start = time.time()
        results = [m @ m.T for m in matrices]
        print(f"åªæœ‰æ“ä½œå†…å¹¶è¡Œ: {time.time()-start:.3f}s")
        
        # é…ç½®2ï¼šåªæœ‰æ“ä½œé—´å¹¶è¡Œ
        torch.set_num_threads(1)
        torch.set_num_interop_threads(4)
        
        start = time.time()
        results = [m @ m.T for m in matrices]
        print(f"åªæœ‰æ“ä½œé—´å¹¶è¡Œ: {time.time()-start:.3f}s")
        
        # é…ç½®3ï¼šä¸¤è€…ç»“åˆ
        torch.set_num_threads(2)
        torch.set_num_interop_threads(2)
        
        start = time.time()
        results = [m @ m.T for m in matrices]
        print(f"ä¸¤è€…ç»“åˆ(2x2): {time.time()-start:.3f}s")

    benchmark()
    ```

    æœ€ä½³å®žè·µå»ºè®®

    * ç¡®å®šå·¥ä½œè´Ÿè½½ç±»åž‹ï¼š

        * è®¡ç®—å¯†é›†åž‹å¤§æ“ä½œ â†’ å¢žåŠ  num_threads

        * å¤šä¸ªç‹¬ç«‹å°æ“ä½œ â†’ å¢žåŠ  num_interop_threads

    * é€šç”¨æŽ¨èé…ç½®ï¼š

        ```python
        import os

        total_cores = os.cpu_count()

        # é€šç”¨åœºæ™¯ï¼šå¹³è¡¡é…ç½®
        torch.set_num_threads(total_cores // 2)
        torch.set_num_interop_threads(2)

        # æ•°æ®é¢„å¤„ç†æµæ°´çº¿
        # torch.set_num_threads(2)  # æ¯ä¸ªé¢„å¤„ç†ä»»åŠ¡ç”¨2æ ¸å¿ƒ
        # torch.set_num_interop_threads(4)  # åŒæ—¶é¢„å¤„ç†4ä¸ªbatch
        ```

    * æ³¨æ„èµ„æºç«žäº‰ï¼š

        ```python
        # ä¸è¦è®¾ç½®è¿‡å¤§ï¼Œé¿å…çº¿ç¨‹ç«žäº‰
        # æ€»çº¿ç¨‹æ•° â‰ˆ num_threads * num_interop_threads
        # å»ºè®®ä¸è¶…è¿‡ç‰©ç†æ ¸å¿ƒæ•°

        # é”™è¯¯ç¤ºä¾‹ï¼ˆå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼‰ï¼š
        # torch.set_num_threads(8)
        # torch.set_num_interop_threads(8)  # æœ€å¤š64ä¸ªçº¿ç¨‹ï¼
        ```

    ä¸Ž DataLoader çš„å…³ç³»

    ```python
    from torch.utils.data import DataLoader

    # DataLoader æœ‰è‡ªå·±çš„å¹¶è¡Œæœºåˆ¶ï¼ˆnum_workersï¼‰
    # ä¸Ž torch çš„çº¿ç¨‹è®¾ç½®æ˜¯ç‹¬ç«‹çš„

    # å…¸åž‹é…ç½®ï¼š
    loader = DataLoader(dataset, 
                       batch_size=32,
                       num_workers=4,      # æ•°æ®åŠ è½½è¿›ç¨‹æ•°
                       pin_memory=True)

    # PyTorch è®¡ç®—çº¿ç¨‹è®¾ç½®
    torch.set_num_threads(4)          # æ¯ä¸ªGPUæ“ä½œç”¨4çº¿ç¨‹
    torch.set_num_interop_threads(1)  # é€šå¸¸GPUæ“ä½œæ˜¯ä¸²è¡Œçš„
    ```

    æ€»ç»“ï¼š

    * torch.set_num_threads()ï¼šåž‚ç›´æ‰©å±•ï¼Œè®©å•ä¸ªä»»åŠ¡æ›´å¿«

    * torch.set_num_interop_threads()ï¼šæ°´å¹³æ‰©å±•ï¼Œè®©å¤šä¸ªä»»åŠ¡åŒæ—¶è¿›è¡Œ

    * æ ¹æ®å…·ä½“åœºæ™¯è°ƒæ•´ï¼Œä¸¤è€…ä¹˜ç§¯ä¸è¦è¶…è¿‡CPUç‰©ç†æ ¸å¿ƒæ•°

    * å¯¹äºŽå¤§å¤šæ•°æ·±åº¦å­¦ä¹ è®­ç»ƒï¼Œè°ƒæ•´ num_threads æ›´é‡è¦

* torch çš„ cpu backend ä¼šè‡ªåŠ¨å¤šçº¿ç¨‹å—ï¼Œè¿˜æ˜¯ä½¿ç”¨ cpu çš„ simd ç›¸å…³æŒ‡ä»¤é›†ï¼Ÿ

    PyTorch çš„ CPU backend ä¼šåŒæ—¶ä½¿ç”¨å¤šçº¿ç¨‹å’Œ SIMD æŒ‡ä»¤é›†ï¼Œä½†ä¸¤è€…çš„è§¦å‘æœºåˆ¶å’Œé…ç½®æ–¹å¼ä¸åŒã€‚

    1. è‡ªåŠ¨å¤šçº¿ç¨‹

        æ˜¯çš„ï¼ŒPyTorch CPU è¿ç®—é»˜è®¤ä¼šè‡ªåŠ¨ä½¿ç”¨å¤šçº¿ç¨‹ï¼Œå…·ä½“è¡¨çŽ°ï¼š

        é…ç½®æ–¹å¼ï¼š

        ```python
        import torch

        # æŸ¥çœ‹å’Œè®¾ç½®çº¿ç¨‹æ•°
        print(torch.get_num_threads())    # èŽ·å–å½“å‰çº¿ç¨‹æ•°
        print(torch.get_num_interop_threads())  # èŽ·å–å¹¶è¡Œæ“ä½œçº¿ç¨‹æ•°

        # è®¾ç½®çº¿ç¨‹æ•°
        torch.set_num_threads(4)         # è®¾ç½®è®¡ç®—çº¿ç¨‹æ•°
        torch.set_num_interop_threads(2) # è®¾ç½®å¹¶è¡Œæ“ä½œçº¿ç¨‹æ•°
        ```

        çŽ¯å¢ƒå˜é‡æŽ§åˆ¶ï¼š

        ```bash
        # å¸¸ç”¨çš„çŽ¯å¢ƒå˜é‡
        export OMP_NUM_THREADS=4        # OpenMP çº¿ç¨‹æ•°ï¼ˆå½±å“åº•å±‚è¿ç®—ï¼‰
        export MKL_NUM_THREADS=4        # MKL çº¿ç¨‹æ•°ï¼ˆå¦‚æžœæœ‰ç”¨ MKLï¼‰
        ```

    2. SIMD æŒ‡ä»¤é›†ä¼˜åŒ–

        PyTorch ä¼šè‡ªåŠ¨åˆ©ç”¨ CPU çš„ SIMD æŒ‡ä»¤é›†ï¼š

        æ”¯æŒçš„æŒ‡ä»¤é›†ï¼š

        * SSE4.2

        * AVX2

        * AVX-512ï¼ˆå¦‚æžœ CPU æ”¯æŒï¼‰

        * ARM NEONï¼ˆARM æž¶æž„ï¼‰

        æ£€æŸ¥æ˜¯å¦å¯ç”¨ï¼š

        ```python
        import torch

        # æ£€æŸ¥æž„å»ºæ—¶å¯ç”¨çš„æŒ‡ä»¤é›†ï¼ˆéœ€è¦ä»Žæºç ç¼–è¯‘çš„ä¿¡æ¯ï¼‰
        print(torch.__config__.show())  # æ˜¾ç¤ºæž„å»ºé…ç½®

        # è¿è¡Œæ—¶æ£€æµ‹
        print("æ˜¯å¦ä½¿ç”¨MKL:", torch.backends.mkl.is_available())
        print("æ˜¯å¦ä½¿ç”¨OpenMP:", torch.backends.openmp.is_available())
        ```

        è‡ªåŠ¨ä¼˜åŒ–æœºåˆ¶ï¼š

        * PyTorch åœ¨ç¼–è¯‘æ—¶ä¼šæ ¹æ® CPU æž¶æž„ç”Ÿæˆå¤šç‰ˆæœ¬ä»£ç 

        * è¿è¡Œæ—¶é€šè¿‡ CPU ç‰¹æ€§æ£€æµ‹é€‰æ‹©æœ€ä¼˜çš„ä»£ç è·¯å¾„

        * åº•å±‚ä½¿ç”¨ Eigenã€MKLã€OpenBLAS ç­‰ä¼˜åŒ–çš„æ•°å­¦åº“

    3. å®žé™…ç¤ºä¾‹

        ```python
        import torch
        import time

        # åˆ›å»ºä¸€ä¸ªè¾ƒå¤§çš„å¼ é‡
        x = torch.randn(10000, 10000)
        y = torch.randn(10000, 10000)

        # æŸ¥çœ‹é»˜è®¤çº¿ç¨‹æ•°
        print(f"çº¿ç¨‹æ•°: {torch.get_num_threads()}")

        # çŸ©é˜µä¹˜æ³•ï¼ˆä¼šè‡ªåŠ¨å¹¶è¡ŒåŒ–å’ŒSIMDä¼˜åŒ–ï¼‰
        start = time.time()
        z = torch.matmul(x, y)
        end = time.time()

        print(f"è®¡ç®—æ—¶é—´: {end - start:.3f}ç§’")
        print(f"å¼ é‡å½¢çŠ¶: {z.shape}")
        ```

    4. æ€§èƒ½è°ƒä¼˜å»ºè®®

        æŽ¨èé…ç½®ï¼š

        ```python
        # åœ¨ç¨‹åºå¼€å§‹æ—¶è®¾ç½®
        import torch
        import os

        # é€šå¸¸è®¾ç½®ä¸ºç‰©ç†æ ¸å¿ƒæ•°
        num_cores = os.cpu_count()
        torch.set_num_threads(num_cores)

        # æˆ–è€…è®© OpenMP è‡ªåŠ¨ç®¡ç†
        os.environ['OMP_NUM_THREADS'] = str(num_cores)
        os.environ['MKL_NUM_THREADS'] = str(num_cores)
        ```

        æ³¨æ„äº‹é¡¹ï¼š

        * çº¿ç¨‹æ•°ä¸è¦è¶…è¿‡ç‰©ç†æ ¸å¿ƒæ•°ï¼Œå¦åˆ™å¯èƒ½å› ä¸Šä¸‹æ–‡åˆ‡æ¢é™ä½Žæ€§èƒ½

        * SIMD ä¼˜åŒ–æ˜¯è‡ªåŠ¨çš„ï¼Œä½†éœ€è¦ PyTorch ç¼–è¯‘æ—¶æ”¯æŒå¯¹åº”çš„æŒ‡ä»¤é›†

        * å°çŸ©é˜µè¿ç®—å¯èƒ½ä¸ä¼šå……åˆ†å¹¶è¡ŒåŒ–ï¼ˆå¼€é”€å¤§äºŽæ”¶ç›Šï¼‰

        * I/O å¯†é›†åž‹ä»»åŠ¡ä¸Žè®¡ç®—å¯†é›†åž‹ä»»åŠ¡çš„çº¿ç¨‹é…ç½®å¯èƒ½ä¸åŒ

    5. éªŒè¯ä¼˜åŒ–æ•ˆæžœ

        ```python
        import torch
        import numpy as np

        # æ¯”è¾ƒæœ‰æ— å¤šçº¿ç¨‹çš„æ€§èƒ½å·®å¼‚
        torch.set_num_threads(1)  # å•çº¿ç¨‹
        x = torch.randn(5000, 5000)
        start = time.time()
        _ = x @ x
        print(f"å•çº¿ç¨‹æ—¶é—´: {time.time() - start:.3f}s")

        torch.set_num_threads(8)  # å¤šçº¿ç¨‹
        start = time.time()
        _ = x @ x
        print(f"8çº¿ç¨‹æ—¶é—´: {time.time() - start:.3f}s")
        ```

    æ€»ç»“ï¼š

    * âœ… å¤šçº¿ç¨‹æ˜¯é»˜è®¤å¼€å¯çš„ï¼Œå¯é€šè¿‡ torch.set_num_threads() é…ç½®

    * âœ… SIMD ä¼˜åŒ–æ˜¯è‡ªåŠ¨çš„ï¼ŒPyTorch ä¼šæ£€æµ‹å¹¶é€‰æ‹©æœ€ä¼˜æŒ‡ä»¤é›†

    * âš ï¸ æœ€ä½³çº¿ç¨‹æ•°å–å†³äºŽå…·ä½“ä»»åŠ¡å’Œç¡¬ä»¶

    * ðŸ“Š å¤§çŸ©é˜µè¿ç®—æœ€èƒ½ä½“çŽ°å¹¶è¡ŒåŒ–å’Œ SIMD çš„ä¼˜åŠ¿

    PyTorch åœ¨ CPU ä¸Šçš„æ€§èƒ½å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºŽåº•å±‚çš„æ•°å­¦åº“ï¼ˆMKLã€OpenBLAS ç­‰ï¼‰ï¼Œè¿™äº›åº“æœ¬èº«å°±åŒ…å«äº†é«˜åº¦ä¼˜åŒ–çš„å¤šçº¿ç¨‹å’Œ SIMD å®žçŽ°ã€‚

* ç®€ä»‹ä¸€ä¸‹ torchmetrics çš„åŸºæœ¬ç”¨æ³•

    TorchMetrics æ˜¯ä¸€ä¸ªä¸“é—¨ä¸º PyTorch æ·±åº¦å­¦ä¹ æ¨¡åž‹æä¾›è¯„ä¼°æŒ‡æ ‡çš„åº“ã€‚å®ƒèƒ½å¤Ÿå¸®ä½ æ ‡å‡†åŒ–ã€ç®€åŒ–æ¨¡åž‹æ€§èƒ½çš„è¯„ä¼°è¿‡ç¨‹ï¼Œå°¤å…¶æ“…é•¿å¤„ç†åˆ†å¸ƒå¼è®­ç»ƒåœºæ™¯ï¼Œå¹¶æœ‰æ•ˆå‡å°‘æ ·æ¿ä»£ç ã€‚

    ä¸‹é¢è¿™ä¸ªè¡¨æ ¼æ±‡æ€»äº† TorchMetrics çš„ä¸€äº›æ ¸å¿ƒç»„ä»¶å’ŒåŸºæœ¬ç”¨æ³•ï¼š

    | ç»„ä»¶/æ¦‚å¿µ | è¯´æ˜Ž | ä½¿ç”¨åœºæ™¯/è¯´æ˜Ž |
    | - | - | - |
    | æ ¸å¿ƒæ–¹æ³• | - | - |
    | update() | ç´¯ç§¯å•ä¸ªæ‰¹æ¬¡çš„é¢„æµ‹å’Œç›®æ ‡å€¼ï¼Œæ›´æ–°æŒ‡æ ‡å†…éƒ¨çŠ¶æ€ã€‚ | é€šå¸¸åœ¨è®­ç»ƒå¾ªçŽ¯ä¸­æ¯ä¸ªæ‰¹æ¬¡ç»“æŸåŽè°ƒç”¨ï¼Œä»…æ›´æ–°çŠ¶æ€ï¼Œä¸è®¡ç®—æœ€ç»ˆç»“æžœï¼Œæ•ˆçŽ‡è¾ƒé«˜ã€‚ |
    | compute() | åœ¨æ‰€æœ‰æ‰¹æ¬¡æ•°æ®ä¸Šè®¡ç®—æœ€ç»ˆæŒ‡æ ‡å€¼ã€‚ | é€šå¸¸åœ¨ä¸€ä¸ªè®­ç»ƒå‘¨æœŸï¼ˆEpochï¼‰ç»“æŸåŽè°ƒç”¨ã€‚ |
    | reset() | é‡ç½®æŒ‡æ ‡å†…éƒ¨çŠ¶æ€ï¼Œå‡†å¤‡ä¸‹ä¸€è½®è¯„ä¼°ã€‚ | åœ¨æ–°çš„è¯„ä¼°å‘¨æœŸå¼€å§‹å‰è°ƒç”¨ã€‚ |
    | forward() | ç›¸å½“äºŽ update() + compute()ï¼ŒåŒæ—¶æ›´æ–°çŠ¶æ€å¹¶è¿”å›žå½“å‰æ‰¹æ¬¡çš„æŒ‡æ ‡ã€‚ | å¦‚éœ€è¦å½“å‰æ‰¹æ¬¡ç»“æžœå¯ä½¿ç”¨ï¼Œä½†æ³¨æ„ä¸Žæœ€ç»ˆå‘¨æœŸç»“æžœå¯èƒ½ä¸åŒ. |
    | å¸¸ç”¨æŒ‡æ ‡ | - | - |	
    | Accuracy | åˆ†ç±»å‡†ç¡®çŽ‡ã€‚ | æ”¯æŒå¤šåˆ†ç±»ã€å¤šæ ‡ç­¾ç­‰ä¸åŒä»»åŠ¡ã€‚ |
    | Precision | ç²¾ç¡®çŽ‡ã€‚ | æ”¯æŒå¤šåˆ†ç±»ï¼Œå¯é€šè¿‡ num_classes å’Œ average å‚æ•°è°ƒæ•´ã€‚ |
    | Recall | å¬å›žçŽ‡ã€‚ | æ”¯æŒå¤šåˆ†ç±»ï¼Œå¯é€šè¿‡ num_classes å’Œ average å‚æ•°è°ƒæ•´ã€‚ |
    | F1Score | F1åˆ†æ•°ï¼ˆç²¾ç¡®çŽ‡å’Œå¬å›žçŽ‡çš„è°ƒå’Œå¹³å‡æ•°ï¼‰ã€‚ | æ”¯æŒå¤šåˆ†ç±»ï¼Œå¯é€šè¿‡ num_classes å’Œ average å‚æ•°è°ƒæ•´ã€‚ |
    | MeanMetric | è®¡ç®—å¹³å‡å€¼ï¼Œä¾‹å¦‚å¹³å‡æŸå¤±. |
    | å·¥å…·ç±» | - | - |
    | MetricCollection | å°†å¤šä¸ªæŒ‡æ ‡åˆå¹¶ä¸ºå•ä¸ªå¯è°ƒç”¨å•å…ƒï¼ŒåŒæ—¶è®¡ç®—å¹¶è¿”å›žæ‰€æœ‰ç»“æžœã€‚ | ç®€åŒ–å¤šæŒ‡æ ‡ç®¡ç†ï¼ŒæŽ¥å£ä¸Žå•æŒ‡æ ‡ä¸€è‡´ã€‚ |

    ðŸ”§ å®‰è£…ä¸ŽåŸºæœ¬ä½¿ç”¨æµç¨‹

    å®‰è£…å¾ˆç®€å•ï¼Œé€šå¸¸é€šè¿‡pipå®‰è£…å³å¯ï¼š

    ```bash
    pip install torchmetrics
    ```

    ä½¿ç”¨ TorchMetrics è¯„ä¼°æ¨¡åž‹æ€§èƒ½çš„åŸºæœ¬æµç¨‹å¦‚ä¸‹ï¼š

    * åˆå§‹åŒ–æŒ‡æ ‡ï¼šé€‰æ‹©ä¸Žä½ çš„ä»»åŠ¡ç›¸åŒ¹é…çš„æŒ‡æ ‡ï¼Œä¾‹å¦‚å¤šåˆ†ç±»ä»»åŠ¡çš„Accuracyã€‚

    * å°†æŒ‡æ ‡ç§»è‡³è®¾å¤‡ï¼šç¡®ä¿æŒ‡æ ‡ä¸Žæ¨¡åž‹ã€æ•°æ®åœ¨åŒä¸€è®¾å¤‡ä¸Šï¼ˆCPUæˆ–GPUï¼‰ã€‚

    * æ›´æ–°æŒ‡æ ‡çŠ¶æ€ï¼šåœ¨æ¯ä¸ªæ‰¹æ¬¡ç»“æŸåŽï¼Œä½¿ç”¨update()æ–¹æ³•å°†æ¨¡åž‹çš„é¢„æµ‹è¾“å‡ºå’ŒçœŸå®žæ ‡ç­¾ä¼ é€’ç»™æŒ‡æ ‡å¯¹è±¡ã€‚

    * è®¡ç®—æœ€ç»ˆæŒ‡æ ‡ï¼šåœ¨ä¸€ä¸ªè®­ç»ƒæˆ–éªŒè¯å‘¨æœŸç»“æŸåŽï¼Œä½¿ç”¨compute()æ–¹æ³•èŽ·å–æ‰€æœ‰æ‰¹æ¬¡ä¸Šçš„æœ€ç»ˆæŒ‡æ ‡å€¼ã€‚

    * é‡ç½®æŒ‡æ ‡çŠ¶æ€ï¼šåœ¨ä¸‹ä¸€ä¸ªè¯„ä¼°å‘¨æœŸå¼€å§‹å‰ï¼Œä½¿ç”¨reset()æ–¹æ³•æ¸…é™¤åŽ†å²çŠ¶æ€ã€‚

    ðŸ’¡ ä½¿ç”¨æŠ€å·§ä¸Žæ³¨æ„äº‹é¡¹

    * åˆ©ç”¨MetricCollectionç®¡ç†å¤šä¸ªæŒ‡æ ‡ï¼šå¦‚æžœä½ éœ€è¦åŒæ—¶è®¡ç®—å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ï¼ŒMetricCollectionä¼šéžå¸¸æ–¹ä¾¿ã€‚

    * è­¦æƒ•æ€§èƒ½å¼€é”€ï¼Œåˆç†ä½¿ç”¨compute()ï¼šé¿å…åœ¨æ¯ä¸€ä¸ªè®­ç»ƒæ‰¹æ¬¡ä¸­éƒ½è°ƒç”¨compute()æ–¹æ³•ï¼Œå› ä¸ºè¿™å¯èƒ½ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œç‰¹åˆ«æ˜¯åœ¨GPUè®­ç»ƒæ—¶ã€‚å»ºè®®åœ¨æ‰¹æ¬¡ä¸­ä½¿ç”¨update()ï¼Œä»…åœ¨å‘¨æœŸç»“æŸæ—¶è°ƒç”¨compute()ã€‚

    * æ³¨æ„æŒ‡æ ‡çš„è®¾å¤‡ä½ç½®ï¼šç¡®ä¿æŒ‡æ ‡ä¸Žè¾“å…¥æ•°æ®ä½äºŽåŒä¸€è®¾å¤‡ä¸Šï¼Œå¦åˆ™å¯èƒ½å¯¼è‡´è¿è¡Œæ—¶é”™è¯¯ã€‚åœ¨å¤§éƒ¨åˆ†æƒ…å†µä¸‹ï¼ŒTorchMetrics ä¼šè‡ªåŠ¨å¤„ç†è®¾å¤‡é—®é¢˜ï¼Œä½†æ˜Žç¡®æŒ‡å®šè®¾å¤‡æ˜¯ä¸€ä¸ªå¥½ä¹ æƒ¯ã€‚

    ðŸ’Ž æ€»ç»“

    TorchMetrics çš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºŽå…¶æ¨¡å—åŒ–è®¾è®¡ã€åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒä»¥åŠä¸ŽPyTorchç”Ÿæ€ç³»ç»Ÿçš„æ— ç¼é›†æˆã€‚

    å¸Œæœ›è¿™äº›ä¿¡æ¯èƒ½å¸®åŠ©ä½ å¿«é€Ÿä¸Šæ‰‹ TorchMetricsã€‚å¦‚æžœä½ åœ¨ä½¿ç”¨ç‰¹å®šæŒ‡æ ‡æ—¶é‡åˆ°é—®é¢˜ï¼Œæˆ–è€…æƒ³äº†è§£æ›´é«˜çº§çš„ç”¨æ³•ï¼Œæ¬¢è¿Žéšæ—¶æé—®ã€‚

* PyTorchæ•°æ®é›†åˆ’åˆ†æ–¹æ³•æ€»ç»“

    1. ä½¿ç”¨ torch.utils.data.random_splitï¼ˆæŽ¨èï¼‰

        è¿™æ˜¯æœ€ç›´æŽ¥çš„æ–¹å¼ï¼Œå¯ä»¥æŒ‰ä»»æ„æ¯”ä¾‹åˆ’åˆ†ï¼š

        ```python
        import torch
        from torch.utils.data import Dataset, DataLoader, random_split
        from torchvision import datasets, transforms

        # ç¤ºä¾‹ï¼šåŠ è½½å®Œæ•´æ•°æ®é›†
        dataset = datasets.MNIST(
            root='./data', 
            train=True,
            transform=transforms.ToTensor(),
            download=True
        )

        # æ‰‹åŠ¨åˆ’åˆ†æ¯”ä¾‹ï¼ˆ7:3ï¼‰
        train_size = int(0.7 * len(dataset))
        val_size = len(dataset) - train_size

        # éšæœºåˆ’åˆ†
        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

        # åˆ›å»º DataLoader
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

        print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}, éªŒè¯é›†å¤§å°: {len(val_dataset)}")
        ```

    2. ä½¿ç”¨ Subset æ‰‹åŠ¨é€‰æ‹©ç´¢å¼•

        å¦‚æžœä½ æƒ³æ›´ç²¾ç¡®åœ°æŽ§åˆ¶å“ªäº›æ•°æ®è¿›å…¥å“ªä¸ªé›†åˆï¼š

        ```python
        from torch.utils.data import Subset
        import numpy as np

        # åˆ›å»ºç´¢å¼•
        indices = list(range(len(dataset)))
        np.random.shuffle(indices)

        # 7:3 åˆ’åˆ†
        split = int(0.7 * len(dataset))
        train_indices = indices[:split]
        val_indices = indices[split:]

        # åˆ›å»ºå­é›†
        train_dataset = Subset(dataset, train_indices)
        val_dataset = Subset(dataset, val_indices)
        ```

    3. ä½¿ç”¨ sklearn çš„ train_test_split

        ```python
        from sklearn.model_selection import train_test_split

        # èŽ·å–æ‰€æœ‰ç´¢å¼•
        indices = list(range(len(dataset)))

        # ä½¿ç”¨ sklearn åˆ’åˆ†ï¼ˆå¯è®¾ç½®éšæœºç§å­ï¼‰
        train_indices, val_indices = train_test_split(
            indices, 
            test_size=0.3,  # éªŒè¯é›†æ¯”ä¾‹
            random_state=42,  # éšæœºç§å­
            shuffle=True
        )

        train_dataset = Subset(dataset, train_indices)
        val_dataset = Subset(dataset, val_indices)
        ```

    4. è‡ªå®šä¹‰ Dataset ç±»å®žçŽ°åˆ’åˆ†

        å¦‚æžœéœ€è¦æ›´å¤æ‚çš„åˆ’åˆ†é€»è¾‘ï¼š

        ```python
        from torch.utils.data import Dataset

        class CustomDataset(Dataset):
            def __init__(self, data, targets, is_train=True, train_ratio=0.7, seed=42):
                self.data = data
                self.targets = targets
                
                # å›ºå®šéšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
                torch.manual_seed(seed)
                indices = torch.randperm(len(data))
                
                split = int(train_ratio * len(data))
                if is_train:
                    self.indices = indices[:split]
                else:
                    self.indices = indices[split:]
            
            def __len__(self):
                return len(self.indices)
            
            def __getitem__(self, idx):
                actual_idx = self.indices[idx]
                return self.data[actual_idx], self.targets[actual_idx]

        # ä½¿ç”¨ç¤ºä¾‹
        full_data = ...  # ä½ çš„æ•°æ®
        full_targets = ...  # ä½ çš„æ ‡ç­¾

        train_dataset = CustomDataset(full_data, full_targets, is_train=True, train_ratio=0.7)
        val_dataset = CustomDataset(full_data, full_targets, is_train=False, train_ratio=0.7)
        ```

    5. ä½¿ç”¨ K-fold äº¤å‰éªŒè¯ï¼ˆæ›´å¤æ‚çš„åˆ’åˆ†ï¼‰

        ```python
        from sklearn.model_selection import KFold

        kfold = KFold(n_splits=5, shuffle=True, random_state=42)

        for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):
            print(f"Fold {fold + 1}")
            train_dataset = Subset(dataset, train_idx)
            val_dataset = Subset(dataset, val_idx)
            
            # è®­ç»ƒå’ŒéªŒè¯...
        ```

    æŽ¨èå®žè·µ

    * å¯¹äºŽç®€å•åˆ’åˆ†ï¼šä½¿ç”¨ random_split æœ€ç®€å•ç›´æŽ¥

    * éœ€è¦å¯é‡å¤æ€§ï¼šä½¿ç”¨ random_split æˆ– train_test_split å¹¶è®¾ç½® generator æˆ– random_state

    * éœ€è¦åˆ†å±‚é‡‡æ ·ï¼šä½¿ç”¨ sklearn çš„ StratifiedShuffleSplit

    * éœ€è¦å¤æ‚é€»è¾‘ï¼šè‡ªå®šä¹‰ Dataset ç±»

    æ³¨æ„äº‹é¡¹

    * è®¾ç½®éšæœºç§å­ç¡®ä¿åˆ’åˆ†å¯é‡å¤ï¼š

        ```python
        torch.manual_seed(42)  # PyTorch
        np.random.seed(42)     # NumPy
        ```

    * ä¿æŒç±»å¹³è¡¡ï¼šå¦‚æžœæ•°æ®é›†ç±»åˆ«ä¸å¹³è¡¡ï¼Œè€ƒè™‘ä½¿ç”¨åˆ†å±‚åˆ’åˆ†ï¼š

        ```python
        from sklearn.model_selection import StratifiedShuffleSplit

        # targets æ˜¯æ ‡ç­¾æ•°ç»„
        sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)
        train_idx, val_idx = next(sss.split(data, targets))
        ```

    PyTorch æœ¬èº«æä¾›äº†è¶³å¤Ÿçµæ´»çš„å·¥å…·æ¥å¤„ç†å„ç§æ•°æ®é›†åˆ’åˆ†éœ€æ±‚ï¼Œç»“åˆ sklearn å¯ä»¥æ»¡è¶³æ›´å¤æ‚çš„åœºæ™¯ã€‚

* ä¸ºä»€ä¹ˆåœ¨å«æœ‰fc layer, relu, softmaxçš„ä¸‰å±‚ç®€å•ç½‘ç»œä¸­ï¼Œè®­ç»ƒmnistæ—¶ï¼Œfc1çš„ weightçš„gradæœ‰æ—¶ä¼šå‡ºçŽ°ä¸€å¤§ç‰‡çš„0ï¼Ÿ

    è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ï¼Œä½ è§‚å¯Ÿåˆ°çš„çŽ°è±¡åœ¨è®­ç»ƒç®€å•ç¥žç»ç½‘ç»œæ—¶å¹¶ä¸å°‘è§ã€‚FC1å±‚æƒé‡æ¢¯åº¦å‡ºçŽ°å¤§ç‰‡é›¶å€¼ï¼Œé€šå¸¸æ˜¯ç”±ReLUæ¿€æ´»å‡½æ•°çš„ç‰¹æ€§å’Œç½‘ç»œåˆå§‹åŒ–/æ•°æ®å…±åŒå¯¼è‡´çš„ã€‚ä¸‹é¢æˆ‘æ¥è¯¦ç»†è§£é‡ŠåŽŸå› å’Œè§£å†³æ–¹æ¡ˆã€‚

    æ ¸å¿ƒåŽŸå› ï¼šReLUçš„â€œDead Neuronâ€é—®é¢˜

    è¿™æ˜¯æœ€å¯èƒ½çš„åŽŸå› ã€‚è®©æˆ‘ä»¬ä¸€æ­¥æ­¥åˆ†æžè¿™ä¸ªè¿‡ç¨‹ï¼š

    * ReLUçš„å‡½æ•°ç‰¹æ€§ï¼š

        ReLUå‡½æ•°çš„å®šä¹‰ä¸º f(x) = max(0, x)ã€‚å®ƒçš„æ¢¯åº¦æ˜¯ï¼š

        * å½“ x > 0 æ—¶ï¼Œæ¢¯åº¦ä¸º 1ã€‚

        * å½“ x <= 0 æ—¶ï¼Œæ¢¯åº¦ä¸º 0ã€‚

    * å‰å‘ä¼ æ’­ï¼š

        è¾“å…¥æ•°æ®ç»è¿‡ç¬¬ä¸€å±‚å…¨è¿žæŽ¥å±‚ fc1 åŽï¼Œäº§ç”Ÿè¾“å‡º z1 = W1 * X + b1ã€‚ç„¶åŽ z1 è¢«é€å…¥ReLUå‡½æ•°ï¼ša1 = ReLU(z1)ã€‚

        å¦‚æžœåœ¨ z1 çš„æŸä¸ªç»´åº¦ï¼ˆå¯¹åº”æŸä¸ªç¥žç»å…ƒï¼‰ä¸Šï¼Œå¯¹äºŽå½“å‰æ•´ä¸ªbatchçš„è¾“å…¥ï¼Œå…¶å€¼éƒ½å°äºŽæˆ–ç­‰äºŽ0ï¼Œé‚£ä¹ˆReLUå°±ä¼šå°†è¿™ä¸ªç¥žç»å…ƒçš„è¾“å‡º a1 çš„å¯¹åº”ç»´åº¦å…¨éƒ¨ç½®ä¸º0ã€‚

    * åå‘ä¼ æ’­ï¼š

        æ ¹æ®é“¾å¼æ³•åˆ™ï¼ŒæŸå¤±å‡½æ•° L å¯¹ fc1 çš„æƒé‡ W1 çš„æ¢¯åº¦ä¸ºï¼š

        âˆ‚L/âˆ‚W1 = (âˆ‚L/âˆ‚a1) * (âˆ‚a1/âˆ‚z1) * (âˆ‚z1/âˆ‚W1)

        å…¶ä¸­ (âˆ‚a1/âˆ‚z1) å°±æ˜¯ReLUçš„æ¢¯åº¦ã€‚

        * å¦‚æžœä¸€ä¸ªç¥žç»å…ƒåœ¨æœ¬æ¬¡å‰å‘ä¼ æ’­ä¸­â€œæ­»äº¡â€äº†ï¼ˆå³ z1 <= 0ï¼‰ï¼Œé‚£ä¹ˆå®ƒçš„ (âˆ‚a1/âˆ‚z1) = 0ã€‚

        * è¿™ä¼šå¯¼è‡´ âˆ‚L/âˆ‚W1 ä¸­ä¸Žè¯¥ç¥žç»å…ƒç›¸è¿žçš„æ‰€æœ‰æƒé‡æ¢¯åº¦éƒ½å˜ä¸º 0ã€‚

    ç»“æžœï¼šåœ¨æƒé‡æ¢¯åº¦çŸ©é˜µ W1.grad ä¸­ï¼Œå¯¹åº”é‚£äº›â€œæ­»äº¡ç¥žç»å…ƒâ€çš„æ•´è¡Œéƒ½ä¼šæ˜¯0ã€‚è¿™å°±è§£é‡Šäº†ä¸ºä»€ä¹ˆä½ çœ‹åˆ°çš„æ˜¯ä¸€ç‰‡0ï¼Œè€Œä¸æ˜¯é›¶æ˜Ÿå‡ ä¸ª0ã€‚

    **å…¶ä»–å¯èƒ½çš„åŽŸå› å’ŒåŠ å‰§å› ç´ **

    * ä¸åˆé€‚çš„æƒé‡åˆå§‹åŒ–

        * å¦‚æžœ fc1 çš„æƒé‡åˆå§‹å€¼è¿‡å¤§æˆ–è¿‡å°ï¼Œä¼šæ›´å®¹æ˜“å¯¼è‡´ç¥žç»å…ƒè¾“å‡º z1 çš„ç»å¯¹å€¼å¾ˆå¤§ã€‚

        * è¿‡å¤§çš„è´Ÿå€¼ä¼šç›´æŽ¥è®©ReLUè¾“å‡º0å¹¶å¯¼è‡´æ¢¯åº¦ä¸º0ã€‚

        * è¿‡å¤§çš„æ­£å€¼åœ¨åˆæœŸå¯èƒ½æ²¡é—®é¢˜ï¼Œä½†å¦‚æžœåŽç»­æŸä¸ªbatchçš„æ•°æ®ä½¿å…¶è¾“å‡ºå˜ä¸ºè´Ÿï¼Œä¹Ÿå¯èƒ½é™·å…¥â€œæ­»äº¡â€ã€‚

    * è¿‡å¤§çš„å­¦ä¹ çŽ‡

        * å¦‚æžœå­¦ä¹ çŽ‡è®¾ç½®å¾—è¿‡é«˜ï¼Œæƒé‡æ›´æ–°æ­¥é•¿ä¼šå¾ˆå¤§ã€‚

        * è¿™å¯èƒ½å¯¼è‡´æŸäº›ç¥žç»å…ƒçš„æƒé‡è¢«â€œéœ‡è¡â€åˆ°ä¸€ä¸ªåŒºåŸŸï¼Œä½¿å¾—å¯¹äºŽå¤§å¤šæ•°è¾“å…¥ï¼Œå…¶è¾“å‡º z1 éƒ½å°äºŽ0ï¼Œä»Žè€Œæ°¸ä¹…æ€§æ­»äº¡ã€‚å³ä½¿è¿™ä¸ªç¥žç»å…ƒæœ¬æ¥æ˜¯æœ‰ç”¨çš„ï¼Œä¸€æ¬¡è¿‡å¤§çš„æ›´æ–°ä¹Ÿå¯èƒ½ä½¿å…¶å†ä¹Ÿæ— æ³•æ¢å¤ã€‚

    * åç½®åˆå§‹åŒ–ä¸º0

        * å¦‚æžœå°†åç½®é¡¹ b1 åˆå§‹åŒ–ä¸º0ï¼Œé‚£ä¹ˆåœ¨è®­ç»ƒåˆæœŸï¼Œç¥žç»å…ƒçš„æ´»æ€§å®Œå…¨ç”± W1 * X å†³å®šã€‚å¦‚æžœ W1 ä¹Ÿåˆå§‹åŒ–å¾—ä¸å¥½ï¼Œä¼šåŠ å‰§ç¥žç»å…ƒæ­»äº¡çš„é—®é¢˜ã€‚

    * è¾“å…¥æ•°æ®é—®é¢˜

        * MNISTæ•°æ®æ˜¯å½’ä¸€åŒ–åˆ° [0, 1] çš„ï¼Œè¿™æœ¬èº«æ˜¯å¥½çš„ã€‚ä½†å¦‚æžœä½ çš„ç½‘ç»œç»“æž„éžå¸¸æ·±ï¼ˆè™½ç„¶ä½ çš„æ˜¯ä¸‰å±‚ï¼Œä½†åŽŸç†ç›¸é€šï¼‰ï¼Œæˆ–è€…æ•°æ®ä¸­å­˜åœ¨å¤§é‡ç›¸åŒçš„ç‰¹å¾å€¼ï¼Œä¹Ÿå¯èƒ½åœ¨æŸäº›å±‚å¯¼è‡´è¾“å‡ºä¸º0ã€‚

    å¦‚ä½•éªŒè¯å’Œè§£å†³

    éªŒè¯æ–¹æ³•

    * æ£€æŸ¥ç¥žç»å…ƒè¾“å‡ºï¼šåœ¨åå‘ä¼ æ’­ä¹‹å‰ï¼Œæ‰“å°å‡º fc1 å±‚è¾“å‡ºç»è¿‡ReLUåŽçš„å€¼ a1ã€‚ä½ ä¼šå¾ˆå¯èƒ½å‘çŽ° a1 ä¸­æœ‰å¾ˆå¤šåˆ—ï¼ˆå¯¹åº”ä¸€ä¸ªç¥žç»å…ƒï¼‰å…¨éƒ¨æ˜¯0ã€‚

        ```python
        # åœ¨ä½ çš„æ¨¡åž‹å‰å‘ä¼ æ’­ä¸­
        a1 = F.relu(self.fc1(x))
        print(â€œReLUè¾“å‡ºä¸­å€¼ä¸º0çš„æ¯”ä¾‹ï¼šâ€, (a1 == 0).float().mean().item())
        ```

        å¦‚æžœè¿™ä¸ªæ¯”ä¾‹å¾ˆé«˜ï¼ˆæ¯”å¦‚è¶…è¿‡50%ï¼‰ï¼Œå°±è¯å®žäº†â€œæ­»äº¡ReLUâ€é—®é¢˜ã€‚

    è§£å†³æ–¹æ¡ˆ

    * ä½¿ç”¨Leaky ReLUæˆ–å…¶å˜ä½“

        * Leaky ReLU: f(x) = max(0.01x, x)ã€‚å½“ x < 0 æ—¶ï¼Œå®ƒæœ‰ä¸€ä¸ªå¾ˆå°çš„æ–œçŽ‡ï¼ˆå¦‚0.01ï¼‰ï¼Œè€Œä¸æ˜¯0ã€‚è¿™ç¡®ä¿äº†æ¢¯åº¦æ°¸è¿œä¸ä¼šå®Œå…¨ä¸º0ï¼Œç»™äº†â€œæ­»äº¡â€ç¥žç»å…ƒå¤æ´»çš„æœºä¼šã€‚

        * PReLU: å°†è´ŸåŒºé—´çš„æ–œçŽ‡ä½œä¸ºä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°ã€‚

        * ELU: åœ¨è´Ÿæ•°åŒºé—´æ˜¯ä¸€ä¸ªæŒ‡æ•°æ›²çº¿ï¼Œèƒ½ç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜å¹¶ä½¿è¾“å‡ºå‡å€¼æŽ¥è¿‘0ã€‚

    * è°ƒæ•´æƒé‡åˆå§‹åŒ–

        * ä½¿ç”¨ Heåˆå§‹åŒ–ï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºé…åˆReLUè®¾è®¡çš„ã€‚å®ƒä¼šæ ¹æ®å‰ä¸€å±‚çš„ç¥žç»å…ƒæ•°é‡æ¥è°ƒæ•´åˆå§‹æƒé‡çš„æ–¹å·®ã€‚

        * åœ¨PyTorchä¸­ï¼Œå¯ä»¥è¿™æ ·å®žçŽ°ï¼š

            ```python
            torch.nn.init.kaiming_normal_(model.fc1.weight, mode=â€˜fan_inâ€™, nonlinearity=â€˜reluâ€™)
            ```

        * åŒæ—¶ï¼Œå°†åç½®åˆå§‹åŒ–ä¸ºä¸€ä¸ªå°çš„å¸¸æ•°ï¼ˆå¦‚0.01ï¼‰æœ‰æ—¶ä¹Ÿæœ‰å¸®åŠ©ï¼Œä½†è¿™åœ¨å½“ä»Šçš„å®žè·µä¸­ä¸é‚£ä¹ˆå¸¸è§ã€‚

    * é™ä½Žå­¦ä¹ çŽ‡

        * å°è¯•ä½¿ç”¨æ›´å°çš„å­¦ä¹ çŽ‡ï¼Œä¾‹å¦‚ä»Ž 0.01 é™åˆ° 0.001 æˆ– 0.0001ï¼Œçœ‹çœ‹é—®é¢˜æ˜¯å¦ç¼“è§£ã€‚é…åˆå­¦ä¹ çŽ‡è°ƒåº¦å™¨ï¼ˆå¦‚ ReduceLROnPlateauï¼‰æ•ˆæžœæ›´å¥½ã€‚

    * ä½¿ç”¨æ‰¹å½’ä¸€åŒ–

        * åœ¨ fc1 å±‚å’ŒReLUä¹‹é—´åŠ å…¥ä¸€ä¸ª BatchNorm1d å±‚ã€‚BNå±‚å¯ä»¥ç¨³å®šç½‘ç»œçš„æ¿€æ´»å€¼åˆ†å¸ƒï¼Œä½¿å…¶å‡å€¼å’Œæ–¹å·®ä¿æŒåœ¨ä¸€ä¸ªç¨³å®šçš„èŒƒå›´å†…ï¼Œä»Žè€Œå¤§å¤§å‡å°‘â€œæ­»äº¡ReLUâ€çš„å‘ç”Ÿã€‚

            ```python
            self.net = nn.Sequential(
                nn.Linear(784, 256),
                nn.BatchNorm1d(256), # åŠ å…¥BNå±‚
                nn.ReLU(),
                nn.Linear(256, 10)
            )
            ```

    æ€»ç»“

    ä½ çœ‹åˆ°çš„FC1æƒé‡æ¢¯åº¦å‡ºçŽ°å¤§ç‰‡é›¶å€¼çš„çŽ°è±¡ï¼Œæ ¹æœ¬åŽŸå› æ˜¯ReLUæ¿€æ´»å‡½æ•°å¯¼è‡´äº†ä¸€æ‰¹ç¥žç»å…ƒåœ¨æœ¬æ¬¡å‰å‘ä¼ æ’­ä¸­â€œæ­»äº¡â€ï¼Œä»Žè€Œåœ¨åå‘ä¼ æ’­æ—¶ï¼Œä¸Žè¿™äº›ç¥žç»å…ƒç›¸è¿žçš„æƒé‡æ— æ³•èŽ·å¾—æ¢¯åº¦æ›´æ–°ã€‚

    æŽ¨èçš„è§£å†³ç»„åˆæ‹³æ˜¯ï¼š

    * å°†ReLUæ›¿æ¢ä¸ºLeaky ReLUï¼ˆæœ€ç®€å•ç›´æŽ¥çš„è§£å†³æ–¹æ¡ˆï¼‰ã€‚

    * ç¡®ä¿ä½¿ç”¨äº†æ­£ç¡®çš„æƒé‡åˆå§‹åŒ–ï¼ˆå¦‚Heåˆå§‹åŒ–ï¼‰ã€‚

    * å¦‚æžœé—®é¢˜ä¾ç„¶å­˜åœ¨ï¼Œè€ƒè™‘åŠ å…¥æ‰¹å½’ä¸€åŒ–å±‚å’Œé€‚å½“è°ƒä½Žå­¦ä¹ çŽ‡ã€‚

    å¯¹äºŽMNISTè¿™æ ·çš„ç®€å•æ•°æ®é›†ï¼Œé€šå¸¸å‰ä¸¤æ­¥å°±è¶³ä»¥è§£å†³é—®é¢˜äº†ã€‚

* optim.SGD() åªèƒ½æŽ¥æ”¶ Parameter ç±»åž‹çš„æ•°æ®å—ï¼Œè¿˜æ˜¯æ™®é€šçš„ tensor ä¹Ÿå¯ä»¥ï¼Ÿ

    torch.optim.SGD() åªèƒ½æŽ¥æ”¶ nn.Parameter ç±»åž‹çš„æ•°æ®ï¼Œä¸èƒ½ç›´æŽ¥æŽ¥æ”¶æ™®é€šçš„ tensorã€‚

    éªŒè¯ç¤ºä¾‹

    ```python
    import torch
    import torch.nn as nn
    import torch.optim as optim

    # æµ‹è¯•1: ä½¿ç”¨ nn.Parameter
    class ModelWithParam(nn.Module):
        def __init__(self):
            super().__init__()
            self.weight = nn.Parameter(torch.randn(3, 3))
            
    model1 = ModelWithParam()
    optimizer1 = optim.SGD(model1.parameters(), lr=0.01)  # âœ… æ­£å¸¸å·¥ä½œ

    # æµ‹è¯•2: ä½¿ç”¨æ™®é€š tensor
    class ModelWithTensor(nn.Module):
        def __init__(self):
            super().__init__()
            self.weight = torch.randn(3, 3)  # æ™®é€š tensor
            
    model2 = ModelWithTensor()
    try:
        optimizer2 = optim.SGD(model2.parameters(), lr=0.01)
        print("æ™®é€š tensor ä¹Ÿå¯ä»¥")
    except:
        print("æ™®é€š tensor ä¸è¡Œ")  # âŒ è¿™é‡Œä¸ä¼šæŠ¥é”™ï¼Œä½†å‚æ•°åˆ—è¡¨ä¸ºç©º

    # æ£€æŸ¥å‚æ•°åˆ—è¡¨
    print("ModelWithParam å‚æ•°:", list(model1.parameters()))  # æœ‰å‚æ•°
    print("ModelWithTensor å‚æ•°:", list(model2.parameters()))  # ç©ºåˆ—è¡¨
    ```

    ä¸ºä»€ä¹ˆéœ€è¦ Parameter

    ```python
    # æ¼”ç¤ºé—®é¢˜
    model = ModelWithTensor()

    # è™½ç„¶ä¸ä¼šæŠ¥é”™ï¼Œä½†ä¼˜åŒ–å™¨æ²¡æœ‰å‚æ•°å¯ä¼˜åŒ–
    optimizer = optim.SGD(model.parameters(), lr=0.01)
    print("å¯ä¼˜åŒ–å‚æ•°æ•°é‡:", len(list(optimizer.param_groups[0]['params'])))  # 0

    # è®­ç»ƒæ—¶ä¸ä¼šæ›´æ–°ä»»ä½•å‚æ•°
    x = torch.randn(1, 3)
    output = model(x)
    loss = output.sum()
    loss.backward()
    optimizer.step()  # ä¸ä¼šæ›´æ–°ä»»ä½•å‚æ•°
    ```

    æ­£ç¡®çš„ç”¨æ³•

    ```python
    class CorrectModel(nn.Module):
        def __init__(self):
            super().__init__()
            # æ­£ç¡®ï¼šä½¿ç”¨ nn.Parameter
            self.weight = nn.Parameter(torch.randn(3, 3))
            self.bias = nn.Parameter(torch.zeros(3))
            
            # é”™è¯¯ï¼šæ™®é€š tensor ä¸ä¼šè¢«ä¼˜åŒ–
            self.non_learnable = torch.randn(3, 3)  # è¿™ä¸ä¼šè¢«è®­ç»ƒ
            
        def forward(self, x):
            return x @ self.weight + self.bias

    model = CorrectModel()
    optimizer = optim.SGD(model.parameters(), lr=0.01)

    # æŸ¥çœ‹å®žé™…ä¼šè¢«ä¼˜åŒ–çš„å‚æ•°
    for name, param in model.named_parameters():
        print(f"å¯å­¦ä¹ å‚æ•°: {name}, å½¢çŠ¶: {param.shape}")
    ```

    ç‰¹æ®Šæƒ…å†µï¼šæ‰‹åŠ¨ä¼ é€’å‚æ•°

    ```python
    # å¦‚æžœä½ æœ‰éž Parameter çš„ tensor ä¹Ÿæƒ³ä¼˜åŒ–ï¼Œå¯ä»¥æ‰‹åŠ¨åŒ…è£…
    tensor1 = torch.randn(3, 3, requires_grad=True)
    tensor2 = torch.randn(2, 2, requires_grad=True)

    # æ–¹æ³•1: è½¬æ¢ä¸º Parameter
    param1 = nn.Parameter(tensor1)
    param2 = nn.Parameter(tensor2)

    optimizer = optim.SGD([param1, param2], lr=0.01)

    # æ–¹æ³•2: ç›´æŽ¥ä¼ é€’ï¼Œä½†éœ€è¦ requires_grad=True
    optimizer = optim.SGD([tensor1, tensor2], lr=0.01)  # è¿™æ ·ä¹Ÿå¯ä»¥ï¼Œä½†ä¸æŽ¨è
    ```

    æ€»ç»“

    * optim.SGD() ä¸»è¦è®¾è®¡ç”¨äºŽ nn.Parameter å¯¹è±¡

    * æ™®é€š tensor ä¸ä¼šè¢« model.parameters() åŒ…å«ï¼Œå› æ­¤ä¼˜åŒ–å™¨çœ‹ä¸åˆ°å®ƒä»¬

    * å³ä½¿æ‰‹åŠ¨ä¼ é€’æ™®é€š tensor ç»™ä¼˜åŒ–å™¨ï¼Œä¹Ÿä¸å¦‚ä½¿ç”¨ nn.Parameter è§„èŒƒ

    * åœ¨è‡ªå®šä¹‰æ¨¡å—ä¸­ï¼Œæ‰€æœ‰éœ€è¦å­¦ä¹ çš„å‚æ•°éƒ½åº”è¯¥ç”¨ nn.Parameter åŒ…è£…

    æœ€ä½³å®žè·µæ˜¯å§‹ç»ˆä½¿ç”¨ nn.Parameter æ¥å®šä¹‰å¯å­¦ä¹ å‚æ•°ï¼Œè¿™æ ·å¯ä»¥ç¡®ä¿ï¼š

    * å‚æ•°è¢«æ­£ç¡®æ³¨å†Œåˆ°æ¨¡å—ä¸­

    * ä¼˜åŒ–å™¨èƒ½å¤Ÿè¯†åˆ«å’Œæ›´æ–°å‚æ•°

    * å‚æ•°èƒ½å¤Ÿéšæ¨¡åž‹ä¸€èµ·ç§»åŠ¨åˆ°ä¸åŒè®¾å¤‡ï¼ˆGPU/CPUï¼‰

    * ä¾¿äºŽå‚æ•°ç®¡ç†å’Œä¿å­˜

* `nn.Parameter()`

    åœ¨ PyTorch ä¸­ï¼Œnn.Parameter() æ˜¯ä¸€ä¸ªç”¨äºŽå°†å¼ é‡åŒ…è£…ä¸ºæ¨¡åž‹å‚æ•°çš„ç±»ï¼Œå®ƒæ˜¯ torch.Tensor çš„å­ç±»ã€‚

    åŽŸåž‹

    ```python
    torch.nn.Parameter(data=None, requires_grad=True)
    ```

    å‚æ•°ï¼š

    * data (Tensor): è¦åŒ…è£…ä¸ºå‚æ•°çš„å¼ é‡

    * requires_grad (bool, å¯é€‰): æ˜¯å¦éœ€è¦åœ¨åå‘ä¼ æ’­ä¸­è®¡ç®—æ¢¯åº¦ï¼Œé»˜è®¤ä¸º True

    ç”¨æ³•

    1. åŸºæœ¬ç”¨æ³•

        ```python
        import torch
        import torch.nn as nn

        # åˆ›å»ºä¸€ä¸ªå¼ é‡å¹¶åŒ…è£…ä¸ºå‚æ•°
        tensor = torch.randn(3, 3)
        param = nn.Parameter(tensor)

        print(type(param))  # <class 'torch.nn.parameter.Parameter'>
        print(param.requires_grad)  # True
        ```

    2. åœ¨è‡ªå®šä¹‰æ¨¡å—ä¸­ä½¿ç”¨

        ```python
        class MyModel(nn.Module):
            def __init__(self):
                super(MyModel, self).__init__()
                # ä½¿ç”¨ nn.Parameter å®šä¹‰å¯å­¦ä¹ å‚æ•°
                self.weight = nn.Parameter(torch.randn(10, 5))
                self.bias = nn.Parameter(torch.zeros(5))
                
            def forward(self, x):
                return x @ self.weight + self.bias

        model = MyModel()
        ```

    3. ä¸Žæ™®é€šå¼ é‡çš„åŒºåˆ«

        ```python
        class CompareModel(nn.Module):
            def __init__(self):
                super(CompareModel, self).__init__()
                # ä½¿ç”¨ nn.Parameter - ä¼šè¢«è‡ªåŠ¨æ³¨å†Œä¸ºå‚æ•°
                self.param_weight = nn.Parameter(torch.randn(3, 3))
                
                # æ™®é€šå¼ é‡ - ä¸ä¼šè¢«æ³¨å†Œä¸ºå‚æ•°
                self.tensor_weight = torch.randn(3, 3)
                
            def forward(self, x):
                return x @ self.param_weight

        model = CompareModel()

        # æŸ¥çœ‹æ¨¡åž‹å‚æ•°
        for name, param in model.named_parameters():
            print(name)  # åªè¾“å‡º "param_weight"ï¼Œä¸ä¼šè¾“å‡º "tensor_weight"
            ```

    4. å‚æ•°è®¿é—®å’Œç®¡ç†

        ```python
        model = MyModel()

        # è®¿é—®æ‰€æœ‰å‚æ•°
        print(list(model.parameters()))

        # èŽ·å–å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        print(f"æ€»å‚æ•°æ•°é‡: {total_params}")

        # å‚æ•°æ¢¯åº¦ç®¡ç†
        with torch.no_grad():
            # åœ¨ä¸è®¡ç®—æ¢¯åº¦çš„æƒ…å†µä¸‹æ›´æ–°å‚æ•°
            model.weight += 0.1
        ```

    ä¸»è¦ç‰¹ç‚¹

    * è‡ªåŠ¨æ³¨å†Œ: åœ¨ nn.Module ä¸­ä½¿ç”¨æ—¶ï¼Œnn.Parameter ä¼šè¢«è‡ªåŠ¨æ·»åŠ åˆ°æ¨¡å—çš„å‚æ•°åˆ—è¡¨ä¸­

    * æ¢¯åº¦è®¡ç®—: é»˜è®¤éœ€è¦æ¢¯åº¦è®¡ç®—ï¼Œå‚ä¸Žåå‘ä¼ æ’­

    * è®¾å¤‡åŒæ­¥: å½“æ¨¡å—ç§»åŠ¨åˆ° GPU æ—¶ï¼Œå‚æ•°ä¹Ÿä¼šè‡ªåŠ¨ç§»åŠ¨

    * ä¼˜åŒ–å™¨è¯†åˆ«: ä¼˜åŒ–å™¨èƒ½å¤Ÿè‡ªåŠ¨è¯†åˆ«å¹¶æ›´æ–°è¿™äº›å‚æ•°

    æ³¨æ„äº‹é¡¹

    * åªæœ‰ nn.Parameter åŒ…è£…çš„å¼ é‡æ‰ä¼šè¢« model.parameters() åŒ…å«

    * åœ¨è‡ªå®šä¹‰æ¨¡å—ä¸­ï¼Œåº”è¯¥ä½¿ç”¨ nn.Parameter æ¥å®šä¹‰æ‰€æœ‰éœ€è¦å­¦ä¹ çš„å‚æ•°

    * å‚æ•°é»˜è®¤éœ€è¦æ¢¯åº¦ï¼Œå¦‚æžœä¸éœ€è¦å¯ä»¥è®¾ç½® requires_grad=False

    nn.Parameter æ˜¯æž„å»ºå¯è®­ç»ƒç¥žç»ç½‘ç»œæ¨¡åž‹çš„åŸºç¡€ç»„ä»¶ï¼Œå®ƒç¡®ä¿äº†å‚æ•°èƒ½å¤Ÿæ­£ç¡®åœ°è¢«ä¼˜åŒ–å™¨è¯†åˆ«å’Œæ›´æ–°ã€‚

* dataloader è¿”å›žçš„æ˜¯`[inputs, gts]`

    è€Œ dataset è¿”å›žçš„æ˜¯`(input, gt)`

    å¦‚æžœ dataset è¿”å›žçš„æ˜¯`(x_1, x_2, x_3)`ï¼Œdataloader è¿”å›žçš„ä¼šä¸ä¼šæ˜¯`(x_1s, x_2s, x_3s)`ï¼Ÿ

* Negative Log Likelihood Loss

    After the output of the softmax layer is calculated (i.e. a value between 0 and 1), negative log is calculated of that value. The final layer combined is called as log-softmax layer. Generally, it is used in multi-class classification problems.

    Formula:

    $$\mathrm{NegativeLogLikelihoodLoss}(x, \mathrm{target}) = âˆ’ \frac 1 N \sum_i \logâ¡(x_{target_i})$$

    Here,

    * $x$ represents the predicted values,

    * target represents the ground truth or target values

    syntax:

    ```py
    torch.nn.NLLLoss(weight=None, size_average=None, ignore_index=- 100, reduce=None, reduction='mean')
    ```

    ```py
    import torch
    import torch.nn as nn

    # size of input (N x C) is = 3 x 5
    input = torch.randn(3, 5, requires_grad=True)
    # every element in target should have 0 <= value < C
    target = torch.tensor([1, 0, 4])
    #initialising loss function
    m = nn.LogSoftmax(dim=1)
    nll_loss = nn.NLLLoss()
    output = nll_loss(m(input), target)
    #backpropagation
    output.backward()
    #printing output
    print(output)
    ```

* ä¸ºä»€ä¹ˆè®¡ç®—å·ç§¯æ—¶ï¼Œéœ€è¦ç¿»è½¬å·ç§¯æ ¸

    è¿žç»­å·ç§¯æ•°å­¦å®šä¹‰ï¼š

    $$(f * g)(t) = \int f(\tau) g(t - \tau) \mathrm d \tau$$

    æ³¨æ„å…¬å¼ä¸­çš„ g(t - Ï„) - Ï„ æ˜¯è´Ÿå·ï¼Œè¿™æ„å‘³ç€å·ç§¯æ ¸éœ€è¦ç¿»è½¬ï¼ˆç¿»è½¬180åº¦ï¼‰ã€‚

    å®žé™…ä¸Šï¼Œ$g(t)$ æ˜¯å¯¹ä¸€ä¸ªå†²å‡»çš„å“åº”ï¼Œåœ¨çŽ°å®žä¸­æ˜¯ä¸€ä¸ªç”µè·¯ moduleï¼Œå®ƒä¼šå¤„ç†å…ˆåˆ°è¾¾çš„ä¿¡å·ï¼Œç„¶åŽå¤„ç†åŽåˆ°è¾¾çš„ä¿¡å·ï¼Œå› æ­¤éœ€è¦å…ˆå¤„ç† $f(t)$ å·¦è¾¹çš„æ•°æ®ï¼Œç„¶åŽä»Žå·¦åˆ°å³ä¾æ¬¡å¤„ç†æ‰€æœ‰æ•°æ®ï¼Œå› æ­¤æˆ‘ä»¬ç¿»è½¬çš„åº”è¯¥æ˜¯ $f(t)$ã€‚

    ä½†æ˜¯å¦‚æžœæˆ‘ä»¬æŠŠåŽŸä¿¡å·çœ‹ä½œé™æ­¢çš„ï¼Œè®©ä¸€ä¸ª filter åœ¨ä¸Šé¢ä»Žå·¦åˆ°å³æ»‘è¿‡ï¼Œä¸ºäº†å®žçŽ°å’Œä¸Šé¢ç›¸åŒçš„æ•ˆæžœï¼Œè¿™ä¸ªæ—¶å€™å°±éœ€è¦ç¿»è½¬ $g(t)$ äº†ã€‚

    example:

    ```py
    import numpy as np

    # åŽŸå§‹ä¿¡å·
    signal = np.array([1, 2, 3, 4, 5])
    # å·ç§¯æ ¸
    kernel = np.array([1, 2, 1])

    # 1. æ•°å­¦å·ç§¯ï¼ˆéœ€è¦ç¿»è½¬ï¼‰
    kernel_flipped = kernel[::-1]  # ç¿»è½¬ï¼š[1, 2, 1] â†’ [1, 2, 1]
    # np.convolve() æ˜¯å¦ä¼šè‡ªåŠ¨ç¿»è½¬å·ç§¯æ ¸ï¼Ÿ
    conv_result = np.convolve(signal, kernel_flipped, mode='valid')

    # 2. äº’ç›¸å…³ï¼ˆä¸ç¿»è½¬ï¼‰
    corr_result = np.correlate(signal, kernel, mode='valid')

    print("ç¿»è½¬åŽçš„å·ç§¯æ ¸ï¼š", kernel_flipped)
    print("æ•°å­¦å·ç§¯ç»“æžœï¼š", conv_result)  # [1*1 + 2*2 + 3*1, 2*1 + 3*2 + 4*1, ...]
    print("äº’ç›¸å…³ç»“æžœï¼š", corr_result)    # [1*1 + 2*1 + 3*2, 2*1 + 3*1 + 4*2, ...]
    ```

    ä¸ºä»€ä¹ˆæ•°å­¦å®šä¹‰è¦ç¿»è½¬ï¼Ÿ

    åªæœ‰åŒ…å«ç¿»è½¬çš„å·ç§¯æ‰æ»¡è¶³ï¼š

    * äº¤æ¢å¾‹ï¼š$f * g = g * f$

    * ç»“åˆå¾‹ï¼š$(f * g) * h = f * (g * h)$

    * å¹³ç§»ä¸å˜æ€§ç­‰æ•°å­¦æ€§è´¨

* 7ç‚¹ç§»åŠ¨å¹³å‡

    æŒ‡æ»‘åŠ¨çª—å£ä¸­å…±æœ‰ 7 ä¸ªæ•°æ®ã€‚æ¯æ¬¡è®¡ç®—æ—¶ï¼Œå–å½“å‰æ•°æ®ç‚¹åŠå…¶å‰åŽå„ 3 ä¸ªç‚¹ï¼ˆå…± 7 ä¸ªç‚¹ï¼‰ã€‚å°†è¿™ 7 ä¸ªæ•°æ®ç‚¹ç›¸åŠ ï¼Œç„¶åŽé™¤ä»¥ 7ã€‚ç›¸å½“äºŽä¸€ä¸ªä½Žé€šæ»¤æ³¢å™¨ï¼Œå¹³æ»‘æŽ‰é«˜é¢‘å™ªå£°ã€‚

    å·ç§¯æ ¸ä»£ç ï¼š`np.ones(7) / 7`

    ç›¸å½“äºŽï¼š`[1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 1/7]`

    7 ç‚¹å¹¶æ²¡æœ‰ä»€ä¹ˆç‰¹æ®Šçš„æ„ä¹‰ï¼Œé€šå¸¸ä¸Ž 5 ç‚¹ï¼Œ9 ç‚¹ï¼Œ15 ç‚¹ç­‰åšæ¯”è¾ƒï¼Œç»“åˆå®žé™…é—®é¢˜èµ‹äºˆæ„ä¹‰ã€‚

    é€šå¸¸é€‰æ‹©å¥‡æ•°ç‚¹ï¼Œè€Œä¸æ˜¯å¶æ•°ç‚¹ã€‚å› ä¸ºå¥‡æ•°ç‚¹ä¸­å¿ƒå¯¹ç§°ï¼Œè¾“å‡ºä¸Žè¾“å…¥æ—¶é—´å¯¹é½ï¼Œå¶æ•°ç‚¹ä¼šäº§ç”ŸåŠä¸ªæ—¶é—´å•ä½çš„ç›¸ä½åç§»ã€‚

    example:

    ```py
    import numpy as np
    import matplotlib.pyplot as plt
    from matplotlib.axes import Axes

    x = np.arange(0, 4 * np.pi, 0.1)
    y = np.sin(x) + np.random.normal(loc=0, scale=0.1, size=x.shape)

    kernel_size = 7
    mean_kernel = np.ones(kernel_size) / kernel_size
    mean_kernel = mean_kernel[::-1]

    y_1 = np.pad(y, kernel_size // 2)  # [0, 0, 0, y, 0, 0, 0]
    y_2 = np.zeros_like(y)
    for i in range(len(y)):
        y_2[i] = np.sum(y_1[i:i+kernel_size] * mean_kernel)

    fig, axes = plt.subplots(2, 1)
    ax: Axes = axes[0]
    ax.plot(x, y, 'b')
    ax = axes[1]
    ax.plot(x, y_2, 'r')
    plt.show()
    ```

    output:

    ![ref_42/pic_1](../../Reference_resources/ref_42/pic_1.png)

    æ­¤æ—¶åœ¨å‰åŽå„ padding `kernel_size // 2`ä¸ª 0 å…ƒç´ ï¼Œæ­£å¥½æŠŠæ¯ä¸ªæ—¶åˆ»æ”¾åœ¨ kernel window çš„æ­£ä¸­é—´ã€‚

* é«˜æ–¯åˆ†å¸ƒï¼ˆæ­£æ€åˆ†å¸ƒï¼‰çš„æŽ¨å¯¼

    é«˜æ–¯å‡½æ•°ï¼ˆæ­£æ€åˆ†å¸ƒï¼‰çš„å‘çŽ°æ˜¯ç§‘å­¦å²ä¸Šçš„é‡è¦é‡Œç¨‹ç¢‘ï¼Œç”±é«˜æ–¯å’Œæ‹‰æ™®æ‹‰æ–¯åˆ†åˆ«ç‹¬ç«‹å‘çŽ°ã€‚æœ€åˆçš„åŠ¨æœºæ˜¯è§£å†³æµ‹é‡è¯¯å·®é—®é¢˜ã€‚

    * åŸºæœ¬å‡è®¾ï¼ˆå…¬ç†ç³»ç»Ÿï¼‰

        é«˜æ–¯å‡½æ•°çš„æŽ¨å¯¼åŸºäºŽä»¥ä¸‹åˆç†å‡è®¾ï¼š

        1. è¯¯å·®å¯¹ç§°æ€§å‡è®¾

            è¯¯å·®å›´ç»•çœŸå€¼å¯¹ç§°åˆ†å¸ƒï¼š

            $p(\varepsilon) = p(- \varepsilon)$

            å…¶ä¸­$\varepsilon = æµ‹é‡å€¼ - çœŸå€¼$

        2. æœ€å¤§ä¼¼ç„¶åŽŸç†

            å‡½æ•°ä¸­æœ€å¯èƒ½çš„å‚æ•°å€¼åº”è¯¥æ˜¯ä½¿æ‰€æœ‰è§‚æµ‹å€¼å‡ºçŽ°çš„æ¦‚çŽ‡ä¹˜ç§¯æœ€å¤§çš„å€¼ã€‚

        3. ç‹¬ç«‹åŒåˆ†å¸ƒå‡è®¾

            å¤šæ¬¡æµ‹é‡è¯¯å·®ç›¸äº’ç‹¬ç«‹ã€‚

    * æŽ¨å¯¼è¿‡ç¨‹

        1. è®¾å®šé—®é¢˜æ¡†æž¶

            è®¾ï¼š

            * çœŸå€¼ï¼š$\mu$

            * æµ‹é‡å€¼ï¼š$x_1$, $x_2$, $\dots$, $x_n$

            * è¯¯å·®ï¼š$\varepsilon_i = x_i - \mu$

            * è¯¯å·®æ¦‚çŽ‡å¯†åº¦å‡½æ•°ï¼š$\varphi(\varepsilon)$

            æ ¹æ®ç‹¬ç«‹æ€§ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—å‡ºè§‚æµ‹åˆ°è¿™äº›æ•°æ®çš„è”åˆæ¦‚çŽ‡ï¼š

            $$L(\mu) = \varphi(x_1 - \mu) \cdot \varphi(x_2 - \mu) \cdot \dots \cdot \varphi(x_n - \mu)$$

        2. æœ€å¤§ä¼¼ç„¶ä¼°è®¡

            å¯¹$L(\mu)$å–å¯¹æ•°ï¼ˆå–å¯¹æ•°åŽï¼Œæ±‚åˆ°çš„æžå€¼å’ŒåŽŸé—®é¢˜ç­‰ä»·å—ï¼Ÿæ˜¯å¦å­˜åœ¨å–å¯¹æ•°åŽï¼Œæžå€¼ä¸ŽåŽŸå‡½æ•°ä¸ç›¸åŒçš„æƒ…å†µï¼Ÿï¼‰ï¼š

            $$\ln L(\mu) = \sum_i \ln \varphi(x_i - \mu)$$

            æœ€å¤§åŒ–æ¡ä»¶ï¼ˆä¸ºä»€ä¹ˆæ˜¯æœ€å¤§åŒ–ï¼Œè€Œä¸æ˜¯æœ€å°åŒ–ï¼Ÿï¼‰ï¼š

            $$\frac{\mathrm d \ [\ln L(\mu)]} {\mathrm d \ \mu} = 0$$

            å³ï¼š

            $$\sum \frac{\varphi'(x_i - \mu)} {\varphi(x_i - \mu)} = 0$$

        3. å¼•å…¥å…³é”®å‡½æ•°

            ä»¤ï¼š

            $$\Psi(\varepsilon) = \frac{\varphi'(\varepsilon)} {\varphi(\varepsilon)}$$

            åˆ™æ–¹ç¨‹å˜ä¸ºï¼š

            $$\sum_i \Psi(x_i - \mu) = 0$$

        4. é«˜æ–¯çš„å…³é”®æ´žå¯Ÿ

            é«˜æ–¯æ„è¯†åˆ°ï¼Œå¦‚æžœå–ç®—æœ¯å¹³å‡ä½œä¸º $\mu$ çš„ä¼°è®¡ï¼š

            $$\hat \mu = \frac{x_1 + x_2 + \dots + x_n} {n}$$

            é‚£ä¹ˆå¯¹äºŽä»»æ„ $a$, $b$ ï¼ˆä¸ºä»€ä¹ˆï¼Ÿä¸æ‡‚ï¼‰ï¼š

            $$\sum \Psi(x_i - (a \cdot x_j + b \cdot x_k)) = 0$$

            è¿™è¦æ±‚ $\Psi$ å¿…é¡»æ˜¯çº¿æ€§å‡½æ•° (ä¾ç„¶ä¸æ‡‚)ï¼š

            $$\Psi (\varepsilon) = k \cdot \varepsilon$$

        5. æ±‚è§£å¾®åˆ†æ–¹ç¨‹

            ç”±

            $$\Psi(\varepsilon) = \varphi'(\varepsilon) / \varphi(\varepsilon) = k \cdot \varepsilon$$

            è§£è¿™ä¸ªå¾®åˆ†æ–¹ç¨‹ï¼š

            $$\mathrm d \, \varphi / \varphi = k \varepsilon \ \mathrm d \, \varepsilon$$

            ä¸¤è¾¹ç§¯åˆ†ï¼š

            $$\ln \varphi(\varepsilon) = (k / 2) \varepsilon^2 + C$$

            æ‰€ä»¥ï¼š

            $$Ï†(Îµ) = A \cdot \exp(kÎµÂ²/2)$$

        6. ç¡®å®šå¸¸æ•°

            å› ä¸ºæ¦‚çŽ‡å¯†åº¦å‡½æ•°å¿…é¡»æ»¡è¶³ï¼š

                å½’ä¸€åŒ–ï¼šâˆ«Ï†(Îµ)dÎµ = 1

                å¯¹ç§°æ€§ï¼šÏ†(Îµ) = Ï†(-Îµ)

                è¡°å‡æ€§ï¼šå½“|Îµ|â†’âˆžæ—¶ï¼ŒÏ†(Îµ)â†’0

            è¿™è¦æ±‚kå¿…é¡»ä¸ºè´Ÿæ•°ï¼Œä»¤ k = -1/ÏƒÂ²

            åˆ™ï¼š

            Ï†(Îµ) = A Â· exp(-ÎµÂ²/(2ÏƒÂ²))

        7. å½’ä¸€åŒ–è®¡ç®—

            è®¡ç®—å½’ä¸€åŒ–å¸¸æ•°Aï¼š

            âˆ«_{-âˆž}^{âˆž} A Â· exp(-ÎµÂ²/(2ÏƒÂ²)) dÎµ = 1

            åˆ©ç”¨é«˜æ–¯ç§¯åˆ†å…¬å¼ï¼š

            âˆ«_{-âˆž}^{âˆž} exp(-Î±xÂ²) dx = âˆš(Ï€/Î±)

            ä»¤ Î± = 1/(2ÏƒÂ²)ï¼Œåˆ™ï¼š

            âˆ« exp(-ÎµÂ²/(2ÏƒÂ²)) dÎµ = âˆš(2Ï€ÏƒÂ²)

            æ‰€ä»¥ï¼š

            A = 1/âˆš(2Ï€ÏƒÂ²)

        8. æœ€ç»ˆå½¢å¼

            å¾—åˆ°æ ‡å‡†çš„é«˜æ–¯åˆ†å¸ƒï¼š

            Ï†(Îµ) = 1/âˆš(2Ï€ÏƒÂ²) Â· exp(-ÎµÂ²/(2ÏƒÂ²))

* æ¢¯åº¦æƒé‡

    åœ¨ PyTorch ä¸­ï¼Œy.backward() çš„å‚æ•°è¡¨ç¤ºçš„æ˜¯æ¢¯åº¦æƒé‡ï¼ˆgradient weightsï¼‰ï¼Œä¹Ÿç§°ä¸º v æˆ– grad_outputã€‚

    y.backward(gradient) ä¸­çš„ gradient å‚æ•°è¡¨ç¤º y å¯¹è‡ªèº«çš„æ¢¯åº¦ï¼Œå³ âˆ‚y/âˆ‚yã€‚åœ¨æ ‡é‡æƒ…å†µä¸‹é€šå¸¸é»˜è®¤ä¸º 1ï¼Œä½†åœ¨å¼ é‡æƒ…å†µä¸‹éœ€è¦æ˜¾å¼æŒ‡å®šã€‚

    å¦‚æžœ `y` æ˜¯å‘é‡ï¼ˆå¤šç»´ï¼‰æ—¶ï¼Œtorch ä¼šè‡ªåŠ¨æž„é€ ä¸€ä¸ª sum è¡¨è¾¾å¼ï¼Œ`y' = w_1 * y_1 + w_2 * y_2 + ... + w_n * y_n`ï¼Œæœ€ç»ˆæ±‚å¯¼æ˜¯å¯¹ `y'` æ±‚å¯¼ã€‚

    `y.backward(v)`è®¡ç®—çš„æ˜¯`âˆ‚(vÂ·y)/âˆ‚x = váµ€ Â· âˆ‚y/âˆ‚x`ã€‚

    å½“ y æ˜¯å¤šç»´å¼ é‡æ—¶ï¼ŒPyTorch ä¼šï¼š

    1. è‡ªåŠ¨æž„é€ ä¸€ä¸ªæ ‡é‡å‡½æ•°ï¼š$y' = w_1 \cdot y_1 + w_2 \cdot y_2 + \dots + w_n \cdot y_n$

    2. ä½¿ç”¨ç”¨æˆ·æä¾›çš„æƒé‡ï¼š$w = [w_1, w_2, \dots, w_n]$ å°±æ˜¯ `backward()` ä¸­çš„å‚æ•°

    3. å¯¹ $y'$ æ±‚å¯¼ï¼šæœ€ç»ˆè®¡ç®—çš„æ˜¯ $\partial y' / \partial x$ï¼Œè€Œä¸æ˜¯ç›´æŽ¥è®¡ç®— $\partial y / \partial x$

    example:

    ```py
    from hlc_utils import *

    x = t.tensor([1, 2, 3], dtype=t.float, requires_grad=True)
    y = x**2
    y.backward(t.tensor([1, 1, 1]))
    print('y:', y)
    print('x.grad:', x.grad)

    x.grad.zero_()
    y = x**2
    y[0].backward()
    # y[1].backward()  # error, è®¡ç®—å›¾åªèƒ½ backward ä¸€æ¬¡
    # y[2].backward()
    print('y:', y)
    print('x.grad:', x.grad)
    ```

    output:

    ```
    y: tensor([1., 4., 9.], grad_fn=<PowBackward0>)
    x.grad: tensor([2., 4., 6.])
    y: tensor([1., 4., 9.], grad_fn=<PowBackward0>)
    x.grad: tensor([2., 0., 0.])
    ```

    è¿˜å¯ä»¥è°ƒç”¨`t.autograd.backward()`:

    ```py
    x = t.tensor([1, 2, 3], dtype=t.float, requires_grad=True)
    y = x**2
    t.autograd.backward([y[0], y[1]])
    print('y:', y)
    print('x.grad:', x.grad)
    ```

    output:

    ```
    y: tensor([1., 4., 9.], grad_fn=<PowBackward0>)
    x.grad: tensor([2., 4., 6.])
    y: tensor([1., 4., 9.], grad_fn=<PowBackward0>)
    x.grad: tensor([2., 4., 0.])
    ```

* è¿›åŒ–ç®—æ³• es

    æ ¸å¿ƒæ€æƒ³ï¼šä»¿ç”Ÿâ€œä¼˜èƒœåŠ£æ±°â€

    è¿›åŒ–ç®—æ³•æ˜¯ä¸€ç§å—ç”Ÿç‰©è¿›åŒ–è®ºï¼ˆç‰©ç«žå¤©æ‹©ï¼Œé€‚è€…ç”Ÿå­˜ï¼‰å¯å‘è€Œè®¾è®¡çš„ä¼˜åŒ–ç®—æ³•ã€‚å®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šé€šè¿‡æ¨¡æ‹Ÿè‡ªç„¶é€‰æ‹©ã€äº¤å‰ï¼ˆæ‚äº¤ï¼‰å’Œå˜å¼‚ç­‰è¿‡ç¨‹ï¼Œè®©ä¸€ä¸ªâ€œç§ç¾¤â€åœ¨ä»£ä»£ç¹è¡ä¸­ä¸æ–­è¿›åŒ–ï¼Œæœ€ç»ˆæ‰¾åˆ°å¤æ‚é—®é¢˜çš„æœ€ä¼˜è§£æˆ–æ»¡æ„è§£ã€‚

    ä¸€ä¸ªç”ŸåŠ¨çš„æ¯”å–»ï¼šå¯»æ‰¾æœ€é«˜å³°

    å‡è®¾ä½ çš„ä»»åŠ¡æ˜¯åœ¨ä¸€ä¸ªå®Œå…¨æ¼†é»‘ã€åœ°å½¢å¤æ‚ï¼ˆæœ‰å¾ˆå¤šå±±ä¸˜å’Œå±±è°·ï¼‰çš„åŒºåŸŸé‡Œæ‰¾åˆ°æœ€é«˜ç‚¹ã€‚ä½ æ²¡æœ‰åœ°å›¾ï¼Œåªèƒ½é æ´¾å‡ºä¸€æ”¯â€œæŽ¢é™©é˜Ÿâ€åŽ»æ‘¸ç´¢ã€‚

        åˆå§‹åŒ–ç§ç¾¤ï¼ˆç¬¬ä¸€ä»£æŽ¢é™©é˜Ÿï¼‰ï¼š

            ä½ éšæœºåœ°åœ¨åœ°å›¾ä¸Šæ’’ä¸‹ä¸€æŠŠâ€œæŽ¢é™©è€…â€ï¼ˆè¿™å°±æ˜¯åˆå§‹ç§ç¾¤ï¼‰ã€‚æ¯ä¸ªæŽ¢é™©è€…éƒ½æœ‰ä¸€ä¸ªä½ç½®åæ ‡ï¼ˆè¿™å°±æ˜¯ä¸€ä¸ªâ€œæŸ“è‰²ä½“â€æˆ–â€œè§£â€ï¼‰ã€‚

        è¯„ä¼°é€‚åº”åº¦ï¼ˆåˆ¤æ–­è°ç«™å¾—é«˜ï¼‰ï¼š

            ä½ è®©æ¯ä¸ªæŽ¢é™©è€…æŠ¥å‘Šä»–ä»¬æ‰€åœ¨ä½ç½®çš„æµ·æ‹”é«˜åº¦ã€‚æµ·æ‹”è¶Šé«˜ï¼Œä»£è¡¨ä»–çš„â€œé€‚åº”åº¦â€è¶Šå¥½ã€‚

        é€‰æ‹©ï¼ˆä¼˜èƒœåŠ£æ±°ï¼‰ï¼š

            ä½ æ›´å€¾å‘äºŽé€‰æ‹©é‚£äº›ç«™å¾—é«˜çš„æŽ¢é™©è€…ä½œä¸ºâ€œçˆ¶æ¯â€ï¼Œè®©ä»–ä»¬ç¹è¡ä¸‹ä¸€ä»£ã€‚ç«™å¾—è¶Šä½Žçš„äººï¼Œè¢«é€‰ä¸­çš„å‡ çŽ‡å°±è¶Šå°ã€‚è¿™ä¿è¯äº†ä¼˜ç§€çš„åŸºå› ï¼ˆä½ç½®ä¿¡æ¯ï¼‰èƒ½ä¼ é€’ä¸‹åŽ»ã€‚

        äº¤å‰/é‡ç»„ï¼ˆçˆ¶æ¯ç”Ÿå­©å­ï¼‰ï¼š

            ä½ è®©é€‰å‡ºçš„â€œçˆ¶æ¯â€ä¸¤ä¸¤é…å¯¹ï¼Œäº¤æ¢ä»–ä»¬çš„ä¸€éƒ¨åˆ†ä½ç½®ä¿¡æ¯ï¼ˆæ¯”å¦‚ï¼Œå–çˆ¶äº²çš„ä¸€åŠåæ ‡å’Œæ¯äº²çš„ä¸€åŠåæ ‡ï¼Œç»„åˆæˆä¸€ä¸ªæ–°çš„åæ ‡ï¼‰ã€‚è¿™æ ·å°±äº§ç”Ÿäº†æ–°çš„â€œå­©å­â€æŽ¢é™©è€…ï¼Œä»–ä»¬å¯èƒ½ç«™åœ¨çˆ¶æ¯ä½ç½®ä¹‹é—´çš„æŸä¸ªæ–°åœ°æ–¹ã€‚

        å˜å¼‚ï¼ˆéšæœºçš„å°å˜åŒ–ï¼‰ï¼š

            åœ¨æ–°ç”Ÿçš„â€œå­©å­â€ä¸­ï¼Œä½ éšæœºåœ°å¯¹æžå°‘æ•°å­©å­çš„åæ ‡è¿›è¡Œä¸€ä¸ªå¾®å°çš„ã€éšæœºçš„å˜åŠ¨ã€‚æ¯”å¦‚ï¼Œè®©æŸä¸ªå­©å­å‘å·¦æˆ–å‘å³éšæœºç§»åŠ¨ä¸€å°æ­¥ã€‚å˜å¼‚éžå¸¸é‡è¦ï¼Œå®ƒèƒ½å¼•å…¥æ–°çš„å¯èƒ½æ€§ï¼Œæ¯”å¦‚è®©å­©å­å¶ç„¶å‘çŽ°ä¸€ä¸ªçˆ¶æ¯ä»ŽæœªæŽ¢ç´¢è¿‡çš„ã€å¯èƒ½æ›´é«˜çš„æ–°å±±ä¸˜ã€‚

        å½¢æˆæ–°ä¸€ä»£ï¼Œå¹¶å¾ªçŽ¯ï¼š

            çŽ°åœ¨ï¼Œä½ ç”¨è¿™äº›æ–°ç”Ÿçš„â€œå­©å­â€ä»¬ï¼ˆé€šè¿‡é€‰æ‹©å’Œäº¤å‰ã€å˜å¼‚äº§ç”Ÿçš„ï¼‰ç»„æˆæ–°ä¸€ä»£çš„æŽ¢é™©é˜Ÿï¼Œæ›¿æ¢æŽ‰å¤§éƒ¨åˆ†è€ä¸€ä»£çš„æˆå‘˜ã€‚

            ç„¶åŽå›žåˆ°ç¬¬2æ­¥ï¼Œé‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼šè¯„ä¼°æ–°é˜Ÿä¼çš„é€‚åº”åº¦ -> é€‰æ‹© -> äº¤å‰ -> å˜å¼‚ã€‚

    ç»è¿‡å¾ˆå¤šä»£è¿™æ ·çš„å¾ªçŽ¯åŽï¼Œä½ ä¼šå‘çŽ°ï¼Œä½ çš„æ•´ä¸ªæŽ¢é™©é˜Ÿä¼šé€æ¸å‘åœ°å›¾ä¸Šæœ€é«˜çš„å‡ åº§å±±å³°èšé›†ï¼Œæœ€ç»ˆæ‰¾åˆ°æœ€é«˜ç‚¹ï¼ˆæˆ–ä¸€ä¸ªéžå¸¸é«˜çš„ç‚¹ï¼‰ã€‚

    è¿›åŒ–ç®—æ³•çš„å…³é”®ç»„æˆéƒ¨åˆ†

    æ ¹æ®ä¸Šé¢çš„æ¯”å–»ï¼Œä¸€ä¸ªæ ‡å‡†çš„è¿›åŒ–ç®—æ³•åŒ…å«ä»¥ä¸‹è¦ç´ ï¼š

        ç§ç¾¤ï¼šä¸€ç»„æ½œåœ¨è§£çš„é›†åˆã€‚

        æŸ“è‰²ä½“/ä¸ªä½“ï¼šå¯¹é—®é¢˜çš„ä¸€ä¸ªæ½œåœ¨è§£çš„ç¼–ç ï¼ˆæ¯”å¦‚ä¸€ä¸²æ•°å­—ï¼‰ã€‚

        é€‚åº”åº¦å‡½æ•°ï¼šç”¨äºŽè¯„ä¼°ä¸€ä¸ªè§£å¥½ä¸å¥½çš„æ ‡å‡†ã€‚ç®—æ³•çš„ç›®æ ‡å°±æ˜¯æœ€å¤§åŒ–æˆ–æœ€å°åŒ–è¿™ä¸ªå‡½æ•°å€¼ã€‚

        é€‰æ‹©ï¼šæ ¹æ®é€‚åº”åº¦é«˜ä½Žï¼Œä»Žå½“å‰ç§ç¾¤ä¸­æŒ‘é€‰å‡ºä¼˜ç§€çš„ä¸ªä½“ä½œä¸ºçˆ¶æ¯ã€‚

        äº¤å‰ï¼šå°†ä¸¤ä¸ªçˆ¶ä»£ä¸ªä½“çš„éƒ¨åˆ†ç»“æž„åŠ ä»¥æ›¿æ¢é‡ç»„ï¼Œç”Ÿæˆæ–°ä¸ªä½“ã€‚è¿™æ˜¯äº§ç”Ÿæ–°è§£çš„ä¸»è¦æ‰‹æ®µã€‚

        å˜å¼‚ï¼šä»¥ä¸€å®šçš„æ¦‚çŽ‡éšæœºæ”¹å˜ä¸ªä½“ç¼–ç çš„æŸäº›éƒ¨åˆ†ï¼Œä»¥å¢žåŠ ç§ç¾¤çš„å¤šæ ·æ€§ï¼Œé¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚

    ä¸»è¦ç‰¹ç‚¹ä¸Žä¼˜åŠ¿

        é€‚ç”¨äºŽé»‘ç®±é—®é¢˜ï¼šä½ ä¸éœ€è¦çŸ¥é“é—®é¢˜çš„ç²¾ç¡®æ•°å­¦æ¨¡åž‹ï¼Œåªéœ€è¦èƒ½è¯„ä¼°æ¯ä¸ªè§£çš„å¥½åï¼ˆé€‚åº”åº¦ï¼‰å³å¯ã€‚

        å…¨å±€æœç´¢èƒ½åŠ›å¼ºï¼šç”±äºŽåŒæ—¶å¤„ç†ä¸€ä¸ªç§ç¾¤ï¼ˆå¤šä¸ªè§£ï¼‰ï¼Œå¹¶ä¸”æœ‰å˜å¼‚æ“ä½œï¼Œå®ƒä¸å¤ªå®¹æ˜“åƒä¼ ç»Ÿæ–¹æ³•é‚£æ ·é™·å…¥å±€éƒ¨æœ€ä¼˜è§£ã€‚

        é²æ£’æ€§å¥½ï¼šå¯¹é—®é¢˜çš„æ•°å­¦æ€§è´¨ï¼ˆå¦‚æ˜¯å¦å¯å¾®ã€æ˜¯å¦è¿žç»­ï¼‰è¦æ±‚ä¸é«˜ï¼Œèƒ½å¤„ç†å„ç§å¥‡å½¢æ€ªçŠ¶çš„æœç´¢ç©ºé—´ã€‚

        é«˜åº¦å¹¶è¡Œï¼šç§ç¾¤ä¸­æ¯ä¸ªä¸ªä½“çš„è¯„ä¼°å¯ä»¥åŒæ—¶è¿›è¡Œï¼Œéžå¸¸é€‚åˆå¹¶è¡Œè®¡ç®—ã€‚

    å¸¸è§çš„è¿›åŒ–ç®—æ³•ç±»åž‹

        é—ä¼ ç®—æ³•ï¼šæœ€ç»å…¸å’Œè‘—åçš„ä¸€ç§ï¼Œé€šå¸¸ä½¿ç”¨äºŒè¿›åˆ¶å­—ç¬¦ä¸²ç¼–ç ã€‚

        é—ä¼ è§„åˆ’ï¼šç”¨äºŽè¿›åŒ–è®¡ç®—æœºç¨‹åºï¼ˆé€šå¸¸è¡¨ç¤ºä¸ºæ ‘å½¢ç»“æž„ï¼‰ã€‚

        è¿›åŒ–ç­–ç•¥ï¼šä¸»è¦ç”¨äºŽè¿žç»­æ•°å€¼ä¼˜åŒ–ï¼Œç‰¹åˆ«å¼ºè°ƒå˜å¼‚æ“ä½œã€‚

        è¿›åŒ–è§„åˆ’ï¼šä¸Žè¿›åŒ–ç­–ç•¥ç±»ä¼¼ï¼Œä½†é€šå¸¸æ²¡æœ‰äº¤å‰æ“ä½œã€‚

    æ€»ç»“

    è¿›åŒ–ç®—æ³•æ˜¯ä¸€ç±»é€šè¿‡æ¨¡æ‹Ÿç”Ÿç‰©è¿›åŒ–è¿‡ç¨‹ä¸­çš„â€œé€‰æ‹©ã€äº¤å‰ã€å˜å¼‚â€æœºåˆ¶ï¼Œæ¥å¼•å¯¼ä¸€ä¸ªå€™é€‰è§£ç§ç¾¤æœç€æ›´ä¼˜æ–¹å‘å‘å±•çš„éšæœºä¼˜åŒ–ç®—æ³•ã€‚å®ƒç‰¹åˆ«æ“…é•¿è§£å†³é‚£äº›ä¼ ç»Ÿæ•°å­¦æ–¹æ³•éš¾ä»¥å¤„ç†çš„ã€å¤æ‚çš„ã€éžçº¿æ€§çš„ä¼˜åŒ–é—®é¢˜ã€‚

    ç®€å•æ¥è¯´ï¼Œå®ƒå°±æ˜¯è®©è®¡ç®—æœºé€šè¿‡ â€œéšæœºç”Ÿæˆ -> è¯„ä¼°å¥½å -> ä¼˜èƒœåŠ£æ±° -> æ··åˆå˜å¼‚â€ çš„å¾ªçŽ¯ï¼Œè‡ªå·±â€œæ‘¸ç´¢â€å‡ºé—®é¢˜ç­”æ¡ˆçš„ä¸€ç§å¼ºå¤§æ–¹æ³•ã€‚

* è¿›åŒ–ç®—æ³•ä¸­çš„äº¤å‰æ˜¯å¦æ˜¯å¿…è¦çš„ï¼Ÿ

    è¿™æ˜¯ä¸€ä¸ªéžå¸¸æ·±åˆ»çš„é—®é¢˜ï¼Œç­”æ¡ˆæ˜¯ï¼šäº¤å‰ä¸æ˜¯ä¸¥æ ¼å¿…è¦çš„ï¼Œä½†å®ƒæžå…¶é‡è¦ã€‚ æ²¡æœ‰äº¤å‰çš„è¿›åŒ–ç®—æ³•ï¼ˆå³åªä¾èµ–é€‰æ‹©å’Œå˜å¼‚ï¼‰ä»ç„¶æ˜¯å®Œæ•´ä¸”ç†è®ºä¸Šå¯è¡Œçš„ç®—æ³•ï¼Œä½†åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå®ƒçš„æ€§èƒ½ä¼šå¤§æ‰“æŠ˜æ‰£ã€‚

    ä¸‹é¢æˆ‘ä»¬ä»Žç†è®ºå’Œå®žè·µä¸¤ä¸ªè§’åº¦æ¥è¯¦ç»†è§£é‡Šã€‚
    1. æ²¡æœ‰äº¤å‰çš„è¿›åŒ–ç®—æ³•ï¼šè¿›åŒ–ç­–ç•¥(1+1)-ES

    ä¸€ä¸ªè‘—åçš„ä¾‹å­æ˜¯ (1+1)-è¿›åŒ–ç­–ç•¥ï¼š

        ç§ç¾¤å¤§å°ï¼š 1ä¸ªä¸ªä½“ã€‚

        æ“ä½œï¼š

            å˜å¼‚ï¼š ä»Žå½“å‰ä¸ªä½“äº§ç”Ÿä¸€ä¸ªå˜å¼‚çš„åŽä»£ã€‚

            é€‰æ‹©ï¼š æ¯”è¾ƒçˆ¶ä»£å’Œå­ä»£ï¼Œä¿ç•™ä¸¤è€…ä¸­é€‚åº”åº¦æ›´é«˜çš„ä¸€ä¸ªã€‚

        è¿™ä¸ªç®—æ³•å®Œå…¨æ²¡æœ‰äº¤å‰æ“ä½œã€‚å®ƒå°±åƒä¸€ä¸ªç‹¬è¡Œä¾ åœ¨è§£ç©ºé—´é‡Œé€šè¿‡éšæœºæ‰°åŠ¨ï¼ˆå˜å¼‚ï¼‰è¿›è¡Œæ‘¸ç´¢ã€‚

    å®ƒèƒ½å·¥ä½œå—ï¼Ÿ èƒ½ï¼å¯¹äºŽè®¸å¤šé—®é¢˜ï¼Œå®ƒéƒ½èƒ½æ‰¾åˆ°ä¸é”™çš„è§£ã€‚å®ƒè¯æ˜Žäº†å˜å¼‚æ˜¯ç»´æŒç§ç¾¤å¤šæ ·æ€§å’Œè¿›è¡ŒæŽ¢ç´¢çš„å¿…è¦æ“ä½œã€‚
    
    2. ä¸ºä»€ä¹ˆäº¤å‰å¦‚æ­¤é‡è¦ï¼Ÿç†è®ºä¾æ®

    äº¤å‰çš„æ ¸å¿ƒä½œç”¨ä¸æ˜¯æŽ¢ç´¢ï¼Œè€Œæ˜¯å¼€å‘ã€‚å®ƒå°†å·²æœ‰çš„ä¼˜è‰¯â€œåŸºå› æ¨¡å—â€è¿›è¡Œé‡ç»„ï¼Œä»Žè€Œé«˜æ•ˆåœ°æž„å»ºå‡ºæ›´ä¼˜çš„è§£ã€‚å…¶ç†è®ºä¾æ®ä¸»è¦æ¥è‡ªä»¥ä¸‹ä¸¤ä¸ªç»å…¸ç†è®ºï¼š
    a) å»ºç­‘å—å‡è¯´

    è¿™æ˜¯é—ä¼ ç®—æ³•æœ€æ ¸å¿ƒçš„ç†è®ºåŸºç¡€ã€‚

        æ ¸å¿ƒæ€æƒ³ï¼š ä¼˜ç§€çš„è§£é€šå¸¸æ˜¯ç”±ä¸€äº›çŸ­çš„ã€æ€§èƒ½è‰¯å¥½çš„â€œåŸºå› æ¨¡å—â€ç»„åˆè€Œæˆã€‚è¿™äº›æ¨¡å—æœ¬èº«å…·æœ‰è¾ƒé«˜çš„å¹³å‡é€‚åº”åº¦ã€‚

        äº¤å‰çš„ä½œç”¨ï¼š äº¤å‰æ“ä½œå…è®¸è¿™äº›åœ¨ä¸åŒä¸ªä½“ä¸­ç‹¬ç«‹è¿›åŒ–å‡ºæ¥çš„ä¼˜è‰¯æ¨¡å—ï¼ˆå»ºç­‘å—ï¼‰ç»„åˆåˆ°ä¸€èµ·ï¼Œä»Žè€Œåƒæ­ç§¯æœ¨ä¸€æ ·ï¼Œå¿«é€Ÿæž„å»ºå‡ºåŒ…å«å¤šä¸ªä¼˜è‰¯æ¨¡å—çš„ã€å…¨å±€æ›´ä¼˜çš„è§£ã€‚

        æ¯”å–»ï¼š

            åªæœ‰å˜å¼‚ï¼š å°±åƒä½ è¯•å›¾ä¸€ä¸ªå­—ä¸€ä¸ªå­—åœ°éšæœºä¿®æ”¹æ¥å†™å‡ºä¸€ç¯‡å¥½æ–‡ç« ï¼Œè¿‡ç¨‹æžå…¶ç¼“æ…¢ã€‚

            åŠ å…¥äº¤å‰ï¼š å°±åƒä¸¤ä½ä½œå®¶ï¼ˆçˆ¶ä»£ï¼‰äº¤æ¢äº†ä»–ä»¬æ–‡ç« ä¸­æœ€ç²¾å½©çš„æ®µè½ï¼ˆå»ºç­‘å—ï¼‰ï¼Œç„¶åŽç»„åˆæˆä¸€ç¯‡å¯èƒ½æ›´ç²¾å½©çš„æ–°æ–‡ç« ï¼ˆå­ä»£ï¼‰ã€‚è¿™å¤§å¤§åŠ é€Ÿäº†åˆ›é€ è¿‡ç¨‹ã€‚

    b) æ¨¡å¼å®šç†

    è¿™æ˜¯å¯¹å»ºç­‘å—å‡è¯´çš„æ•°å­¦åŒ–æè¿°ï¼Œç”±é—ä¼ ç®—æ³•ä¹‹çˆ¶John Hollandæå‡ºã€‚

        æ¨¡å¼ï¼š ä¸€ä¸ªæ¨¡å¼æ˜¯æè¿°ä¸€ç»„å…·æœ‰ç‰¹å®šåŸºå› ç›¸ä¼¼æ€§çš„å­—ç¬¦ä¸²çš„æ¨¡æ¿ã€‚ä¾‹å¦‚ï¼Œåœ¨äºŒè¿›åˆ¶ç¼–ç ä¸­ï¼Œæ¨¡å¼ 1**0*1 ä»£è¡¨äº†æ‰€æœ‰ç¬¬ä¸€ä½ä¸º1ã€ç¬¬å››ä½ä¸º0ã€ç¬¬å…­ä½ä¸º1çš„å­—ç¬¦ä¸²ï¼ˆ*æ˜¯é€šé…ç¬¦ï¼‰ã€‚

        å®šç†å†…å®¹ï¼š æ¨¡å¼å®šç†å®šé‡åœ°è¯æ˜Žäº†ï¼šçŸ­å®šä¹‰çš„ã€ä½Žé˜¶çš„ã€é«˜äºŽå¹³å‡é€‚åº”åº¦çš„æ¨¡å¼ï¼ˆå³å»ºç­‘å—ï¼‰åœ¨ç§ç¾¤ä¸­ä¼šä»¥æŒ‡æ•°çº§å¢žé•¿ã€‚

        äº¤å‰çš„è§’è‰²ï¼š äº¤å‰è™½ç„¶ä¼šç ´åæŸäº›é•¿çš„æ¨¡å¼ï¼Œä½†å®ƒå¯¹çŸ­çš„ã€ä¼˜è‰¯çš„æ¨¡å¼ç ´åæ¦‚çŽ‡å¾ˆä½Žã€‚å› æ­¤ï¼Œæ€»ä½“ä¸Šï¼Œè¿™äº›â€œå»ºç­‘å—â€èƒ½å¤Ÿé€šè¿‡é€‰æ‹©è¢«ä¿ç•™ï¼Œå¹¶é€šè¿‡äº¤å‰è¢«ä¼ æ’­å’Œé‡ç»„ï¼Œä»Žè€Œåœ¨ç§ç¾¤ä¸­è¿…é€Ÿå æ®ä¸»å¯¼åœ°ä½ã€‚

    ç®€å•æ¥è¯´ï¼Œæ¨¡å¼å®šç†ä»Žæ•°å­¦ä¸Šè§£é‡Šäº†ä¸ºä»€ä¹ˆäº¤å‰èƒ½æœ‰æ•ˆåœ°è®©â€œå¥½ç‚¹å­â€åœ¨ç§ç¾¤ä¸­ä¼ æ’­å’Œç»„åˆã€‚

    3. äº¤å‰ vs. å˜å¼‚ï¼šåˆ†å·¥æ˜Žç¡®

    ä¸ºäº†æ›´å¥½åœ°ç†è§£ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹æ¯”ä¸€ä¸‹ä¸¤è€…çš„è§’è‰²ï¼š

    ç‰¹æ€§	å˜å¼‚	äº¤å‰
    ä¸»è¦è§’è‰²	æŽ¢ç´¢	å¼€å‘
    æ“ä½œå¯¹è±¡	å•ä¸ªä¸ªä½“	ä¸¤ä¸ªæˆ–å¤šä¸ªä¸ªä½“
    åˆ›é€ æ–°åŸºå› 	èƒ½ã€‚é€šè¿‡éšæœºæ”¹å˜ï¼Œå¯ä»¥äº§ç”Ÿç§ç¾¤ä¸­ä»Žæœªæœ‰è¿‡çš„åŸºå› å€¼ã€‚	ä¸èƒ½ã€‚å®ƒåªèƒ½é‡æ–°ç»„åˆçŽ°æœ‰åŸºå› ï¼Œæ— æ³•åˆ›é€ å…¨æ–°çš„åŸºå› ä¿¡æ¯ã€‚
    ä½œç”¨æ–¹å¼	å±€éƒ¨ã€éšæœºçš„å¾®è°ƒã€‚	å…¨å±€ã€ç»“æž„åŒ–çš„é‡ç»„ã€‚
    ç±»æ¯”	ä¸€ä¸ªå‘æ˜Žå®¶åœ¨è‡ªå·±çš„å®žéªŒå®¤é‡Œå¶ç„¶å‘çŽ°äº†ä¸€ä¸ªæ–°ææ–™ã€‚	ä¸¤ä¸ªå…¬å¸åˆå¹¶ï¼Œå°†å„è‡ªçš„æ ¸å¿ƒæŠ€æœ¯ï¼ˆå»ºç­‘å—ï¼‰æ•´åˆæˆä¸€ä¸ªæ›´å¼ºå¤§çš„æ–°å…¬å¸ã€‚
    ç»“è®ºä¸Žæ€»ç»“

        éžå¿…è¦æ€§ï¼š ä»Žå­˜åœ¨æ€§ä¸Šè®²ï¼Œäº¤å‰ä¸æ˜¯å¿…éœ€çš„ã€‚ä¸€ä¸ªåªåŒ…å«å˜å¼‚å’Œé€‰æ‹©çš„è¿›åŒ–ç®—æ³•æ˜¯å®Œæ•´çš„ï¼Œå¹¶ä¸”å¯ä»¥è§£å†³é—®é¢˜ã€‚

        å…³é”®é‡è¦æ€§ï¼š ä»Žæ•ˆçŽ‡å’Œæ€§èƒ½ä¸Šè®²ï¼Œäº¤å‰é€šå¸¸æ˜¯è‡³å…³é‡è¦çš„ã€‚å®ƒæ˜¯è¿›åŒ–ç®—æ³•åŒºåˆ«äºŽå…¶ä»–éšæœºæœç´¢ç®—æ³•çš„å…³é”®ç‰¹å¾ã€‚

        ç†è®ºä¾æ®ï¼š å»ºç­‘å—å‡è¯´å’Œæ¨¡å¼å®šç†ä¸ºäº¤å‰çš„é‡è¦æ€§æä¾›äº†åšå®žçš„ç†è®ºä¾æ®ã€‚å®ƒä»¬è§£é‡Šäº†äº¤å‰å¦‚ä½•é€šè¿‡é‡ç»„ä¼˜è‰¯çš„â€œåŸºå› æ¨¡å—â€æ¥æŒ‡æ•°çº§åœ°åŠ é€Ÿæœç´¢è¿‡ç¨‹ï¼Œå®žçŽ°â€œ1+1>2â€çš„æ•ˆæžœã€‚

        ååŒå·¥ä½œï¼š å˜å¼‚å’Œäº¤å‰æ˜¯ç›¸è¾…ç›¸æˆçš„ã€‚å˜å¼‚è´Ÿè´£â€œå¼€ç–†æ‹“åœŸâ€ï¼ŒæŽ¢ç´¢æ–°çš„å¯èƒ½æ€§å’Œç»´æŒå¤šæ ·æ€§ï¼›äº¤å‰è´Ÿè´£â€œç²¾è€•ç»†ä½œâ€ï¼Œé«˜æ•ˆåœ°æ•´åˆå·²æœ‰çš„æˆæžœã€‚ æ²¡æœ‰å˜å¼‚ï¼Œç®—æ³•ä¼šè¿‡æ—©æ”¶æ•›ï¼Œå¤±åŽ»å‘çŽ°æ–°æœºä¼šçš„èƒ½åŠ›ï¼›æ²¡æœ‰äº¤å‰ï¼Œç®—æ³•çš„æ”¶æ•›é€Ÿåº¦ä¼šéžå¸¸ç¼“æ…¢ï¼Œéš¾ä»¥è§£å†³å¤æ‚é—®é¢˜ã€‚

    å› æ­¤ï¼Œåœ¨å®žé™…åº”ç”¨ä¸­ï¼Œç»å¤§å¤šæ•°è¿›åŒ–ç®—æ³•éƒ½ä¼šåŒæ—¶åŒ…å«äº¤å‰å’Œå˜å¼‚è¿™ä¸¤ä¸ªæ“ä½œå‘˜ï¼Œè®©å®ƒä»¬åœ¨æœç´¢è¿‡ç¨‹ä¸­å„å¸å…¶èŒï¼ŒååŒå·¥ä½œã€‚

* è¿›åŒ–ç®—æ³•å¼•å…¥è¡¨è§‚é—ä¼ å­¦æœºåˆ¶

    åœ¨è®¡ç®—æœºæ¨¡åž‹ä¸­ï¼Œè¡¨è§‚é—ä¼ å¯ä»¥è¢«æ¨¡æ‹Ÿä¸ºï¼š

        å¯é—ä¼ çš„æ ‡è®°ï¼šåœ¨åŸºå› åž‹ï¼ˆæŸ“è‰²ä½“ç¼–ç ï¼‰ä¹‹ä¸Šï¼Œå¢žåŠ ä¸€ä¸ªâ€œæ ‡è®°å±‚â€ã€‚è¿™ä¸ªæ ‡è®°å±‚å¯ä»¥å†³å®šæŸä¸ªåŸºå› æ˜¯â€œå¼€å¯â€è¿˜æ˜¯â€œå…³é—­â€ï¼ˆè¡¨è¾¾æˆ–ä¸è¡¨è¾¾ï¼‰ï¼Œè€Œè¿™ä¸ªæ ‡è®°æœ¬èº«ä¹Ÿå¯ä»¥ä»¥ä¸€å®šçš„æ¦‚çŽ‡é—ä¼ ç»™åŽä»£ã€‚

        å¯¹çŽ¯å¢ƒçš„å­¦ä¹ ä¸Žç»§æ‰¿ï¼š

            æ‹‰é©¬å…‹è¿›åŒ–çš„å¼•å…¥ï¼šè¡¨è§‚é—ä¼ åœ¨ä¸€å®šç¨‹åº¦ä¸Šæ”¯æŒäº†â€œèŽ·å¾—æ€§é—ä¼ â€çš„å¯èƒ½æ€§ã€‚çˆ¶æ¯ä¸€ç”Ÿä¸­å› çŽ¯å¢ƒå› ç´ ï¼ˆå¦‚é¥®é£Ÿã€åŽ‹åŠ›ï¼‰å¯¼è‡´çš„è¡¨è§‚æ ‡è®°å˜åŒ–ï¼Œæœ‰å¯èƒ½ä¼ é€’ç»™åŽä»£ã€‚

            åœ¨ç®—æ³•ä¸­çš„ä½“çŽ°ï¼šä¸ªä½“åœ¨ç”Ÿå‘½å‘¨æœŸå†…å¯ä»¥é€šè¿‡å±€éƒ¨æœç´¢ã€å­¦ä¹ ç­‰ç­–ç•¥æ¥â€œä¼˜åŒ–â€è‡ªå·±çš„è¡¨çŽ°åž‹ï¼Œç„¶åŽå°†è¿™ç§ä¼˜åŒ–æˆæžœé€šè¿‡æŸç§æœºåˆ¶ï¼ˆä¾‹å¦‚ï¼Œæ”¹å˜åŸºå› çš„æ˜¾æ€§/éšæ€§ï¼Œæˆ–ç›´æŽ¥ä¿®æ”¹ç¼–ç ï¼‰éƒ¨åˆ†åœ°é—ä¼ ç»™åŽä»£ã€‚è¿™è¢«è¯æ˜Žå¯ä»¥æ˜¾è‘—åŠ é€Ÿæ”¶æ•›ã€‚

* å…¶ä»–è¢«å¼•å…¥è¿›åŒ–ç®—æ³•çš„å¤æ‚ç”Ÿç‰©å­¦æœºåˆ¶

    å‘è‚²ç”Ÿç‰©å­¦ï¼š

        é—®é¢˜ï¼šä¼ ç»Ÿè¿›åŒ–ç®—æ³•ä¸­ï¼ŒåŸºå› åž‹åˆ°è¡¨çŽ°åž‹æ˜¯ç›´æŽ¥è½¬æ¢ï¼ˆå¦‚ï¼ŒäºŒè¿›åˆ¶å­—ç¬¦ä¸²ç›´æŽ¥è§£ç ä¸ºä¸€ä¸ªæ•°å­—ï¼‰ã€‚

        æ›´ç”Ÿç‰©å­¦çš„æ¨¡åž‹ï¼šå¼•å…¥ä¸€ä¸ªå‘è‚²è¿‡ç¨‹ã€‚åŸºå› åž‹ä½œä¸ºâ€œé…æ–¹â€ï¼Œé€šè¿‡ä¸€ä¸ªæ¨¡æ‹ŸèƒšèƒŽå‘è‚²çš„è¿‡ç¨‹ï¼ˆå¦‚åŸºå› è°ƒæŽ§ç½‘ç»œï¼‰é€æ­¥â€œç”Ÿé•¿â€æˆå¤æ‚çš„è¡¨çŽ°åž‹ã€‚è¿™ä½¿å¾—å°çš„åŸºå› å˜åŒ–èƒ½é€šè¿‡å‘è‚²è¿‡ç¨‹äº§ç”Ÿå·¨å¤§è€Œç»“æž„åŒ–çš„è¡¨çŽ°åž‹å˜åŒ–ï¼Œä»Žè€Œåˆ›é€ å‡ºæ›´å¤æ‚ã€æ›´é²æ£’çš„è§£å†³æ–¹æ¡ˆã€‚

    ç”Ÿæ€ä½ä¸Žå…±ç”Ÿï¼š

        é—®é¢˜ï¼šä¼ ç»Ÿç®—æ³•ä¸­ä¸ªä½“é—´ä¸»è¦æ˜¯ç«žäº‰å…³ç³»ã€‚

        æ›´ç”Ÿç‰©å­¦çš„æ¨¡åž‹ï¼šæ¨¡æ‹Ÿç”Ÿæ€ç³»ç»Ÿï¼Œä¸ªä½“å¯ä»¥å æ®ä¸åŒçš„â€œç”Ÿæ€ä½â€ï¼Œé¿å…ç›´æŽ¥ç«žäº‰ã€‚è¿˜å¯ä»¥å¼•å…¥å…±ç”Ÿå…³ç³»ï¼Œå³ä¸åŒä¸ªä½“é€šè¿‡åˆä½œäº§ç”Ÿå•ç‹¬æ— æ³•å®žçŽ°çš„é€‚åº”åº¦ä¼˜åŠ¿ã€‚

    æ€§é€‰æ‹©ä¸Žå®¿ä¸»-å¯„ç”Ÿè™«ååŒè¿›åŒ–ï¼š

        ä¸ä»…ä»…åŸºäºŽç”Ÿå­˜èƒ½åŠ›è¿›è¡Œé€‰æ‹©ï¼Œè¿˜å¼•å…¥åŸºäºŽâ€œå¸å¼•åŠ›â€çš„é€‰æ‹©ã€‚

        é€šè¿‡æ¨¡æ‹Ÿå®¿ä¸»ä¸Žå¯„ç”Ÿè™«ä¹‹é—´çš„â€œå†›å¤‡ç«žèµ›â€ï¼Œæ¥ç»´æŒç§ç¾¤çš„å¤šæ ·æ€§å’Œé¿å…è¿‡æ—©æ”¶æ•›ã€‚

* `torch.randint()`è¦æ±‚ size å‚æ•°å¿…é¡»ä¸º tuple ç±»åž‹

    æ¯”å¦‚ï¼š

    * å¯¹äºŽä¸€ç»´å¼ é‡: `(batch_size,)`

    * å¯¹äºŽäºŒç»´å¼ é‡: `(batch_size, seq_len)`

* å¤šæ¨¡æ€æŽ¨ç†ï¼ˆMultimodal Reasoningï¼‰

    1. å¤šæ¨¡æ€è¡¨å¾å­¦ä¹ 

        * æ¨¡æ€å¯¹é½ï¼ˆAlignmentï¼‰ï¼šå°†ä¸åŒæ¨¡æ€çš„æ•°æ®æ˜ å°„åˆ°ç»Ÿä¸€çš„è¯­ä¹‰ç©ºé—´ï¼Œä½¿ç›¸ä¼¼è¯­ä¹‰çš„å†…å®¹ï¼ˆå¦‚å›¾åƒä¸­çš„ç‹—å’Œæ–‡æœ¬ä¸­çš„â€œç‹—â€ï¼‰åœ¨è¡¨å¾ç©ºé—´ä¸­æŽ¥è¿‘ã€‚ä¾‹å¦‚ï¼š

            å¯¹æ¯”å­¦ä¹ ï¼ˆå¦‚CLIPï¼‰ï¼šé€šè¿‡å¯¹æ¯”æŸå¤±å‡½æ•°æ‹‰è¿‘åŒ¹é…çš„å›¾æ–‡å¯¹ï¼ŒæŽ¨å¼€ä¸åŒ¹é…çš„å¯¹ã€‚

            è·¨æ¨¡æ€ç¼–ç å™¨ï¼ˆå¦‚ViLBERTã€UniTï¼‰ï¼šç”¨Transformeræž¶æž„è”åˆç¼–ç å¤šæ¨¡æ€è¾“å…¥ã€‚

        * æ¨¡æ€èžåˆï¼ˆFusionï¼‰ï¼šå°†ä¸åŒæ¨¡æ€çš„ç‰¹å¾åˆå¹¶ä¸ºç»Ÿä¸€çš„è¡¨å¾ã€‚å¸¸è§æ–¹æ³•åŒ…æ‹¬ï¼š

            æ—©æœŸèžåˆï¼šåœ¨è¾“å…¥å±‚ç›´æŽ¥æ‹¼æŽ¥ä¸åŒæ¨¡æ€çš„åŽŸå§‹ç‰¹å¾ã€‚

            æ™šæœŸèžåˆï¼šåˆ†åˆ«å¤„ç†å„æ¨¡æ€åŽåˆå¹¶é«˜å±‚ç‰¹å¾ï¼ˆå¦‚æ³¨æ„åŠ›æœºåˆ¶åŠ æƒèžåˆï¼‰ã€‚

    2. è·¨æ¨¡æ€å…³è”ä¸ŽæŽ¨ç†

        * äº’è¡¥æ€§åˆ©ç”¨ï¼šä¸åŒæ¨¡æ€æä¾›çš„ä¿¡æ¯å¯èƒ½äº’è¡¥ï¼ˆå¦‚è§†é¢‘ä¸­çš„åŠ¨ä½œ+éŸ³é¢‘ä¸­çš„å£°éŸ³å¯æ›´å‡†ç¡®è¯†åˆ«åœºæ™¯ï¼‰ã€‚

        * å†—ä½™æ€§æ¶ˆé™¤ï¼šé€šè¿‡è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ï¼ˆå¦‚Cross-Modal Attentionï¼‰åŠ¨æ€é€‰æ‹©é‡è¦ä¿¡æ¯ï¼Œå¿½ç•¥é‡å¤æˆ–å™ªå£°ã€‚

        * ç¬¦å·-æ„ŸçŸ¥ç»“åˆï¼šå°†ç¥žç»ç½‘ç»œçš„æ„ŸçŸ¥èƒ½åŠ›ï¼ˆå¦‚å›¾åƒåˆ†ç±»ï¼‰ä¸Žç¬¦å·æŽ¨ç†ï¼ˆå¦‚é€»è¾‘è§„åˆ™ï¼‰ç»“åˆï¼Œå®žçŽ°é«˜å±‚æŽ¨ç†ï¼ˆå¦‚Visual Question Answeringä¸­å›žç­”â€œå›¾ç‰‡ä¸­æ˜¯å¦æœ‰æ¯”çŒ«æ›´å¤§çš„ç‰©ä½“ï¼Ÿâ€ï¼‰ã€‚

    3. å¤šæ¨¡æ€é¢„è®­ç»ƒæ¨¡åž‹

        çŽ°ä»£å¤šæ¨¡æ€æŽ¨ç†å¸¸åŸºäºŽå¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡åž‹ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

        * è‡ªç›‘ç£å­¦ä¹ ï¼šåˆ©ç”¨æµ·é‡æ— æ ‡æ³¨å¤šæ¨¡æ€æ•°æ®ï¼ˆå¦‚äº’è”ç½‘å›¾æ–‡å¯¹ï¼‰è¿›è¡Œé¢„è®­ç»ƒï¼Œå­¦ä¹ é€šç”¨è¡¨å¾ã€‚

            * ä»»åŠ¡ç¤ºä¾‹ï¼šå›¾æ–‡åŒ¹é…ã€æŽ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰ã€æŽ©ç åŒºåŸŸå»ºæ¨¡ï¼ˆMRMï¼‰ç­‰ã€‚

        * å¾®è°ƒï¼ˆFine-tuningï¼‰ï¼šåœ¨ç‰¹å®šä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚è§†è§‰æŽ¨ç†ã€å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æžï¼‰ä¸Šå¾®è°ƒæ¨¡åž‹ã€‚

        å…¸åž‹æ¨¡åž‹ï¼š

        * CLIPï¼ˆOpenAIï¼‰ï¼šé€šè¿‡å¯¹æ¯”å­¦ä¹ å¯¹é½å›¾æ–‡è¡¨å¾ã€‚

        * Flamingoï¼ˆDeepMindï¼‰ï¼šå¤„ç†äº¤é”™å›¾æ–‡åºåˆ—ï¼Œæ”¯æŒå°‘æ ·æœ¬å­¦ä¹ ã€‚

        * GPT-4Vï¼ˆOpenAIï¼‰ï¼šæ‰©å±•å¤§è¯­è¨€æ¨¡åž‹è‡³å¤šæ¨¡æ€è¾“å…¥ï¼Œå®žçŽ°å¤æ‚æŽ¨ç†ã€‚

    4. æŽ¨ç†æœºåˆ¶çš„å…·ä½“å®žçŽ°

        * æ³¨æ„åŠ›æœºåˆ¶ï¼šé€šè¿‡è·¨æ¨¡æ€æ³¨æ„åŠ›æƒé‡åŠ¨æ€èšç„¦å…³é”®ä¿¡æ¯ï¼ˆå¦‚æ–‡æœ¬æè¿°ä¸­çš„å…³é”®è¯ä¸Žå›¾åƒåŒºåŸŸçš„å…³è”ï¼‰ã€‚

        * å›¾ç¥žç»ç½‘ç»œï¼ˆGNNï¼‰ï¼šå°†å¤šæ¨¡æ€æ•°æ®è¡¨ç¤ºä¸ºå›¾ç»“æž„ï¼ˆå¦‚å¯¹è±¡å…³ç³»å›¾ï¼‰ï¼Œé€šè¿‡æ¶ˆæ¯ä¼ é€’è¿›è¡ŒæŽ¨ç†ã€‚

        * ç¥žç»ç¬¦å·ç³»ç»Ÿï¼šç»“åˆç¥žç»ç½‘ç»œï¼ˆå¤„ç†æ„ŸçŸ¥ï¼‰å’Œç¬¦å·æŽ¨ç†ï¼ˆå¤„ç†é€»è¾‘ï¼‰ï¼Œä¾‹å¦‚ï¼š

            * Neuro-Symbolic Concept Learnerï¼ˆNS-CLï¼‰ï¼šä»Žå›¾åƒä¸­æå–ç¬¦å·åŒ–æ¦‚å¿µåŽè¿›è¡Œé€»è¾‘æŽ¨ç†ã€‚

    5. åº”ç”¨ä¸ŽæŒ‘æˆ˜

        * åº”ç”¨åœºæ™¯ï¼š

            * è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ã€è§†é¢‘ç†è§£ã€åŒ»ç–—è¯Šæ–­ï¼ˆç»“åˆå½±åƒå’ŒæŠ¥å‘Šï¼‰ã€è‡ªåŠ¨é©¾é©¶ï¼ˆèžåˆæ¿€å…‰é›·è¾¾ã€æ‘„åƒå¤´ã€åœ°å›¾ï¼‰ã€‚

        * å…³é”®æŒ‘æˆ˜ï¼š

            * æ¨¡æ€å¼‚æž„æ€§ï¼šä¸åŒæ¨¡æ€çš„æ•°æ®åˆ†å¸ƒå·®å¼‚å¤§ï¼ˆå¦‚æ–‡æœ¬ç¦»æ•£ã€å›¾åƒè¿žç»­ï¼‰ã€‚

            * æ•°æ®ç¨€ç¼ºæ€§ï¼šé«˜è´¨é‡å¯¹é½çš„å¤šæ¨¡æ€æ•°æ®è¾ƒå°‘ã€‚

            * å¯è§£é‡Šæ€§ï¼šå¤æ‚æ¨¡åž‹çš„å†³ç­–è¿‡ç¨‹éš¾ä»¥é€æ˜ŽåŒ–ã€‚

    ç¤ºä¾‹ï¼šå¤šæ¨¡æ€é—®ç­”çš„æŽ¨ç†æµç¨‹

    1. è¾“å…¥ï¼šé—®é¢˜ï¼ˆæ–‡æœ¬ï¼‰â€œå›¾ä¸­æˆ´å¸½å­çš„äººæ‰‹é‡Œæ‹¿ç€ä»€ä¹ˆï¼Ÿâ€ + å›¾åƒã€‚

    2. è¡¨å¾ï¼šæ–‡æœ¬ç”¨BERTç¼–ç ï¼Œå›¾åƒç”¨CNNæå–åŒºåŸŸç‰¹å¾ã€‚

    3. å¯¹é½ï¼šé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶æ‰¾åˆ°â€œæˆ´å¸½å­çš„äººâ€å¯¹åº”çš„å›¾åƒåŒºåŸŸã€‚

    4. æŽ¨ç†ï¼šç»“åˆåŒºåŸŸç‰¹å¾ï¼ˆæ£€æµ‹â€œæ‰‹â€å’Œâ€œç‰©ä½“â€ï¼‰å’Œé—®é¢˜è¯­ä¹‰é¢„æµ‹ç­”æ¡ˆï¼ˆå¦‚â€œæ¯å­â€ï¼‰ã€‚

* `torch.relu()`

    å®šä¹‰ï¼šrelu(x) = max(0, x)

    é€šä¿—è§£é‡Šï¼šå®ƒåƒä¸€ä¸ªâ€œè¿‡æ»¤å™¨â€ï¼ŒæŠŠæ‰€æœ‰è¾“å…¥è¿›æ¥çš„è´Ÿæ•°éƒ½å˜æˆ 0ï¼Œè€Œæ­£æ•°åˆ™ä¿æŒä¸å˜ã€‚

    åœ¨ç¥žç»ç½‘ç»œä¸­çš„æ„ä¹‰ï¼š

    * å¼•å…¥éžçº¿æ€§ã€‚å¦‚æžœæ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼Œæ— è®ºç¥žç»ç½‘ç»œæœ‰å¤šå°‘å±‚ï¼Œå®ƒéƒ½ç­‰ä»·äºŽä¸€ä¸ªçº¿æ€§æ¨¡åž‹ï¼Œè¡¨è¾¾èƒ½åŠ›éžå¸¸æœ‰é™ã€‚ReLU çš„åŠ å…¥ä½¿å¾—ç½‘ç»œå¯ä»¥å­¦ä¹ å¹¶æ‹Ÿåˆå¤æ‚çš„æ•°æ®æ¨¡å¼ã€‚

    * è®¡ç®—ç®€å•ï¼Œåªæœ‰æ¯”è¾ƒå’Œå–0çš„æ“ä½œï¼Œå› æ­¤è®­ç»ƒé€Ÿåº¦æ¯” Sigmoidã€Tanh ç­‰å‡½æ•°æ›´å¿«ã€‚

    * æœ‰åŠ©äºŽç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼ˆåœ¨æ­£æ•°åŒºåŸŸï¼Œæ¢¯åº¦æ’ä¸º1

    example:

    ```py
    import torch

    x = torch.tensor([-2.0, -0.5, 0.0, 1.0, 5.0])
    y = torch.relu(x)
    print(y)
    # è¾“å‡ºï¼štensor([0., 0., 0., 1., 5.])
    ```

    ReLU çš„å¯¼æ•°:

    å½“ x < 0 æ—¶ï¼šå‡½æ•°å€¼æ˜¯å¸¸æ•° 0ï¼Œæ‰€ä»¥å¯¼æ•°ä¸º 0

    å½“ x > 0 æ—¶ï¼šå‡½æ•°æ˜¯ f(x) = xï¼Œæ‰€ä»¥å¯¼æ•°ä¸º 1

    å…³é”®é—®é¢˜ï¼šåœ¨ x = 0 å¤„çš„å¯¼æ•°

    åœ¨ x = 0 è¿™ä¸ªç‚¹ï¼ŒReLU å‡½æ•°æ˜¯ä¸å¯å¾®çš„ï¼Œæˆ–è€…è¯´æ˜¯ä¸€ä¸ªæ¬¡æ¢¯åº¦ç‚¹ã€‚

    * å·¦å¯¼æ•° = 0

    * å³å¯¼æ•° = 1

    * å·¦å³å¯¼æ•°ä¸ç›¸ç­‰ï¼Œå› æ­¤åœ¨ x=0 å¤„å¯¼æ•°ä¸å­˜åœ¨

    åœ¨å®žé™…åº”ç”¨ä¸­ï¼ˆå¦‚æ·±åº¦å­¦ä¹ æ¡†æž¶ PyTorch, TensorFlowï¼‰ï¼Œé€šå¸¸é‡‡ç”¨ä»¥ä¸‹çº¦å®šä¹‹ä¸€ï¼š

    1. å°† x=0 å¤„çš„å¯¼æ•°å®šä¹‰ä¸º 0ï¼ˆè¿™æ˜¯æœ€å¸¸è§çš„é€‰æ‹©ï¼‰

    2. æˆ–è€…å®šä¹‰ä¸º 1

    3. æˆ–è€…éšæœºé€‰æ‹© 0 æˆ– 1

    åœ¨ PyTorch ä¸­ï¼Œtorch.relu åœ¨ x=0 å¤„çš„å¯¼æ•°è¢«å®šä¹‰ä¸º 0ã€‚

    example:

    ```py
    import torch

    x = torch.tensor([-2.0, 0.0, 3.0], requires_grad=True)
    y = torch.relu(x)

    # å‡è®¾ä¸Šæ¸¸æ¢¯åº¦ä¸º 1
    y.backward(torch.tensor([1.0, 1.0, 1.0]))
    print(x.grad)  # è¾“å‡ºï¼štensor([0., 0., 1.])
    ```

    å…³äºŽ nn.ReLUï¼š

    ```py
    # å¯¹äºŽ Sequential æ¨¡åž‹ï¼Œä½¿ç”¨ nn.ReLU æ¨¡å—
    model = nn.Sequential(
        nn.Linear(10, 20),
        nn.ReLU(),      # è¿™æ˜¯ä¸€ä¸ªæ¨¡å—ï¼Œæœ‰çŠ¶æ€ï¼Œå¯ä»¥è®­ç»ƒå‚æ•°ï¼ˆè™½ç„¶ReLUæ²¡æœ‰å‚æ•°ï¼‰
        nn.Linear(20, 1)
    )
    ```

    åœ¨ nn.Sequential ä¸­å¿…é¡»ä½¿ç”¨ nn.ReLU() æ¨¡å—

* å¦‚æžœåœ¨`batch_loss.backward()`ä¹‹å‰å°±å°è¯•æ‹¿ grad `net.fc1.weight.grad`ï¼Œé‚£ä¹ˆ grad ä¸º None

* ä¸èƒ½ä½¿ç”¨`sgd = torch.optim.sgd.SGD()`, ä½†æ˜¯å¯ä»¥ä½¿ç”¨`from torch.optim.sgd import SGD`ã€‚ä¸æ¸…æ¥šä¸ºä»€ä¹ˆã€‚

* pytorch model save(), load()

    ```py
    # ä¿å­˜æ¨¡åž‹
    t.save(net.state_dict(), 'model_weights.pth')

    # åŠ è½½æ¨¡åž‹
    net.load_state_dict(t.load('model_weights.pth'))
    ```

    æ³¨æ„è¿™ç§æ–¹æ³•æ²¡æœ‰ä¿å­˜ model çš„ç»“æž„ï¼Œåªä¿å­˜äº†å‚æ•°ã€‚

* dataloader

    syntax:

    ```py
    DataLoader(dataset, shuffle=True, sampler=None, batch_size=32)
    ```

    ä¸€ä¸ªç®€å•çš„ example:

    ```py
    import torch as t
    from torch.utils.data import Dataset, DataLoader

    class MyDataset(Dataset):
        def __init__(self):
            self.m_arr = list(range(8))
            self.m_len = len(self.m_arr)
        
        def __len__(self):
            return self.m_len
        
        def __getitem__(self, index):
            return self.m_arr[index]
        
    my_dataset = MyDataset()
    print("first elm: {}".format(my_dataset[0]))
    print("dataset len: {}".format(len(my_dataset)))

    my_dataloader = DataLoader(my_dataset, batch_size=2, shuffle=True)
    for batch_data in my_dataloader:
        print("batch_data: {}, type: {}, shape: {}".format(batch_data, type(batch_data), batch_data.shape))
    ```

    output:

    ```
    first elm: 0
    dataset len: 8
    batch_data: tensor([4, 1]), type: <class 'torch.Tensor'>, shape: torch.Size([2])
    batch_data: tensor([6, 7]), type: <class 'torch.Tensor'>, shape: torch.Size([2])
    batch_data: tensor([3, 0]), type: <class 'torch.Tensor'>, shape: torch.Size([2])
    batch_data: tensor([5, 2]), type: <class 'torch.Tensor'>, shape: torch.Size([2])
    ```

    DataLoaders on Built-in Datasets:

    ```py
    # importing the required libraries
    import torch
    from torch.utils.data import Dataset
    from torch.utils.data import DataLoader
    import seaborn as sns
    from torch.utils.data import TensorDataset

    # defining the dataset consisting of 
    # two columns from iris dataset
    iris = sns.load_dataset('iris')
    petal_length = torch.tensor(iris['petal_length'])
    petal_width = torch.tensor(iris['petal_width'])
    dataset = TensorDataset(petal_length, petal_width)

    # implementing dataloader on the dataset 
    # and printing per batch
    dataloader = DataLoader(dataset, 
                            batch_size=5, 
                            shuffle=True)

    for i in dataloader:
        print(i)
    ```

* `torch.nn.Module`

    * `__init__()`: The __init__ method is used to initialize the module's parameters. This method is called when the module is created, and it allows we to set up any internal state that the module needs. For example, we might use this method to initialize the weights of a neural network or to create other modules that the module needs in order to function.

    * `forward()`: The forward method is used to perform the computation that the module represents. This method takes in one or more input tensors, performs computations on them, and returns the output tensors. It is a forward pass of the module.

    example:

    ```py
    class MyModule(nn.Module):
        
        # Initialize the parameter
        def __init__(self, num_inputs, num_outputs, hidden_size):
            super(MyModule, self).__init__()
            self.linear1 = nn.Linear(num_inputs, hidden_size)
            self.linear2 = nn.Linear(hidden_size, num_outputs)
        
        # Forward pass
        def forward(self, input):
            lin    = self.linear1(input)
            output = nn.functional.relu(lin)
            pred   = self.linear2(output)
            return pred

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(my_module.parameters(), lr=0.005)

    transform = transforms.Compose([transforms.ToTensor(), 
                                    transforms.Normalize((0.5,), (0.5,))])
    ```

    complete code:

    ```py
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torchvision import datasets, transforms
    from sklearn.metrics import classification_report

    class MyModule(nn.Module):
        def __init__(self, num_inputs, num_outputs, hidden_size):
            super(MyModule, self).__init__()
            self.linear1 = nn.Linear(num_inputs, hidden_size)
            self.linear2 = nn.Linear(hidden_size, num_outputs)

        def forward(self, input):
            lin    = self.linear1(input)
            output = nn.functional.relu(lin)
            pred   = self.linear2(output)
            return pred

    # Instantiate the custom module
    my_module = MyModule(num_inputs=28*28, num_outputs=10, hidden_size=20)

    # Define the loss function and optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(my_module.parameters(), lr=0.01)

    # Define the transformations for the dataset
    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])

    # Load the MNIST dataset
    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)

    # Define the data loader
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)

    # Train the model
    for epoch in range(10):
        for i, (images, labels) in enumerate(train_loader):
            images = images.view(-1, 28*28)
            optimizer.zero_grad()
            output = my_module(images)
            loss = criterion(output, labels)
            loss.backward()
            optimizer.step()
        print('Epoch -->',epoch,'-->',loss)

        

    #Test the model
    with torch.no_grad():
        y_true = []
        y_pred = []
        correct = 0
        total = 0
        for images, labels in test_loader:
            images = images.view(-1, 28*28)
            output = my_module(images)
            _, predicted = torch.max(output.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum()
            y_true += labels.tolist()
            y_pred += predicted.tolist()

        # Accuracy
        print('Accuracy: {} %'.format(100 * correct / total))
        
        # Classification Report
        report = classification_report(y_true, y_pred)
        print(report)
    ```

* ä¸€ä¸ªå¯ä»¥è·‘é€šçš„ lstm example

    ```py
    import torch
    import torch.nn as nn
    import numpy as np
    import matplotlib.pyplot as plt

    # ==== è¶…å‚æ•° ====
    seq_len = 20       # æ¯ä¸ªè¾“å…¥åºåˆ—é•¿åº¦
    hidden_size = 64   # LSTM éšå±‚ç»´åº¦
    num_layers = 1
    num_epochs = 200
    lr = 0.01
    torch.manual_seed(0)
    np.random.seed(0)

    # ==== ç”Ÿæˆæ•°æ® ====
    x = np.linspace(0, 100, 1000)
    y = np.sin(x)
    y = (y - y.min()) / (y.max() - y.min())

    # æž„é€ åºåˆ—
    def create_dataset(data, seq_len):
        xs, ys = [], []
        for i in range(len(data) - seq_len):
            xs.append(data[i:i+seq_len])
            ys.append(data[i+seq_len])
        return np.array(xs), np.array(ys)

    X, Y = create_dataset(y, seq_len)
    X = torch.tensor(X, dtype=torch.float32).unsqueeze(-1)  # [batch, seq_len, 1]
    Y = torch.tensor(Y, dtype=torch.float32).unsqueeze(-1)  # [batch, 1]

    train_size = int(0.8 * len(X))
    X_train, X_test = X[:train_size], X[train_size:]
    Y_train, Y_test = Y[:train_size], Y[train_size:]

    # ==== æ¨¡åž‹å®šä¹‰ ====
    class LSTMModel(nn.Module):
        def __init__(self, input_size=1, hidden_size=64, num_layers=1, output_size=1):
            super(LSTMModel, self).__init__()
            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
            self.fc = nn.Linear(hidden_size, output_size)

        def forward(self, x):
            out, _ = self.lstm(x)
            out = out[:, -1, :]  # å–æœ€åŽæ—¶åˆ»è¾“å‡º
            out = self.fc(out)
            return out

    model = LSTMModel(hidden_size=hidden_size, num_layers=num_layers)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    # ==== è®­ç»ƒ ====
    for epoch in range(num_epochs):
        model.train()
        optimizer.zero_grad()
        output = model(X_train)
        loss = criterion(output, Y_train)
        loss.backward()
        optimizer.step()
        if (epoch + 1) % 20 == 0:
            print(f"Epoch [{epoch+1}/{num_epochs}]  Loss: {loss.item():.6f}")

    # ==== æµ‹è¯• ====
    model.eval()
    with torch.no_grad():
        pred = model(X_test)   # shape [test_len, 1]
        loss = criterion(pred, Y_test)
        print(f"Test MSE: {loss.item():.6f}")

    # ==== æœªæ¥è¶‹åŠ¿é¢„æµ‹ï¼ˆä¿®æ­£åŽçš„å¾ªçŽ¯ï¼‰ ====
    future_steps = 50  # é¢„æµ‹æœªæ¥ 50 ä¸ªç‚¹
    future_preds = []

    # å–æµ‹è¯•é›†æœ€åŽä¸€ä¸ªåºåˆ—ä½œä¸ºèµ·ç‚¹ï¼ˆæ³¨æ„æ˜¯æœ€åŽä¸€ä¸ª X_testï¼‰
    last_seq = X_test[-1].clone().detach().unsqueeze(0)  # shape [1, seq_len, 1]

    model.eval()
    with torch.no_grad():
        for _ in range(future_steps):
            # next_pred: shape [1, 1]
            next_pred = model(last_seq)
            future_preds.append(next_pred.item())
            # æŠŠ next_pred æ‰©æˆ [1, 1, 1]ï¼Œç„¶åŽæ»‘åŠ¨çª—å£æ‹¼æŽ¥æˆæ–°çš„ last_seq
            next_pred_expanded = next_pred.unsqueeze(-1)     # [1, 1, 1]
            last_seq = torch.cat((last_seq[:, 1:, :], next_pred_expanded), dim=1)  # [1, seq_len, 1]

    # ==== å¯è§†åŒ–ï¼ˆç”¨è§£æž sin ç”Ÿæˆ future GT å¹¶åªæ ‡ 10 ä¸ª xï¼‰ ====
    plt.figure(figsize=(12,4))

    # èƒŒæ™¯å®Œæ•´ sin æ›²çº¿ï¼ˆæ·¡åŒ–ï¼‰
    plt.plot(y, label='True sin (background)', alpha=0.15, zorder=1)

    # æµ‹è¯•åŒºé—´é¢„æµ‹ï¼ˆå®žçº¿ï¼‰
    test_pred_idx_start = train_size + seq_len
    test_pred_idx_end = test_pred_idx_start + len(pred)
    plt.plot(range(test_pred_idx_start, test_pred_idx_end),
             pred.squeeze().cpu().numpy(), label='Predicted (test)', zorder=2)

    # æœªæ¥é¢„æµ‹ï¼ˆçº¢çº¿ï¼‰
    future_idx_start = test_pred_idx_end
    future_idx_end = future_idx_start + future_steps
    plt.plot(range(future_idx_start, future_idx_end),
             np.array(future_preds), label='Future Prediction', color='red', linewidth=2, zorder=3)

    # --- ç”¨è§£æžå¼ç»§ç»­ç”Ÿæˆ future x ä¸Ž GTï¼ˆå¹¶ç”¨å’Œè®­ç»ƒæ—¶ç›¸åŒçš„å½’ä¸€åŒ–ï¼‰ ---
    # åŽŸå§‹ x æ•°ç»„åä¸º xï¼›æˆ‘ä»¬å‡è®¾å®ƒè¿˜åœ¨ä½œç”¨åŸŸå†…
    step = x[1] - x[0]
    future_x = x[-1] + step * np.arange(1, future_steps + 1)  # length future_steps
    future_y_raw = np.sin(future_x)

    # ä½¿ç”¨è®­ç»ƒæ—¶çš„å½’ä¸€åŒ–å‚æ•°ï¼ˆä¸Žä¹‹å‰ y çš„å½’ä¸€åŒ–ä¿æŒä¸€è‡´ï¼‰
    # æ³¨æ„ï¼šåœ¨ä½ çš„è„šæœ¬é‡Œ y = np.sin(x); ç„¶åŽ y = (y - y.min()) / (y.max() - y.min())
    # æ‰€ä»¥æˆ‘ä»¬ç”¨åŒæ ·çš„ min/max æ¥å½’ä¸€åŒ– future_y_raw
    orig_y_raw = np.sin(x)  # åŽŸå§‹æœªå½’ä¸€åŒ–çš„ yï¼ˆåŸºäºŽåŽŸ xï¼‰
    y_min, y_max = orig_y_raw.min(), orig_y_raw.max()
    future_y = (future_y_raw - y_min) / (y_max - y_min)

    # åœ¨æœªæ¥æ®µå‡åŒ€é€‰å– 10 ä¸ªç‚¹æ ‡å‡º 'x'
    n_marks = 10
    if future_steps >= n_marks:
        mark_indices = np.linspace(0, future_steps - 1, n_marks, dtype=int)
    else:
        # å¦‚æžœ future_steps å°‘äºŽ 10ï¼Œæ ‡å…¨éƒ¨ç‚¹
        mark_indices = np.arange(future_steps, dtype=int)

    x_gt_marks = np.array(range(future_idx_start, future_idx_end))[mark_indices]
    y_gt_marks = future_y[mark_indices]

    plt.scatter(x_gt_marks, y_gt_marks, marker='x', color='darkred',
                s=80, linewidths=2.5, label='Ground Truth (sampled x)', zorder=4)

    # è®¡ç®—å¹¶æ‰“å°æœªæ¥é¢„æµ‹ä¸Žè§£æž GT çš„è¯¯å·®ï¼ˆå…¨é‡æ¯”è¾ƒï¼‰
    pred_array = np.array(future_preds)
    mse_future = np.mean((pred_array - future_y) ** 2)
    print(f"Future MSE against analytic sin (future {future_idx_start}..{future_idx_end-1}): {mse_future:.6f}")

    plt.legend()
    plt.title("LSTM: sin(x) prediction + future trend (10 sampled GT 'x' marks)")
    plt.xlabel("Time step")
    plt.ylabel("Normalized sin(x)")
    plt.tight_layout()
    plt.show()
    ```

    output:

    ```
    Epoch [20/200]  Loss: 0.030417
    Epoch [40/200]  Loss: 0.002496
    Epoch [60/200]  Loss: 0.000462
    Epoch [80/200]  Loss: 0.000082
    Epoch [100/200]  Loss: 0.000028
    Epoch [120/200]  Loss: 0.000012
    Epoch [140/200]  Loss: 0.000006
    Epoch [160/200]  Loss: 0.000003
    Epoch [180/200]  Loss: 0.000002
    Epoch [200/200]  Loss: 0.000002
    Test MSE: 0.000002
    ```

    è¿˜ä¼šè¾“å‡ºä¸€ä¸ª sin æ›²çº¿çš„å›¾åƒã€‚

* Long Short-Term Memory (LSTM) 

    * Hidden State (h_n)

        The hidden state in an LSTM represents the short-term memory of the network.

        Shape: The hidden state h_n has the shape (num_layers * num_directions, batch, hidden_size). This shape indicates that the hidden state is maintained for each layer and direction in the LSTM.

    * Output (output)

        The output of an LSTM is the sequence of hidden states from the last layer for each time step. 

* Natural language processing (NLP) å¸¸è§çš„ä»»åŠ¡

    * Automatic Text Generation: Deep learning model can learn the corpus of text and new text like summaries, essays can be automatically generated using these trained models.

    * Language translation: Deep learning models can translate text from one language to another, making it possible to communicate with people from different linguistic backgrounds. 

    * Sentiment analysis: Deep learning models can analyze the sentiment of a piece of text, making it possible to determine whether the text is positive, negative or neutral.

    * Speech recognition: Deep learning models can recognize and transcribe spoken words, making it possible to perform tasks such as speech-to-text conversion, voice search and voice-controlled devices. 

* Batch Normalization

    Batch Normalization (BN) is a critical technique in the training of neural networks, designed to address issues like vanishing or exploding gradients during training.

    Batch Normalization(BN) is a popular technique used in deep learning to improve the training of neural networks by normalizing the inputs of each layer.

    How Batch Normalization works?

    1. During each training iteration (epoch), BN takes a mini batch of data and normalizes the activations (outputs) of a hidden layer. This normalization transforms the activations to have a mean of 0 and a standard deviation of 1.

    2. While normalization helps with stability, it can also disrupt the network's learned features. To compensate, BN introduces two learnable parameters: gamma and beta. Gamma rescales the normalized activations, and beta shifts them, allowing the network to recover the information present in the original activations.

    It ensures that each element or component is in the right proportion before distributing the inputs into the layers and each layer is normalized before being passed to the next layer.

    PyTorch provides the nn.BatchNormXd module (where X is 1 for 1D data, 2 for 2D data like images, and 3 for 3D data) for convenient BN implementation.

    example:

    ```py
    # Define your neural network architecture with batch normalization
    class MLP(nn.Module):
        def __init__(self):
            super().__init__()
            self.layers = nn.Sequential(
                nn.Flatten(),                   # Flatten the input image tensor
                nn.Linear(28 * 28, 64),         # Fully connected layer from 28*28 to 64 neurons
                nn.BatchNorm1d(64),             # Batch normalization for stability and faster convergence
                nn.ReLU(),                      # ReLU activation function
                nn.Linear(64, 32),              # Fully connected layer from 64 to 32 neurons
                nn.BatchNorm1d(32),             # Batch normalization for stability and faster convergence
                nn.ReLU(),                      # ReLU activation function
                nn.Linear(32, 10)               # Fully connected layer from 32 to 10 neurons (for MNIST classes)
            )

        def forward(self, x):
            return self.layers(x)
    ```

    BN æ”¾åœ¨ ReLU ä¹‹å‰å’Œä¹‹åŽçš„åŒºåˆ«ï¼š

    * BN åœ¨ ReLU ä¹‹å‰ï¼ˆæ›´å¸¸è§çš„æƒ…å†µï¼‰ï¼š

        * æ•°æ®åˆ†å¸ƒæ›´å¯¹ç§°

            ```py
            # BN å…ˆå°†è¾“å…¥è§„èŒƒåŒ–ä¸º ~N(0,1)
            # è¿™æ · ReLU æ¿€æ´»æ—¶ï¼Œçº¦50%çš„ç¥žç»å…ƒä¼šè¢«æ¿€æ´»
            normalized = BN(linear_output)  # ~N(0,1)
            activated = ReLU(normalized)    # ä¸€åŠä¸º0ï¼Œä¸€åŠä¸ºæ­£
            ```

        * é¿å…ReLUçš„Dead Neuroné—®é¢˜

            å¦‚æžœæŸäº›ç¥žç»å…ƒè¾“å‡ºæ€»æ˜¯è´Ÿå€¼ï¼ŒReLUä¼šä½¿å…¶å®Œå…¨å¤±æ´», BNå…ˆè¿›è¡Œå½’ä¸€åŒ–ï¼Œå‡å°‘è¿™ç§æƒ…å†µ

        * ä¸ŽåŽŸå§‹è®ºæ–‡ä¸€è‡´

            Batch Normalization åŽŸå§‹è®ºæ–‡æŽ¨èæ”¾åœ¨æ¿€æ´»å‡½æ•°ä¹‹å‰

    * BNåœ¨ReLUä¹‹åŽ:

        * æ¿€æ´»å€¼ç›´æŽ¥å½’ä¸€åŒ–

            ```py
            activated = ReLU(linear_output)  # éƒ½æ˜¯éžè´Ÿæ•°
            normalized = BN(activated)       # å½’ä¸€åŒ–éžè´Ÿåˆ†å¸ƒ
            ```

        * ç›´æŽ¥å¯¹æ¿€æ´»åŽçš„å€¼è¿›è¡Œå½’ä¸€åŒ–, å¯èƒ½åœ¨æŸäº›æƒ…å†µä¸‹æ›´ç¨³å®š

    ä¸¤ç§é¡ºåºæ€§èƒ½å·®å¼‚é€šå¸¸å¾ˆå°ï¼Œå¯èƒ½å› ç½‘ç»œæž¶æž„ã€æ•°æ®é›†è€Œå¼‚ã€‚


    å¯¹äºŽæŸäº›æ¿€æ´»å‡½æ•°ï¼Œæ¯”å¦‚ Sigmoid/Tanhï¼Œé¡ºåºå¯èƒ½æ›´é‡è¦ï¼ŒBN åœ¨å‰å¯ä»¥é˜²æ­¢é¥±å’Œã€‚å¯¹äºŽ Leaky ReLUï¼šä¸¤ç§é¡ºåºå·®å¼‚å¯èƒ½æ›´å°

    BN å¯èƒ½æœ‰å®³çš„æƒ…å†µ:

    1. å°æ‰¹é‡å¤§å°ï¼ˆSmall Batch Sizeï¼‰

        ```py
        # å½“ batch_size å¾ˆå°æ—¶
        batch_size = 2  # æˆ–è€… 4, 8
        nn.BatchNorm1d(64)  # è¿™æ—¶å€™BNçš„ç»Ÿè®¡ä¼°è®¡ä¸å¯é ï¼Œå¯èƒ½æŸå®³æ€§èƒ½
        ```

    2. RNN/LSTM ç­‰åºåˆ—æ¨¡åž‹

        åœ¨RNNä¸­BNå¾ˆéš¾ç”¨ï¼Œé€šå¸¸ç”¨LayerNormä»£æ›¿, å› ä¸ºåºåˆ—é•¿åº¦å˜åŒ–ï¼ŒBNç»Ÿè®¡ä¸ç¨³å®š, æ•°æ®åˆ†å¸ƒä¸€ç›´åœ¨å˜ï¼ŒBNçš„running statsè·Ÿä¸ä¸Š

    3. å™ªå£°æ•æ„Ÿçš„ä»»åŠ¡

        åœ¨ä¸€äº›å¯¹å™ªå£°æ•æ„Ÿçš„ä»»åŠ¡ä¸­, BNå¼•å…¥çš„éšæœºæ€§ï¼ˆæ¥è‡ªbatchç»Ÿè®¡ï¼‰å¯èƒ½æœ‰å®³

    4. æŸäº›ç”Ÿæˆæ¨¡åž‹

        GANsä¸­BNæœ‰æ—¶ä¼šå¯¼è‡´æ¨¡å¼å´©æºƒ, å¾ˆå¤šçŽ°ä»£GANç”¨LayerNormæˆ–InstanceNormä»£æ›¿

    BN æ›´å¥½ç”¨çš„æƒ…å†µï¼š

    - å¤§åž‹æ•°æ®é›†ï¼ˆImageNetç­‰ï¼‰
    - è¶³å¤Ÿå¤§çš„batch_sizeï¼ˆ32+ï¼‰
    - å·ç§¯ç½‘ç»œ/MLP
    - ç¨³å®šçš„æ•°æ®åˆ†å¸ƒ

    Benefits of Batch Normalization

    * Faster Convergence: By stabilizing the gradients, BN allows you to use higher learning rates, which can significantly speed up training.
    
    * Reduced Internal Covariate Shift: As the network trains, the distribution of activations within a layer can change (internal covariate shift). BN helps mitigate this by normalizing activations before subsequent layers, making the training process less sensitive to these shifts.

    * Initialization Insensitivity: BN makes the network less reliant on the initial weight values, allowing for more robust training and potentially better performance.

* Apply a 2D Max Pooling in PyTorch

    There are two main types of pooling used in deep learning: Max Pooling and Average Pooling.

    Max Pooling: Max Pooling selects the maximum value from each set of overlapping filters and passes this maximum value to the next layer. This helps to retain the most important feature information while reducing the size of the representation.

    Average Pooling: Average Pooling computes the average value of each set of overlapping filters, and passes this average value to the next layer. This helps to retain a more general form of the feature information, but with a reduced spatial resolution.

    Pooling is usually applied after a convolution operation and helps to reduce overfitting and improve the generalization performance of the model.

* transforms.Compose èƒ½å¤ŸæŽ¥æ”¶ PIL.Image ç±»åž‹çš„å¯¹è±¡ï¼Œæ˜¯å› ä¸ºå®ƒå†…éƒ¨ç»„åˆçš„å„ä¸ªå˜æ¢ï¼ˆtransformï¼‰éƒ½å®žçŽ°äº†å¯¹ PIL.Image çš„å¤„ç†é€»è¾‘ã€‚

    å†…éƒ¨çš„å¯èƒ½å®žçŽ°å¦‚ä¸‹ï¼š

    ```py
    # åœ¨ torchvision/transforms/functional.py ä¸­
    def to_tensor(pic):
        """Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.
        
        Args:
            pic (PIL Image or numpy.ndarray): Image to be converted to tensor.
        """
        if not(_is_pil_image(pic) or _is_numpy(pic)):
            raise TypeError('pic should be PIL Image or ndarray. Got {}'.format(type(pic)))
        
        if _is_pil_image(pic):
            # å¤„ç†PIL.Imageçš„ä»£ç è·¯å¾„
            # ... å°†PILå›¾åƒè½¬ä¸ºnumpyï¼Œå†è½¬ä¸ºtensor
        elif _is_numpy(pic):
            # å¤„ç†numpyæ•°ç»„çš„ä»£ç è·¯å¾„
            # ... ç›´æŽ¥å¤„ç†numpyæ•°ç»„
        
        return result
    ```

* `SubsetRandomSampler()`

    ä»Žä¸€ä¸ªå®Œæ•´çš„æ•°æ®é›†ä¸­ï¼Œéšæœºåœ°é€‰å–ä¸€ä¸ªå­é›†ï¼Œå¹¶ä¸”åœ¨è¿™ä¸ªå­é›†ä¸Šè¿›è¡Œæ— æ”¾å›žåœ°éšæœºé‡‡æ ·ã€‚

    syntax:

    ```py
    torch.utils.data.SubsetRandomSampler(indices, generator=None)
    ```

    å‚æ•°è¯¦è§£

    1. indices

        ç±»åž‹: Sequence (åºåˆ—)

        è¯´æ˜Ž: è¿™æ˜¯ä¸€ä¸ªæ•´æ•°ç´¢å¼•çš„åºåˆ—ï¼Œç”¨äºŽæŒ‡å®šè¦ä»ŽåŽŸå§‹æ•°æ®é›†ä¸­æŠ½å–å“ªäº›æ ·æœ¬ã€‚

        è¯¦ç»†ä¿¡æ¯:

            å®ƒå¯ä»¥æ˜¯ä»»ä½• Python åºåˆ—ç±»åž‹ï¼Œå¦‚ list, range, numpy.array, torch.Tensor ç­‰ã€‚

            ç´¢å¼•å¯¹åº”çš„æ˜¯åŽŸå§‹æ•°æ®é›†ä¸­çš„æ ·æœ¬ä½ç½®ï¼ˆä»Ž 0 å¼€å§‹ï¼‰ã€‚

            é‡‡æ ·å™¨ä¼šä»Žè¿™äº›æŒ‡å®šçš„ç´¢å¼•ä¸­éšæœºæŠ½å–ï¼Œä¸ä¼šé‡å¤æŠ½å–åŒä¸€ä¸ªç´¢å¼•ï¼ˆæ— æ”¾å›žæŠ½æ ·ï¼‰ã€‚

            ç´¢å¼•çš„é¡ºåºä¸éœ€è¦æ˜¯æŽ’åºçš„ï¼Œä¹Ÿä¸éœ€è¦æ˜¯è¿žç»­çš„ã€‚

    2. generator

        ç±»åž‹: torch.Generator

        é»˜è®¤å€¼: None

        è¯´æ˜Ž: ç”¨äºŽæŽ§åˆ¶éšæœºæ•°ç”Ÿæˆçš„ç”Ÿæˆå™¨ã€‚

        è¯¦ç»†ä¿¡æ¯:

            å¦‚æžœæŒ‡å®šäº† generatorï¼Œé‡‡æ ·å™¨å°†ä½¿ç”¨è¿™ä¸ªç‰¹å®šçš„ç”Ÿæˆå™¨æ¥è¿›è¡Œéšæœºæ‰“ä¹±ã€‚

            å¦‚æžœä¸º Noneï¼ˆé»˜è®¤ï¼‰ï¼Œé‡‡æ ·å™¨å°†ä½¿ç”¨é»˜è®¤çš„éšæœºæ•°ç”Ÿæˆå™¨ã€‚

            è¿™ä¸ªå‚æ•°ä¸»è¦ç”¨äºŽç¡®ä¿ç»“æžœçš„å¯é‡çŽ°æ€§ã€‚å½“ä½ å¸Œæœ›æ¯æ¬¡è¿è¡Œä»£ç æ—¶éƒ½èƒ½å¾—åˆ°ç›¸åŒçš„éšæœºé¡ºåºæ—¶ï¼Œå¯ä»¥ä¼ å…¥ä¸€ä¸ªå›ºå®šç§å­çš„ç”Ÿæˆå™¨ã€‚

        example:

        ```py
        # ä½¿ç”¨å›ºå®šç§å­çš„ç”Ÿæˆå™¨ä»¥ç¡®ä¿å¯é‡çŽ°æ€§
        generator = torch.Generator().manual_seed(42)
        sampler = SubsetRandomSampler(indices, generator=generator)
        ```

    è¿”å›žå€¼

        è¿”å›žä¸€ä¸ª SubsetRandomSampler è¿­ä»£å™¨å¯¹è±¡ã€‚

        å½“åœ¨ DataLoader ä¸­è¿­ä»£æ—¶ï¼Œè¿™ä¸ªé‡‡æ ·å™¨ä¼šæŒ‰ç…§éšæœºé¡ºåºé€ä¸ªè¿”å›ž indices ä¸­çš„ç´¢å¼•ã€‚

        å½“éåŽ†å®Œæ‰€æœ‰ indices åŽï¼Œä¸€ä¸ª epoch å°±ç»“æŸäº†ã€‚

    example:

    ```py
    import torch
    from torch.utils.data import DataLoader, TensorDataset, SubsetRandomSampler

    # 1. åˆ›å»ºç¤ºä¾‹æ•°æ®é›†
    data = torch.randn(10, 3)  # 10ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬3ä¸ªç‰¹å¾
    labels = torch.arange(10)  # 10ä¸ªæ ‡ç­¾
    dataset = TensorDataset(data, labels)

    # 2. å®šä¹‰è¦ä½¿ç”¨çš„ç´¢å¼•
    indices = [2, 5, 1, 8, 3, 9]  # åªä½¿ç”¨è¿™6ä¸ªæ ·æœ¬

    # 3. åˆ›å»ºé‡‡æ ·å™¨ï¼ˆå¸¦å›ºå®šç”Ÿæˆå™¨ä»¥ç¡®ä¿å¯é‡çŽ°æ€§ï¼‰
    generator = torch.Generator().manual_seed(42)  # å›ºå®šéšæœºç§å­
    sampler = SubsetRandomSampler(indices, generator=generator)

    # 4. åˆ›å»º DataLoader
    dataloader = DataLoader(
        dataset, 
        batch_size=2, 
        sampler=sampler,  # ä½¿ç”¨è‡ªå®šä¹‰é‡‡æ ·å™¨
        # shuffle=True    # æ³¨æ„ï¼šè¿™é‡Œä¸èƒ½è®¾ç½® shuffle=Trueï¼
    )

    # 5. æµ‹è¯•è¾“å‡º
    for batch_idx, (data, target) in enumerate(dataloader):
        print(f"Batch {batch_idx}:")
        print(f"  Indices: {target.tolist()}")  # è¿™é‡Œtargetæ­£å¥½æ˜¯åŽŸå§‹ç´¢å¼•
        print(f"  Data shape: {data.shape}")
    ```

    ä½¿ç”¨æ—¶è¦é¿å…åœ¨ DataLoader ä¸­è®¾ç½® shuffle=Trueã€‚åŒæ—¶ï¼Œé€šå¸¸ä¹Ÿä¸æŒ‡å®š batch_samplerã€‚

    example: åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†

    ```py
    import torch
    from torch.utils.data import DataLoader, TensorDataset, SubsetRandomSampler

    # 1. åˆ›å»ºä¸€ä¸ªç¤ºä¾‹æ•°æ®é›† (10000ä¸ªæ ·æœ¬)
    dataset = TensorDataset(torch.randn(10000, 10), torch.randint(0, 2, (10000,)))

    # 2. å®šä¹‰æ•°æ®é›†æ€»å¤§å°å’Œåˆ’åˆ†æ¯”ä¾‹
    dataset_size = len(dataset)
    indices = list(range(dataset_size)) # ç”Ÿæˆ [0, 1, 2, ..., 9999] çš„ç´¢å¼•åˆ—è¡¨
    split = int(0.8 * dataset_size) # è®¡ç®—åˆ’åˆ†ç‚¹ï¼š8000

    # 3. éšæœºæ‰“ä¹±ç´¢å¼•ï¼Œä»¥ç¡®ä¿åˆ’åˆ†æ˜¯éšæœºçš„
    torch.manual_seed(42) # è®¾ç½®éšæœºç§å­ä»¥ä¿è¯ç»“æžœå¯å¤çŽ°
    indices.shuffle() # å°±åœ°æ‰“ä¹±ç´¢å¼•åˆ—è¡¨

    # 4. åˆ›å»ºè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„ç´¢å¼•å­é›†
    train_indices = indices[:split]   # å‰8000ä¸ªç´¢å¼•ä½œä¸ºè®­ç»ƒé›†
    val_indices = indices[split:]     # åŽ2000ä¸ªç´¢å¼•ä½œä¸ºéªŒè¯é›†

    # 5. åˆ›å»º SubsetRandomSampler
    train_sampler = SubsetRandomSampler(train_indices)
    val_sampler = SubsetRandomSampler(val_indices)

    # 6. åˆ›å»ºå¯¹åº”çš„ DataLoader
    train_loader = DataLoader(dataset, batch_size=64, sampler=train_sampler)
    val_loader = DataLoader(dataset, batch_size=64, sampler=val_sampler)

    # çŽ°åœ¨å°±å¯ä»¥åœ¨è®­ç»ƒå¾ªçŽ¯ä¸­ä½¿ç”¨ train_loaderï¼Œåœ¨éªŒè¯ä¸­ä½¿ç”¨ val_loader äº†
    # for data, target in train_loader:
    #     ...
    ```

    ä¸Ž Subset çš„åŒºåˆ«ï¼š

    * SubsetRandomSampler æ˜¯ä¸€ä¸ª é‡‡æ ·å™¨ï¼Œå®ƒä½œç”¨äºŽ DataLoader çº§åˆ«ã€‚DataLoader ä»ç„¶ä¼šéåŽ†æ•´ä¸ªæ•°æ®é›†ï¼Œä½†é‡‡æ ·å™¨å‘Šè¯‰å®ƒåªä»ŽæŒ‡å®šçš„ç´¢å¼•ä¸­å–æ•°æ®ã€‚

    * torch.utils.data.Subset æ˜¯ä¸€ä¸ª æ•°æ®é›†ï¼Œå®ƒç›´æŽ¥è¿”å›žåŽŸå§‹æ•°æ®é›†çš„ä¸€ä¸ªå­é›†ã€‚å½“ä½ ä½¿ç”¨ Subset åŽï¼Œå¾—åˆ°çš„å°±æ˜¯ä¸€ä¸ªå…¨æ–°çš„ã€æ›´å°çš„æ•°æ®é›†å¯¹è±¡ã€‚

    * å¦‚ä½•é€‰æ‹©ï¼šå¦‚æžœä½ éœ€è¦éšæœºæ‰“ä¹±ï¼Œç”¨ SubsetRandomSamplerã€‚å¦‚æžœä½ åªæ˜¯æƒ³é™æ€åœ°èŽ·å–ä¸€ä¸ªå­é›†ï¼ˆä¸æ‰“ä¹±ï¼‰ï¼Œå¯ä»¥ç”¨ Subsetã€‚

* PIL æ˜¾ç¤º np.array çš„å›¾ç‰‡

    ä½¿ç”¨ Image.fromarray()

    ```py
    from PIL import Image
    import numpy as np

    # åˆ›å»ºæˆ–åŠ è½½numpyæ•°ç»„
    # å‡è®¾ä½ çš„æ•°ç»„å½¢çŠ¶ä¸º (height, width, channels) æˆ– (height, width)
    array = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)

    # è½¬æ¢ä¸ºPILå›¾åƒ
    img = Image.fromarray(array)

    # æ˜¾ç¤ºå›¾åƒ
    img.show()
    ```

    PIL åªæŽ¥å— uint8 ç±»åž‹ï¼š

    ```py
    # å¯¹äºŽä¸åŒæ•°æ®ç±»åž‹çš„å¤„ç†
    # uint8 ç±»åž‹ (0-255)
    array_uint8 = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)
    img1 = Image.fromarray(array_uint8)

    # float ç±»åž‹ (0.0-1.0)
    array_float = np.random.rand(100, 100, 3)
    # éœ€è¦è½¬æ¢ä¸ºuint8
    array_float_uint8 = (array_float * 255).astype(np.uint8)
    img2 = Image.fromarray(array_float_uint8)
    ```

    å¤„ç†ç°åº¦å›¾åƒï¼š

    ```py
    # ç°åº¦å›¾åƒ (2Dæ•°ç»„)
    gray_array = np.random.randint(0, 255, (100, 100), dtype=np.uint8)
    gray_img = Image.fromarray(gray_array)
    gray_img.show()
    ```

    æ³¨æ„ï¼š

    * æ•°æ®ç±»åž‹ï¼šç¡®ä¿numpyæ•°ç»„çš„æ•°æ®ç±»åž‹æ˜¯np.uint8

    * æ•°å€¼èŒƒå›´ï¼šRGBå€¼åº”è¯¥åœ¨0-255èŒƒå›´å†…

    * æ•°ç»„å½¢çŠ¶ï¼š

        * å½©è‰²å›¾åƒï¼š(height, width, 3) æˆ– (height, width, 4)

        * ç°åº¦å›¾åƒï¼š(height, width)

* åœ¨ jupyter ä¸­æ˜¾ç¤º PIL å›¾ç‰‡

    ```py
    from PIL import Image
    from IPython.display import display

    img = Image.open('example.jpg')
    display(img)  # åœ¨ Jupyter ä¸­ç›´æŽ¥æ˜¾ç¤º
    ```

* PIL ç»“åˆ matplotlib æ˜¾ç¤ºå›¾ç‰‡

    ```py
    from PIL import Image
    import matplotlib.pyplot as plt

    file = '/home/hlc/Pictures/SAVE_20250313_134654.jpg'

    img = Image.open(file)
    plt.imshow(img)
    plt.axis('off')  # ä¸æ˜¾ç¤ºåæ ‡è½´
    plt.show()
    ```

* ä½¿ç”¨ PIL ç»“åˆ tk æ˜¾ç¤ºå›¾ç‰‡

    ```py
    from PIL import Image, ImageTk
    import tkinter as tk

    file = '/home/hlc/Pictures/SAVE_20250313_134654.jpg'

    # åˆ›å»ºä¸»çª—å£
    root = tk.Tk()
    root.title("PIL Image Display")

    # æ‰“å¼€å›¾ç‰‡
    img = Image.open(file)

    # è½¬æ¢ä¸º Tkinter å…¼å®¹çš„æ ¼å¼
    tk_img = ImageTk.PhotoImage(img)

    # åˆ›å»ºæ ‡ç­¾æ˜¾ç¤ºå›¾ç‰‡
    label = tk.Label(root, image=tk_img)
    label.pack()

    # è¿è¡Œä¸»å¾ªçŽ¯
    root.mainloop()
    ```

* PIL (Python Imaging Library) æ˜¾ç¤ºå›¾ç‰‡

    å®‰è£…ï¼š`pip install Pillow`

    ```py
    from PIL import Image

    img = Image.open('/home/hlc/Pictures/SAVE_20250313_134654.jpg')
    img.show()
    ```

    è¿™ä¸ªæ–¹æ³•ä¼šå°†å›¾ç‰‡ä¿å­˜ä¸ºä¸€ä¸ªä¸´æ—¶æ–‡ä»¶ï¼ˆé€šå¸¸æ˜¯ PNG æ ¼å¼ï¼‰, ç„¶åŽä½¿ç”¨æ“ä½œç³»ç»Ÿé»˜è®¤çš„å›¾ç‰‡æŸ¥çœ‹å™¨æ‰“å¼€è¯¥æ–‡ä»¶.

    åœ¨ Windows ä¸Šé€šå¸¸ç”¨"ç…§ç‰‡"åº”ç”¨ï¼Œåœ¨ macOS ä¸Šç”¨"é¢„è§ˆ"ï¼Œåœ¨ Linux ä¸Šç”¨ xdg-open

* ä½¿ç”¨å¤šç§ä¼˜åŒ–å¼çš„ training è¿‡ç¨‹

    ```py
    import torch
    import torchvision
    import torchvision.transforms as transforms

    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])

    trainset = torchvision.datasets.MNIST(root='./data', train=True,
                                            download=False, transform=transform)

    trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,
                                              shuffle=True, num_workers=2)

    class Net(torch.nn.Module):
        def __init__(self):
            super(Net, self).__init__()
            self.fc1 = torch.nn.Linear(784, 128)
            self.fc2 = torch.nn.Linear(128, 128)
            self.fc3 = torch.nn.Linear(128, 10)

        def forward(self, x):
            x = x.view(-1, 784)
            x = torch.nn.functional.relu(self.fc1(x))
            x = torch.nn.functional.relu(self.fc2(x))
            x = torch.nn.functional.softmax(self.fc3(x), dim=1)
            return x

    net = Net()

    criterion = torch.nn.CrossEntropyLoss()

    # SGD optimizer
    optimizer_sgd = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)

    # Adam optimizer
    optimizer_adam = torch.optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))
    # optimizer_adam = torch.optim.Adam(net.parameters(), lr=0.001)

    # Adagrad optimizer
    optimizer_adagrad = torch.optim.Adagrad(net.parameters(), lr=0.01)

    # Adadelta optimizer
    optimizer_adadelta = torch.optim.Adadelta(net.parameters(), rho=0.9)

    device = 'cpu'

    # Train the neural network using different optimization algorithms
    for epoch in range(10):
        running_loss = 0.0
        correct = 0
        total = 0
        for i, data in enumerate(trainloader, 0):
            inputs, labels = data
            inputs, labels = inputs.to(device), labels.to(device)
            # optimizer_sgd.zero_grad()
            optimizer_adam.zero_grad()
            # optimizer_adagrad.zero_grad()
            # optimizer_adadelta.zero_grad()
            outputs = net(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            # optimizer_sgd.step()
            optimizer_adam.step()
            # optimizer_adagrad.step()
            # optimizer_adadelta.step()
            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        print('Epoch: %d | Loss: %.3f | Accuracy: %.3f %%' %
              (epoch + 1, running_loss / len(trainloader), 100 * correct / total))
    ```

    output:

    ```
    Epoch: 1 | Loss: 1.618 | Accuracy: 85.717 %
    Epoch: 2 | Loss: 1.545 | Accuracy: 91.893 %
    Epoch: 3 | Loss: 1.526 | Accuracy: 93.702 %
    Epoch: 4 | Loss: 1.517 | Accuracy: 94.523 %
    Epoch: 5 | Loss: 1.511 | Accuracy: 95.153 %
    Epoch: 6 | Loss: 1.506 | Accuracy: 95.550 %
    Epoch: 7 | Loss: 1.503 | Accuracy: 95.872 %
    Epoch: 8 | Loss: 1.501 | Accuracy: 96.028 %
    Epoch: 9 | Loss: 1.500 | Accuracy: 96.173 %
    Epoch: 10 | Loss: 1.497 | Accuracy: 96.412 %
    ```

* adam ä¼˜åŒ–å™¨çš„å­¦ä¹ çŽ‡åº”è¯¥è®¾ç½®ä¸º sgd çš„ 1/10ï¼Œæ¯”å¦‚ sgd ä¸º 0.01ï¼Œadam åº”è¯¥è®¾ç½®ä¸º 0.001

* ä¸‹è½½ mnist æ•°æ®é›†

    ```py
    import torchvision
    from torchvision.datasets.utils import download_url
    import os

    mirror = "https://storage.googleapis.com/cvdf-datasets/mnist/"
    root = "./data/MNIST/raw"
    os.makedirs(root, exist_ok=True)

    files = {
        "train-images-idx3-ubyte.gz": "f68b3c2dcbeaaa9fbdd348bbdeb94873",
        "train-labels-idx1-ubyte.gz": "d53e105ee54ea40749a09fcbcd1e9432",
        "t10k-images-idx3-ubyte.gz": "9fb629c4189551a2d022fa330f9573f3",
        "t10k-labels-idx1-ubyte.gz": "ec29112dd5afa0611ce80d1b7f02629c",
    }

    for filename, md5 in files.items():
        download_url(mirror + filename, root, filename, md5)
    ```

    æ‰‹åŠ¨è§£åŽ‹ï¼š

    ```bash
    cd ./data/MNIST/raw
    gunzip *.gz
    ```

    æ­¤æ—¶å†è¿è¡Œï¼š

    ```py
    import torchvision

    trainset = torchvision.datasets.MNIST(root='./data', train=True, download=False)
    ```

    å³å¯æ­£å¸¸ä½¿ç”¨ã€‚

* convolution

    $$(f âˆ— g) (t)= \int_{-\infty}^{\infty} â€‹f(\tau) g(t âˆ’ \tau) d\tau$$

    Where f and g are functions representing the image and the filter respectively, and * denotes the convolution operator.

* Batch Processing for Efficient Training

    ```py
    for epoch in range(2):  
        for inputs, labels in dataloader:
            
            outputs = inputs + 1  
            print(f"Epoch {epoch + 1}, Inputs: {inputs}, Labels: {labels}, Outputs: {outputs}")
    ```

    ä¸æ¸…æ¥šä¸ºå•¥ outputs ä¼šæ˜¯ inputs + 1ã€‚è¿™ä¸ªçœ‹ä¸ŠåŽ»åªæ˜¯ä¸ªçŸ©é˜µæ‰€æœ‰å…ƒç´ åŠ ä¸€ï¼Œè€Œä¸”ä¹Ÿå¹¶ä¸æ˜¯åºåˆ—æ•°æ®ï¼Œæ¯”å¦‚ target = input + 1ã€‚è€Œä¸”è¿™ä¸ªä¹Ÿä¸åƒ c è¯­è¨€çš„ ptr -> ptr + 1 å°±å¯ä»¥æ‹¿åˆ°ä¸‹ä¸ªæ•°æ®ã€‚

    è¿™ä¸€æ­¥å¯èƒ½å’Œä¸Šä¸€æ­¥çš„ data aug ç»“åˆçš„ï¼Œå¦‚æžœèƒ½æ‰¾åˆ°ä¸Šä¸€æ­¥ data aug çš„ä»£ç ï¼Œå¯ä»¥è·‘è·‘çœ‹ï¼Œåˆ›å»ºå‡ºæ¥ dataloader åŽï¼Œå°±å¯ä»¥çœ‹åˆ° outputs å’Œ inputs çš„å†…å®¹äº†ã€‚

* imdb äºŒåˆ†ç±» example

    ```py
    from datasets import load_dataset
    from transformers import (AutoTokenizer,
                              AutoModelForSequenceClassification,
                              TrainingArguments,
                              Trainer)
    import numpy as np
    from sklearn.metrics import accuracy_score

    # 1. åŠ è½½æ•°æ®é›†å’Œåˆ†è¯å™¨
    dataset = load_dataset("imdb")
    model_checkpoint = "distilbert-base-uncased" # é€‰æ‹©ä¸€ä¸ªè½»é‡ä¸”é«˜æ•ˆçš„æ¨¡åž‹ï¼Œä¾‹å¦‚ DistilBERT
    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

    # 2. å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯å¤„ç†
    def tokenize_function(examples):
        # å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯ truncation å’Œ padding
        # è¿™é‡Œè®¾ç½®æœ€å¤§é•¿åº¦ï¼Œè¶…è¿‡çš„éƒ¨åˆ†ä¼šè¢«æˆªæ–­
        return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=256)

    # ä½¿ç”¨ map å‡½æ•°æ‰¹é‡å¤„ç†æ•´ä¸ªæ•°æ®é›†
    tokenized_datasets = dataset.map(tokenize_function, batched=True)

    # ä¸ºäº†èŠ‚çœæ—¶é—´å’Œå†…å­˜ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ›´å°çš„å­é›†è¿›è¡Œæ¼”ç¤ºï¼ˆå¯é€‰ï¼‰
    small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
    small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))

    # 3. åŠ è½½é¢„è®­ç»ƒæ¨¡åž‹
    # num_labels=2 è¡¨ç¤ºäºŒåˆ†ç±»
    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)

    # 4. å®šä¹‰è¯„ä¼°æŒ‡æ ‡
    def compute_metrics(eval_pred):
        logits, labels = eval_pred
        predictions = np.argmax(logits, axis=-1)
        return {"accuracy": accuracy_score(labels, predictions)}

    # 5. è®¾ç½®è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir="./my_imdb_model",      # è¾“å‡ºç›®å½•ï¼Œæ¨¡åž‹å’Œæ£€æŸ¥ç‚¹ä¼šä¿å­˜åœ¨è¿™é‡Œ
        evaluation_strategy="epoch",       # æ¯ä¸ª epoch ç»“æŸåŽè¿›è¡Œè¯„ä¼°
        learning_rate=2e-5,                # å­¦ä¹ çŽ‡
        per_device_train_batch_size=16,    # è®­ç»ƒæ‰¹æ¬¡å¤§å°
        per_device_eval_batch_size=16,     # è¯„ä¼°æ‰¹æ¬¡å¤§å°
        num_train_epochs=3,                # è®­ç»ƒè½®æ•°
        weight_decay=0.01,                 # æƒé‡è¡°å‡
    )

    # 6. åˆ›å»º Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=small_train_dataset, # ä½¿ç”¨å­é›†ï¼Œå®Œæ•´è®­ç»ƒè¯·ç”¨ tokenized_datasets["train"]
        eval_dataset=small_eval_dataset,   # ä½¿ç”¨å­é›†ï¼Œå®Œæ•´è¯„ä¼°è¯·ç”¨ tokenized_datasets["test"]
        compute_metrics=compute_metrics,
        tokenizer=tokenizer, # ç¡®ä¿åˆ†è¯å™¨åœ¨ä¿å­˜æ¨¡åž‹æ—¶ä¹Ÿè¢«ä¿å­˜
    )

    # 7. å¼€å§‹è®­ç»ƒï¼
    trainer.train()

    # 8. åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡åž‹ï¼ˆä½¿ç”¨æˆ‘ä»¬åˆ›å»ºçš„å°å­é›†ï¼‰
    final_metrics = trainer.evaluate(small_eval_dataset)
    print(f"\næœ€ç»ˆè¯„ä¼°ç»“æžœ: {final_metrics}")

    # 9. ä¿å­˜æ¨¡åž‹ï¼ˆå¯é€‰ï¼‰
    # trainer.save_model("./my_final_imdb_model")
    ```

* RNN (å¾ªçŽ¯ç¥žç»ç½‘ç»œ) 

    RNNæ˜¯ä¸€ç§ä¸“é—¨ç”¨äºŽå¤„ç†åºåˆ—æ•°æ®çš„ç¥žç»ç½‘ç»œã€‚å®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šç½‘ç»œèƒ½å¯¹åºåˆ—ä¸­çš„å…ƒç´ è¿›è¡Œå¾ªçŽ¯æ“ä½œï¼Œä¸”èƒ½å¤Ÿé€šè¿‡å†…éƒ¨çŠ¶æ€ï¼ˆéšè—çŠ¶æ€ï¼‰è®°ä½ä¹‹å‰çš„ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨è¿™äº›ä¿¡æ¯æ¥å½±å“åŽç»­çš„è¾“å‡ºã€‚

    æ ¸å¿ƒç‰¹å¾ï¼š

    * â€œå¾ªçŽ¯â€ä¸Žâ€œè®°å¿†â€ï¼šRNNå•å…ƒä¸ä»…æŽ¥æ”¶å½“å‰çš„è¾“å…¥ï¼ˆå¦‚å¥å­ä¸­çš„ä¸€ä¸ªè¯ï¼‰ï¼Œè¿˜æŽ¥æ”¶æ¥è‡ªä¸Šä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼ˆHidden Stateï¼‰ã€‚è¿™ä¸ªéšè—çŠ¶æ€å……å½“äº†ç½‘ç»œçš„â€œè®°å¿†â€ï¼Œå®ƒåŒ…å«äº†ä¹‹å‰æ‰€æœ‰æ—¶é—´æ­¥çš„åºåˆ—ä¿¡æ¯ã€‚

    * å‚æ•°å…±äº«ï¼šRNNåœ¨æ¯ä¸ªæ—¶é—´æ­¥ä¸Šä½¿ç”¨ç›¸åŒçš„æƒé‡å‚æ•°ï¼ˆU, W, Vï¼‰ã€‚è¿™ä½¿å¾—æ¨¡åž‹å¯ä»¥å¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—ï¼Œå¹¶å‡å°‘éœ€è¦è®­ç»ƒçš„å‚æ•°æ•°é‡ã€‚

    * è®¡ç®—è¿‡ç¨‹ï¼š

        * åœ¨ä»»æ„æ—¶é—´æ­¥ $t$ï¼š

            * æ–°çš„éšè—çŠ¶æ€ $h_t$ ç”±å½“å‰è¾“å…¥ $x_t$ å’Œå‰ä¸€ä¸ªéšè—çŠ¶æ€ $h_{t-1}$ å…±åŒè®¡ç®—å¾—å‡ºï¼š$h_t = \tanh(W \cdot h_{t-1} + U \cdot x_t + b)$

            * è¾“å‡º $o_t$ ç”±å½“å‰éšè—çŠ¶æ€ $h_t$ è®¡ç®—å¾—å‡ºï¼š$o_t = \mathrm{softmax}(V \cdot h_t + c)$

    * å¸¸è§é—®é¢˜ï¼š

        æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸ï¼ˆVanishing/Exploding Gradientsï¼‰ï¼šåœ¨å¤„ç†é•¿åºåˆ—æ—¶ï¼ŒRNNéš¾ä»¥å­¦ä¹ åˆ°è¿œè·ç¦»æ—¶é—´æ­¥ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œå› ä¸ºæ¢¯åº¦åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ä¼šæŒ‡æ•°çº§åœ°å‡å°æˆ–å¢žå¤§ã€‚

    example:

    ```py
    import torch
    import torch.nn as nn
    import numpy as np
    import matplotlib.pyplot as plt

    # 1. è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æžœå¯å¤çŽ°
    torch.manual_seed(42)
    np.random.seed(42)

    # 2. ç”Ÿæˆæ­£å¼¦æ³¢åºåˆ—æ•°æ®
    def generate_sine_wave_data(seq_length=50, num_samples=1000):
        """
        ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼šç”¨å‰seq_lengthä¸ªç‚¹é¢„æµ‹ç¬¬seq_length+1ä¸ªç‚¹
        X: [num_samples, seq_length, 1]
        y: [num_samples, 1]
        """
        time_steps = np.linspace(0, 100, num_samples + seq_length)
        data = np.sin(time_steps)
        data = data.reshape(-1, 1) # è½¬æ¢ä¸ºç‰¹å¾ç»´åº¦ä¸º1

        X = []
        y = []
        for i in range(num_samples):
            X.append(data[i:i+seq_length])
            y.append(data[i+seq_length])
        
        return np.array(X), np.array(y)

    # ç”Ÿæˆæ•°æ®
    seq_length = 10
    X, y = generate_sine_wave_data(seq_length, 1000)
    X = torch.from_numpy(X).float()
    y = torch.from_numpy(y).float()

    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    train_ratio = 0.8
    train_size = int(train_ratio * len(X))
    X_train, y_train = X[:train_size], y[:train_size]
    X_test, y_test = X[train_size:], y[train_size:]

    # 3. å®šä¹‰ç®€å•çš„RNNæ¨¡åž‹
    class SinePredictor(nn.Module):
        def __init__(self, input_size=1, hidden_size=50, output_size=1):
            super(SinePredictor, self).__init__()
            self.hidden_size = hidden_size
            # ä½¿ç”¨ä¸€ä¸ªRNNå±‚
            self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
            # å…¨è¿žæŽ¥å±‚ç”¨äºŽè¾“å‡ºé¢„æµ‹
            self.fc = nn.Linear(hidden_size, output_size)
        
        def forward(self, x):
            # xçš„å½¢çŠ¶: (batch_size, seq_length, input_size)
            # out: æ‰€æœ‰æ—¶é—´æ­¥çš„éšè—çŠ¶æ€ (batch_size, seq_length, hidden_size)
            # hidden: æœ€åŽä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ (1, batch_size, hidden_size)
            out, hidden = self.rnn(x)
            # æˆ‘ä»¬åªä½¿ç”¨æœ€åŽä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€æ¥è¿›è¡Œé¢„æµ‹
            out = self.fc(out[:, -1, :]) # å–åºåˆ—çš„æœ€åŽä¸€ä¸ªè¾“å‡º
            return out

    # åˆå§‹åŒ–æ¨¡åž‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    model = SinePredictor()
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # 4. è®­ç»ƒæ¨¡åž‹
    num_epochs = 100
    train_losses = []

    for epoch in range(num_epochs):
        model.train()
        optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­
        outputs = model(X_train)
        loss = criterion(outputs, y_train)
        
        # åå‘ä¼ æ’­å’Œä¼˜åŒ–
        loss.backward()
        optimizer.step()
        
        train_losses.append(loss.item())
        
        if (epoch+1) % 10 == 0:
            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.6f}')

    # 5. è¯„ä¼°æ¨¡åž‹å¹¶å¯è§†åŒ–
    model.eval()
    with torch.no_grad():
        train_predictions = model(X_train)
        test_predictions = model(X_test)

    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(train_losses)
    plt.title('Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('MSE Loss')

    # ç»˜åˆ¶ä¸€éƒ¨åˆ†æµ‹è¯•é›†ä¸Šçš„çœŸå®žå€¼å’Œé¢„æµ‹å€¼
    plt.subplot(1, 2, 2)
    # å–å‰100ä¸ªæµ‹è¯•ç‚¹è¿›è¡Œç»˜åˆ¶
    plt.plot(y_test[:100].numpy(), label='True Value', alpha=0.7)
    plt.plot(test_predictions[:100].numpy(), label='Prediction', alpha=0.7)
    plt.title('Sine Wave Prediction on Test Set')
    plt.legend()

    plt.tight_layout()
    plt.show()

    # æ‰“å°æœ€ç»ˆè®­ç»ƒæŸå¤±å’Œæµ‹è¯•æŸå¤±
    with torch.no_grad():
        test_loss = criterion(test_predictions, y_test)
    print(f'Final Training Loss: {train_losses[-1]:.6f}')
    print(f'Final Test Loss: {test_loss.item():.6f}')
    ```

    output:

    ```
    Epoch [10/100], Loss: 0.302159
    Epoch [20/100], Loss: 0.080454
    Epoch [30/100], Loss: 0.047464
    Epoch [40/100], Loss: 0.025511
    Epoch [50/100], Loss: 0.005334
    Epoch [60/100], Loss: 0.002867
    Epoch [70/100], Loss: 0.001352
    Epoch [80/100], Loss: 0.001240
    Epoch [90/100], Loss: 0.000831
    Epoch [100/100], Loss: 0.000813
    Final Training Loss: 0.000813
    Final Test Loss: 0.000799
    ```

    ä»£ç è¯´æ˜Žï¼š

    * æ•°æ®ç”Ÿæˆï¼šæˆ‘ä»¬ç”Ÿæˆäº†ä¸€ä¸ªæ­£å¼¦æ³¢ï¼Œå¹¶åˆ›å»ºäº†è¾“å…¥-è¾“å‡ºå¯¹ã€‚æ¯ä¸ªè¾“å…¥æ˜¯ä¸€ä¸ªé•¿åº¦ä¸ºseq_lengthçš„åºåˆ—ï¼Œè¾“å‡ºæ˜¯åºåˆ—åŽçš„ä¸‹ä¸€ä¸ªå€¼ã€‚

    * æ¨¡åž‹å®šä¹‰ï¼š

        * `nn.RNN`å±‚æ˜¯æ ¸å¿ƒï¼Œå®ƒå¤„ç†è¾“å…¥åºåˆ—å¹¶è¿”å›žæ‰€æœ‰æ—¶é—´æ­¥çš„è¾“å‡ºå’Œæœ€åŽä¸€ä¸ªéšè—çŠ¶æ€ã€‚

        * æˆ‘ä»¬åªä½¿ç”¨äº†æœ€åŽä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼ˆout[:, -1, :]ï¼‰å¹¶é€šè¿‡ä¸€ä¸ªå…¨è¿žæŽ¥å±‚(nn.Linear)æ¥ç”Ÿæˆæœ€ç»ˆçš„é¢„æµ‹å€¼ã€‚è¿™æ˜¯ä¸€ç§å¸¸è§çš„åšæ³•ï¼Œé€‚ç”¨äºŽâ€œå¤šå¯¹ä¸€â€çš„åºåˆ—ä»»åŠ¡ã€‚

    * è®­ç»ƒï¼šä½¿ç”¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ä½œä¸ºæŸå¤±å‡½æ•°ï¼ŒAdamä½œä¸ºä¼˜åŒ–å™¨ã€‚

    * è¯„ä¼°ï¼šæ¨¡åž‹åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹ï¼Œå¹¶ç»˜åˆ¶ç»“æžœå›¾ã€‚ä½ ä¼šçœ‹åˆ°é¢„æµ‹æ›²çº¿ï¼ˆæ©™è‰²ï¼‰èƒ½å¤Ÿå¾ˆå¥½åœ°è·ŸéšçœŸå®žæ­£å¼¦æ›²çº¿ï¼ˆè“è‰²ï¼‰ã€‚

    æ³¨ï¼š

    1. ç”»å‡ºæ¥çš„å›¾æ˜¯ [0, 100]ï¼Œå®žé™…ä¸Šç»™å‡ºçš„æ˜¯ y_test å’Œ y_pred çš„æœ€åŽ 100 ä¸ªæ•°æ®ï¼Œå¹¶ä¸æ˜¯ x æ•°æ®çš„ 0 åˆ° 100ï¼Œæ‰€ä»¥ sin å›¾åƒåªæœ‰ 1 ä¸ªåŠçš„æ³¢é•¿

    1. `time_steps = np.linspace(0, 100, num_samples + seq_length)`ï¼Œå…¶ä¸­çš„`num_samples + seq_length`è¡¨ç¤ºï¼Œx ä¸€å…±æœ‰`num_samples`ä¸ªï¼Œä½†`x_i`å¹¶ä¸æ˜¯æ ‡é‡ï¼Œè€Œæ˜¯ä¸€ä¸ªé•¿åº¦ä¸º`seq_length`çš„å‘é‡ï¼Œ`y_i`åˆ™ä¸º`x_i[0]`åŽçš„ç¬¬`seq_length + 1`ä¸ªæ•°ï¼Œæ˜¯ä¸ªæ ‡é‡ã€‚

        å› æ­¤ä¸ºäº† x èµ·å§‹ä½ç½®å…±æœ‰`num_sample`ä¸ªï¼Œè€Œ y çš„æœ€å¤§å€¼åˆ™éœ€è¦æ¯” y å†å¤š`seq_length`ä¸ªã€‚è¿™å°±æ˜¯æ‰€æœ‰éœ€è¦ç”¨åˆ°çš„æ•°æ®ã€‚

* éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰

    æ²¿ç€æŸå¤±å‡½æ•°æ¢¯åº¦çš„åæ–¹å‘æ›´æ–°å‚æ•°ï¼Œä»Žè€Œæœ€å°åŒ–æŸå¤±å‡½æ•°ã€‚

    åŸºæœ¬SGDï¼ˆæ— åŠ¨é‡ï¼‰:

    å¯¹äºŽä¸€ç»„å¯å­¦ä¹ çš„å‚æ•°ï¼ˆæƒé‡ï¼‰$\theta$ï¼ŒæŸå¤±å‡½æ•°ä¸º $J(\theta)$ï¼Œå­¦ä¹ çŽ‡ä¸º $\eta$ã€‚

    åœ¨æ¯ä¸€æ­¥ï¼ˆæ¯ä¸ªbatchï¼‰$t$ï¼ŒåŸºæœ¬çš„SGDæ›´æ–°è§„åˆ™ä¸ºï¼š

    $\theta_{t+1} = \theta_t - \eta \cdot \nabla_\theta J_t (\theta_t)$

    å…¶ä¸­ï¼š

    * $\theta_t$ æ˜¯ç¬¬ `t` æ­¥ï¼ˆè¿­ä»£ï¼‰æ—¶çš„å‚æ•°å€¼ã€‚

    * $\nabla_\theta J_t(\theta_t)$ æ˜¯ç¬¬ `t `æ­¥æŸå¤±å‡½æ•° $J_t$ å…³äºŽå‚æ•° $\theta$ çš„æ¢¯åº¦ï¼ˆåœ¨å½“å‰ batch ä¸Šè®¡ç®—å¾—å‡ºï¼‰ã€‚

    * $\eta$ æ˜¯å­¦ä¹ çŽ‡ï¼ˆlearning rateï¼‰ï¼ŒæŽ§åˆ¶æ¯æ¬¡æ›´æ–°çš„æ­¥é•¿ã€‚

* `tens.index_copy_()`

    å°†æŒ‡å®šç»´åº¦ä¸Šçš„æŒ‡å®šç´¢å¼•ï¼ˆå¯ä»¥æ˜¯å¤šä¸ªï¼‰å¤åˆ¶åˆ°`tens`çš„å¯¹åº”ä½ç½®ã€‚

    syntax:

    ```py
    index_copy_(dim, index, tensor) -> Tensor
    ```

    example:

    ```py
    import torch

    tens_1 = torch.ones(4, 4)
    tens_2 = torch.randn(2, 4)
    my_indices = torch.tensor([1,3])

    tens_1.index_copy_(0, my_indices, tens_2)
    print("tens_1: {}".format(tens_1))
    ```

    output:

    ```
    tens_1: tensor([[ 1.0000,  1.0000,  1.0000,  1.0000],
            [ 0.3654,  0.9840, -0.4651,  1.4270],
            [ 1.0000,  1.0000,  1.0000,  1.0000],
            [ 0.0722, -1.2526, -0.8574, -1.2249]])
    ```

    æ³¨æ„ï¼š

    * `my_indices`å…ƒç´ çš„æ•°é‡å¿…é¡»å’Œ`tens_2`åœ¨`dim`ç»´åº¦ä¸Šçš„é•¿åº¦å¯¹åº”ï¼Œå³`my_indices.size() == tens_2.shape[dim]`ã€‚ä¸Šé¢ä¾‹å­ä¸­ï¼Œå¦‚æžœ`tens_2 = torch.randn(3, 4)`ï¼Œåˆ™ä¼šæŠ¥é”™ã€‚

    `index_copy()`æ˜¯å…¶ out-of-place ç‰ˆæœ¬ã€‚

* torch æ‹Ÿåˆ xor å‡½æ•°

    ```py
    import torch
    import torch.nn as nn
    from torch import optim

    class SimpleNN(nn.Module):
        def __init__(self):
            super(SimpleNN, self).__init__()
            self.fc1 = nn.Linear(2, 4)
            self.fc2 = nn.Linear(4, 1)

        def forward(self, x):
            x = torch.relu(self.fc1(x))
            x = self.fc2(x)
            return x
        
    X_train = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])
    y_train = torch.tensor([[0.0], [1.0], [1.0], [0.0]])

    # Instantiate the Model, Define Loss Function and Optimizer
    model = SimpleNN()
    criterion = nn.MSELoss()
    optimizer = optim.SGD(model.parameters(), lr=0.1)

    for epoch in range(100):
        model.train()

        # Forward pass
        outputs = model(X_train)
        loss = criterion(outputs, y_train)
        
        # Backward pass and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if (epoch + 1) % 10 == 0:
            print(f'Epoch [{epoch + 1}/100], Loss: {loss.item():.4f}')

    model.eval()
    with torch.no_grad():
        test_data = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])
        predictions = model(test_data)
        print(f'Predictions:\n{predictions}')
    ```

    output:

    ```
    Epoch [10/100], Loss: 0.2205
    Epoch [20/100], Loss: 0.1844
    Epoch [30/100], Loss: 0.1600
    Epoch [40/100], Loss: 0.1357
    Epoch [50/100], Loss: 0.1115
    Epoch [60/100], Loss: 0.0890
    Epoch [70/100], Loss: 0.0671
    Epoch [80/100], Loss: 0.0481
    Epoch [90/100], Loss: 0.0320
    Epoch [100/100], Loss: 0.0199
    Predictions:
    tensor([[0.1897],
            [0.9428],
            [0.8315],
            [0.0905]])
    ```

    è¯´æ˜Žï¼š

    1. `super(SimpleNN, self).__init__()`ä¸Ž`super().__init__()`æ˜¯ç­‰ä»·çš„

    1. `model.train()`å°†æ¨¡åž‹åˆ‡æ¢ä¸ºè®­ç»ƒæ¨¡å¼ï¼Œä¸éœ€è¦å†™æˆ`model = model.train()`

        ç‰¹ç‚¹ï¼š

        * Dropoutå±‚ä¼šéšæœºä¸¢å¼ƒç¥žç»å…ƒ

        * BatchNormå±‚ä½¿ç”¨å½“å‰æ‰¹æ¬¡çš„ç»Ÿè®¡é‡ï¼ˆå‡å€¼å’Œæ–¹å·®ï¼‰

        * å¯ç”¨æ¢¯åº¦è®¡ç®—ï¼ˆautogradï¼‰

        * é€‚åˆè®­ç»ƒé˜¶æ®µä½¿ç”¨

    1. `model.eval()`å°†æ¨¡åž‹åˆ‡æ¢ä¸ºè¯„ä¼°æ¨¡å¼

        * Dropoutå±‚ä¸ä¼šä¸¢å¼ƒç¥žç»å…ƒï¼ˆæ‰€æœ‰ç¥žç»å…ƒéƒ½å‚ä¸Žè®¡ç®—ï¼‰

        * BatchNormå±‚ä½¿ç”¨è®­ç»ƒé˜¶æ®µå­¦åˆ°çš„è¿è¡Œç»Ÿè®¡é‡

        * é€šå¸¸ä¸Žtorch.no_grad()ä¸€èµ·ä½¿ç”¨æ¥ç¦ç”¨æ¢¯åº¦è®¡ç®—

        * é€‚åˆæµ‹è¯•ã€éªŒè¯å’ŒæŽ¨ç†é˜¶æ®µä½¿ç”¨

* å¸¦è‡ªå›žå½’çš„ Encoder-Decoder æž¶æž„

    ä¸€ç§ç”¨äºŽå¤„ç†åºåˆ—åˆ°åºåˆ—ï¼ˆSeq2Seqï¼‰ ä»»åŠ¡çš„æ·±åº¦å­¦ä¹ æ¨¡åž‹æ¡†æž¶ã€‚å®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†ä¸€ä¸ªè¾“å…¥åºåˆ—ï¼ˆå¦‚ä¸€å¥è‹±æ–‡å¥å­ï¼‰è½¬æ¢ä¸ºä¸€ä¸ªè¾“å‡ºåºåˆ—ï¼ˆå¦‚å¯¹åº”çš„ä¸­æ–‡å¥å­ï¼‰ï¼Œå¹¶ä¸”è¾“å‡ºåºåˆ—çš„ç”Ÿæˆæ˜¯é€æ­¥ã€è‡ªå›žå½’åœ°è¿›è¡Œçš„ã€‚

    * Encoderï¼ˆç¼–ç å™¨ï¼‰ï¼š

        * ä½œç”¨ï¼šè¯»å–å¹¶ç†è§£æ•´ä¸ªè¾“å…¥åºåˆ—ã€‚

        * å·¥ä½œæ–¹å¼ï¼šå®ƒæŽ¥æ”¶æ•´ä¸ªè¾“å…¥åºåˆ—ï¼ˆä¾‹å¦‚ â€œI love machine learningâ€ï¼‰ï¼Œå¹¶é€šè¿‡ç¥žç»ç½‘ç»œï¼ˆé€šå¸¸æ˜¯ RNN, LSTM, GRU æˆ– Transformerï¼‰å°†å…¶åŽ‹ç¼©æˆä¸€ä¸ªå›ºå®šç»´åº¦çš„ä¸Šä¸‹æ–‡å‘é‡ï¼ˆContext Vectorï¼‰ æˆ–ä¸€ç»„éšè—çŠ¶æ€ã€‚è¿™ä¸ªå‘é‡/çŠ¶æ€é›†æ—¨åœ¨åŒ…å«è¾“å…¥åºåˆ—çš„å…¨éƒ¨è¯­ä¹‰ä¿¡æ¯ã€‚

    * Decoderï¼ˆè§£ç å™¨ï¼‰ï¼š

        * ä½œç”¨ï¼šæ ¹æ®ç¼–ç å™¨çš„ä¿¡æ¯å’Œå·²ç”Ÿæˆçš„éƒ¨åˆ†è¾“å‡ºï¼Œé€æ­¥ç”Ÿæˆå®Œæ•´çš„è¾“å‡ºåºåˆ—ã€‚

        * å·¥ä½œæ–¹å¼ï¼šè§£ç å™¨çš„ç”Ÿæˆè¿‡ç¨‹æ˜¯è‡ªå›žå½’çš„ï¼ˆAutoregressiveï¼‰ã€‚è¿™æ˜¯æœ€å…³é”®çš„ä¸€ç‚¹ã€‚

            * è‡ªå›žå½’ï¼šæ„å‘³ç€åœ¨ç”Ÿæˆè¾“å‡ºåºåˆ—çš„æ¯ä¸€ä¸ªæ–°è¯ï¼ˆæˆ– tokenï¼‰æ—¶ï¼Œéƒ½ä¼šå°†ä¹‹å‰å·²ç»ç”Ÿæˆçš„æ‰€æœ‰è¯ä½œä¸ºé¢å¤–è¾“å…¥ã€‚

            * å…·ä½“æ­¥éª¤ï¼š

                1. è§£ç å™¨ä»Žç¼–ç å™¨å¾—åˆ°çš„ä¸Šä¸‹æ–‡å‘é‡å’Œä¸€å€‹ç‰¹æ®Šçš„å¼€å§‹ç¬¦ï¼ˆå¦‚ <start>ï¼‰ å¼€å§‹ã€‚
                
                2. å®ƒäº§ç”Ÿç¬¬ä¸€ä¸ªè¾“å‡ºè¯ï¼ˆå¦‚ â€œæˆ‘â€ï¼‰ã€‚

                3. ç„¶åŽï¼Œå®ƒå°†è¿™ä¸ªåˆšåˆšç”Ÿæˆçš„è¯â€œæˆ‘â€ï¼ˆè€Œä¸æ˜¯çœŸå®žçš„ç›®æ ‡è¯ï¼‰å’Œå½“å‰çš„éšè—çŠ¶æ€ä¸€èµ·ä½œä¸ºè¾“å…¥ï¼Œæ¥ç”Ÿæˆä¸‹ä¸€ä¸ªè¯â€œçˆ±â€ã€‚

                4. å¦‚æ­¤å¾ªçŽ¯ï¼Œæ¯æ¬¡ç”Ÿæˆéƒ½ä¾èµ–äºŽä¹‹å‰çš„è¾“å‡ºï¼Œç›´åˆ°ç”Ÿæˆä¸€ä¸ªç‰¹æ®Šçš„ç»“æŸç¬¦ï¼ˆå¦‚ `<end>`ï¼‰ è¡¨ç¤ºç”Ÿæˆä¸ºæ­¢ã€‚

    ç®€å•æ¯”å–»ï¼š

    å°±åƒä¸€ä¸ªåŒå£°ä¼ è¯‘å‘˜ã€‚

    * Encoderï¼šå¬å®Œæ•´å¥è‹±æ–‡ï¼Œå¹¶ç†è§£å…¶å«ä¹‰ã€‚

    * Decoderï¼šå¼€å§‹ç”¨ä¸­æ–‡ç¿»è¯‘ï¼Œæ¯è¯´ä¸€ä¸ªè¯ï¼ˆâ€œæˆ‘â€ï¼‰ï¼Œéƒ½ä¼šå‚è€ƒè‡ªå·±åˆšæ‰è¯´çš„è¯å’Œå¬åˆ°çš„è‹±æ–‡åŽŸæ„ï¼Œæ¥å†³å®šä¸‹ä¸€ä¸ªè¯è¯´ä»€ä¹ˆï¼ˆâ€œçˆ±â€ï¼‰ï¼Œç›´åˆ°ç¿»è¯‘å®Œæ•´ä¸ªå¥å­ã€‚

    ç›¸å…³çš„æ¨¡åž‹ï¼š

    * RNN-based Seq2Seq (2014)

        ç”± Sutskever ç­‰äººå’Œ Bahdanau ç­‰äººæå‡ºã€‚

        ä½¿ç”¨RNNæˆ–LSTMä½œä¸ºEncoderå’ŒDecoderçš„æ ¸å¿ƒã€‚æœ€åˆçš„æ¨¡åž‹å°†æ•´ä¸ªè¾“å…¥åºåˆ—åŽ‹ç¼©æˆä¸€ä¸ªå›ºå®šçš„ä¸Šä¸‹æ–‡å‘é‡ï¼Œè¿™åœ¨å¤„ç†é•¿åºåˆ—æ—¶ä¼šé€ æˆä¿¡æ¯ç“¶é¢ˆã€‚

        æ”¹è¿›ï¼šæ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttention Mechanismï¼‰ è¢«å¼•å…¥ï¼ˆBahdanau et al.ï¼‰ï¼Œå…è®¸è§£ç å™¨åœ¨ç”Ÿæˆæ¯ä¸ªè¯æ—¶â€œå›žå¤´çœ‹â€ç¼–ç å™¨çš„æ‰€æœ‰éšè—çŠ¶æ€ï¼Œä»Žè€ŒåŠ¨æ€åœ°èŽ·å–æœ€ç›¸å…³çš„ä¿¡æ¯ï¼Œæžå¤§æå‡äº†é•¿åºåˆ—çš„å¤„ç†èƒ½åŠ›ã€‚ï¼ˆæ³¨æ„ï¼šå¸¦æ³¨æ„åŠ›çš„Seq2Seqæ˜¯æžå…¶é‡è¦çš„å˜ä½“ï¼‰

    * transformer (2017)

        ç”± Vaswani ç­‰äººåœ¨è®ºæ–‡ã€ŠAttention Is All You Needã€‹ä¸­æå‡ºã€‚

        å®Œå…¨åŸºäºŽè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰ çš„æ¨¡åž‹ï¼Œå½»åº•æŠ›å¼ƒäº†RNNã€‚å®ƒä»ç„¶æ˜¯Encoder-Decoderæž¶æž„ï¼Œä½†å…¶ç¼–ç å’Œè§£ç çš„æ–¹å¼å‘ç”Ÿäº†é©å‘½æ€§å˜åŒ–ã€‚

        Encoderï¼šç”±å¤šå±‚è‡ªæ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œç»„æˆï¼Œå¹¶è¡Œå¤„ç†æ•´ä¸ªè¾“å…¥åºåˆ—ã€‚

        Decoderï¼šåŒæ ·æ˜¯è‡ªå›žå½’çš„ï¼Œä½†åœ¨è‡ªæ³¨æ„åŠ›å±‚ä¸­åŠ å…¥äº†æŽ©ç ï¼ˆMaskï¼‰ï¼Œç¡®ä¿åœ¨ç”Ÿæˆä½ç½® i çš„è¯æ—¶ï¼Œåªèƒ½çœ‹åˆ°ä½ç½® 1 åˆ° i-1 çš„è¯ï¼Œè€Œä¸èƒ½çœ‹åˆ°â€œæœªæ¥â€çš„ä¿¡æ¯ã€‚

    * åŸºäºŽTransformerçš„è‘—åæ¨¡åž‹ï¼ˆéƒ½å±žäºŽæ­¤æž¶æž„ï¼‰

        * GPT ç³»åˆ—ï¼šä¸¥æ ¼æ¥è¯´ï¼ŒGPTæ˜¯åªæœ‰Decoderçš„æ¨¡åž‹ã€‚å®ƒé€šè¿‡æŽ©ç è‡ªæ³¨æ„åŠ›å®žçŽ°è‡ªå›žå½’ç”Ÿæˆï¼Œå¯ä»¥çœ‹ä½œæ˜¯Decoder-onlyæž¶æž„ï¼Œä½†å…¶æ ¸å¿ƒæ€æƒ³â€”â€”è‡ªå›žå½’ç”Ÿæˆâ€”â€”ä¸ŽEncoder-Decoderä¸­çš„Decoderéƒ¨åˆ†å®Œå…¨ç›¸åŒã€‚

        * BART å’Œ T5ï¼šè¿™äº›æ˜¯ç»å…¸çš„ã€çœŸæ­£çš„å¸¦è‡ªå›žå½’Decoderçš„Encoder-Decoderæ¨¡åž‹ã€‚å®ƒä»¬åœ¨é¢„è®­ç»ƒæ—¶ä¸“é—¨ä¸ºæ­¤æž¶æž„è®¾è®¡ï¼ˆå¦‚é€šè¿‡åŽ»å™ªã€æ–‡æœ¬å¡«å……ç­‰ä»»åŠ¡ï¼‰ï¼Œåœ¨æ‘˜è¦ã€ç¿»è¯‘ã€é—®ç­”ç­‰ä»»åŠ¡ä¸Šè¡¨çŽ°å“è¶Šã€‚

        * çŽ°ä»£å¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰ï¼šå¦‚ ChatGPT èƒŒåŽçš„æ¨¡åž‹ï¼Œè™½ç„¶å…¶åŸºç¡€ï¼ˆGPTï¼‰æ˜¯Decoder-onlyï¼Œä½†å…¶é€šè¿‡æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Tuningï¼‰å’Œäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰å­¦ä¼šäº†å¾ˆå¤šâ€œç†è§£-ç”Ÿæˆâ€çš„å¯¹è¯èƒ½åŠ›ï¼Œå…¶ç”Ÿæˆå›žå¤çš„è¿‡ç¨‹å°±æ˜¯å…¸åž‹çš„è‡ªå›žå½’æ–¹å¼ã€‚

    * å¥ åŸºæ€§è®ºæ–‡ï¼š

        * Seq2Seq å¼€åˆ›ï¼šSutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural networks. In NeurIPS. [å¿…è¯»]

        * æ³¨æ„åŠ›æœºåˆ¶ï¼šBahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473. [å¿…è¯»]

        * Transformerï¼šVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In NeurIPS. [å¿…è¯»ä¸­çš„å¿…è¯»]

* å¯ä»¥è·‘é€šçš„ pytorch example

    ```py
    import torch as t
    import torch
    from torch import nn
    from torch.utils.data import DataLoader
    from torchvision import datasets
    from torchvision.transforms import ToTensor

    # Get cpu, gpu or mps device for training.
    device = (
        "cuda"
        if torch.cuda.is_available()
        else "mps"
        if torch.backends.mps.is_available()
        else "cpu"
    )
    print(f"Using {device} device")

    def train(dataloader, model, loss_fn, optimizer):
        size = len(dataloader.dataset)
        model.train()
        for batch, (X, y) in enumerate(dataloader):
            X, y = X.to(device), y.to(device)

            # Compute prediction error
            pred = model(X)
            loss = loss_fn(pred, y)

            # Backpropagation
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            if batch % 100 == 0:
                loss, current = loss.item(), (batch + 1) * len(X)
                print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")

    def test(dataloader, model, loss_fn):
        size = len(dataloader.dataset)
        num_batches = len(dataloader)
        model.eval()
        test_loss, correct = 0, 0
        with torch.no_grad():
            for X, y in dataloader:
                X, y = X.to(device), y.to(device)
                pred = model(X)
                test_loss += loss_fn(pred, y).item()
                correct += (pred.argmax(1) == y).type(torch.float).sum().item()
        test_loss /= num_batches
        correct /= size
        print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")

    def main():
        training_data = datasets.FashionMNIST(
            root="data",
            train=True,
            download=True,
            transform=ToTensor(),
        )

        test_data = datasets.FashionMNIST(
            root="data",
            train=False,
            download=True,
            transform=ToTensor(),
        )

        batch_size = 64

        # Create data loaders.
        train_dataloader = DataLoader(training_data, batch_size=batch_size)
        test_dataloader = DataLoader(test_data, batch_size=batch_size)

        for X, y in test_dataloader:
            print(f"Shape of X [N, C, H, W]: {X.shape}")
            print(f"Shape of y: {y.shape} {y.dtype}")
            break

        # Define model
        class NeuralNetwork(nn.Module):
            def __init__(self):
                super().__init__()
                self.flatten = nn.Flatten()
                self.linear_relu_stack = nn.Sequential(
                    nn.Linear(28*28, 512),
                    nn.ReLU(),
                    nn.Linear(512, 512),
                    nn.ReLU(),
                    nn.Linear(512, 10)
                )

            def forward(self, x):
                x = self.flatten(x)
                logits = self.linear_relu_stack(x)
                return logits

        model = NeuralNetwork().to(device)
        print(model)

        loss_fn = nn.CrossEntropyLoss()
        optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)

        epochs = 5
        for t in range(epochs):
            print(f"Epoch {t+1}\n-------------------------------")
            train(train_dataloader, model, loss_fn, optimizer)
            test(test_dataloader, model, loss_fn)
        print("Done!")

        torch.save(model.state_dict(), "model.pth")
        print("Saved PyTorch Model State to model.pth")

        model = NeuralNetwork().to(device)
        model.load_state_dict(torch.load("model.pth", weights_only=True))

        classes = [
            "T-shirt/top",
            "Trouser",
            "Pullover",
            "Dress",
            "Coat",
            "Sandal",
            "Shirt",
            "Sneaker",
            "Bag",
            "Ankle boot",
        ]

        model.eval()
        x, y = test_data[0][0], test_data[0][1]
        with torch.no_grad():
            x = x.to(device)
            pred = model(x)
            predicted, actual = classes[pred[0].argmax(0)], classes[y]
            print(f'Predicted: "{predicted}", Actual: "{actual}"')

        return

    if __name__ == '__main__':
        main()
    ```

    output:

    ```
    Using cuda device
    Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])
    Shape of y: torch.Size([64]) torch.int64
    NeuralNetwork(
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (linear_relu_stack): Sequential(
        (0): Linear(in_features=784, out_features=512, bias=True)
        (1): ReLU()
        (2): Linear(in_features=512, out_features=512, bias=True)
        (3): ReLU()
        (4): Linear(in_features=512, out_features=10, bias=True)
      )
    )
    Epoch 1
    -------------------------------
    loss: 2.301282  [   64/60000]
    loss: 2.282217  [ 6464/60000]
    loss: 2.261925  [12864/60000]
    loss: 2.257128  [19264/60000]
    loss: 2.247277  [25664/60000]
    loss: 2.211387  [32064/60000]
    loss: 2.218867  [38464/60000]
    loss: 2.184250  [44864/60000]
    loss: 2.178685  [51264/60000]
    loss: 2.146116  [57664/60000]
    Test Error: 
     Accuracy: 52.2%, Avg loss: 2.137231 

    Epoch 2
    -------------------------------
    loss: 2.150523  [   64/60000]
    loss: 2.139497  [ 6464/60000]
    loss: 2.077158  [12864/60000]
    loss: 2.098047  [19264/60000]
    loss: 2.051788  [25664/60000]
    loss: 1.977449  [32064/60000]
    loss: 2.012526  [38464/60000]
    loss: 1.926008  [44864/60000]
    loss: 1.933322  [51264/60000]
    loss: 1.853627  [57664/60000]
    Test Error: 
     Accuracy: 60.0%, Avg loss: 1.850576 

    Epoch 3
    -------------------------------
    loss: 1.884275  [   64/60000]
    loss: 1.859825  [ 6464/60000]
    loss: 1.733056  [12864/60000]
    loss: 1.781410  [19264/60000]
    loss: 1.680241  [25664/60000]
    loss: 1.617407  [32064/60000]
    loss: 1.645341  [38464/60000]
    loss: 1.538832  [44864/60000]
    loss: 1.571115  [51264/60000]
    loss: 1.457203  [57664/60000]
    Test Error: 
     Accuracy: 62.4%, Avg loss: 1.475583 

    Epoch 4
    -------------------------------
    loss: 1.537457  [   64/60000]
    loss: 1.513721  [ 6464/60000]
    loss: 1.354834  [12864/60000]
    loss: 1.441262  [19264/60000]
    loss: 1.327532  [25664/60000]
    loss: 1.310910  [32064/60000]
    loss: 1.334382  [38464/60000]
    loss: 1.248879  [44864/60000]
    loss: 1.292152  [51264/60000]
    loss: 1.186263  [57664/60000]
    Test Error: 
     Accuracy: 64.9%, Avg loss: 1.212287 

    Epoch 5
    -------------------------------
    loss: 1.276597  [   64/60000]
    loss: 1.273734  [ 6464/60000]
    loss: 1.098410  [12864/60000]
    loss: 1.221964  [19264/60000]
    loss: 1.097947  [25664/60000]
    loss: 1.114543  [32064/60000]
    loss: 1.145893  [38464/60000]
    loss: 1.072613  [44864/60000]
    loss: 1.119054  [51264/60000]
    loss: 1.029024  [57664/60000]
    Test Error: 
     Accuracy: 66.1%, Avg loss: 1.050324 

    Done!
    Saved PyTorch Model State to model.pth
    Predicted: "Ankle boot", Actual: "Ankle boot"
    ```

## topics

### å†…å­˜å¸ƒå±€ï¼ŒGPU ä¸Ž CPU

* ä½¿ç”¨ permute å¯¼è‡´ tensor å˜æˆ continuous çš„ä¾‹å­

    ```py
    import torch as t

    a = t.rand(3, 4)
    print('a shape: {}'.format(a.shape))
    a = a.permute(1, 0)
    print('after permute, a shape: {}'.format(a.shape))
    print('is continuous: {}'.format(a.is_contiguous()))
    a = a.view(2, 6)
    ```

    output:

    ```
    a shape: torch.Size([3, 4])
    after permute, a shape: torch.Size([4, 3])
    is continuous: False
    Traceback (most recent call last):
      File "/home/hlc/Documents/Projects/torch_test/main.py", line 8, in <module>
        a = a.view(2, 6)
    RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.
    ```

* å°† tensor ä»Ž cpu è½¬ç§»åˆ° gpu

    * æŽ¨èæŽ¥å£`.to()`

        ```py
        import torch

        # å‡è®¾æœ‰ä¸€ä¸ªåœ¨ CPU ä¸Šçš„ tensor
        cpu_tensor = torch.tensor([1, 2, 3])
        print(cpu_tensor.device) # è¾“å‡ºï¼šcpu

        # æ£€æŸ¥ GPU æ˜¯å¦å¯ç”¨
        if torch.cuda.is_available():
            device = torch.device("cuda") # æŒ‡å®šç›®æ ‡è®¾å¤‡ä¸º GPU
            gpu_tensor = cpu_tensor.to(device) # è½¬ç§»åˆ° GPU
            print(gpu_tensor.device) # è¾“å‡ºï¼šcuda:0

            # ä½ ä¹Ÿå¯ä»¥ç›´æŽ¥ä½¿ç”¨å­—ç¬¦ä¸²
            gpu_tensor_2 = cpu_tensor.to('cuda')
        ```

    * æ—§å…¼å®¹æŽ¥å£`.cuda()`

        ```py
        if torch.cuda.is_available():
            gpu_tensor = cpu_tensor.cuda() # è½¬ç§»åˆ°é»˜è®¤ GPU (cuda:0)
            gpu_tensor = cpu_tensor.cuda(0) # æ˜Žç¡®è½¬ç§»åˆ°ç¬¬ä¸€ä¸ª GPU
        ```

    åœ¨åˆ›å»ºæ—¶æŒ‡å®šè®¾å¤‡ï¼š

    ```py
    # ç›´æŽ¥åœ¨ GPU ä¸Šåˆ›å»º tensorï¼ŒçœåŽ»è½¬ç§»æ­¥éª¤
    gpu_tensor = torch.tensor([1, 2, 3], device='cuda')
    # æˆ–è€…
    gpu_tensor = torch.tensor([1, 2, 3]).to('cuda')
    ```

* list åœ¨ append tensor æ—¶ï¼Œéœ€è¦ tensor clone()ï¼Œå¦åˆ™ append çš„éƒ½æ˜¯ tensor çš„å¼•ç”¨ï¼Œå€¼éƒ½æ˜¯ä¸€æ¨¡ä¸€æ ·çš„

    `params_record.append(param.clone().detach())`

* å°† tensor æ•°æ®æ”¾åˆ° gpu é‡Œ

    ```py
    # æ£€æŸ¥è®¾å¤‡
    print("Tensor è®¾å¤‡:", torch_tensor.device)

    # å¦‚æžœéœ€è¦ï¼Œç§»åŠ¨åˆ° GPU
    if torch.cuda.is_available():
        torch_tensor = torch_tensor.cuda()
    ```

* `tensor.view()`å’Œ`tensor.reshape()`éƒ½æ˜¯æµ…æ‹·è´ï¼Œ`reshape()`å¯èƒ½æ˜¯æ·±æ‹·è´

    ```py
    import torch

    # åŽŸå§‹å¼ é‡
    original_tensor = torch.arange(6)  # tensor([0, 1, 2, 3, 4, 5])
    reshaped_tensor = original_tensor.view(2, 3)

    # ä¿®æ”¹reshapeåŽçš„å¼ é‡
    reshaped_tensor[0, 0] = 100

    print(original_tensor)  # tensor([100,   1,   2,   3,   4,   5])
    print(reshaped_tensor)  # tensor([[100,   1,   2],
                            #         [  3,   4,   5]])
    ```

    output:

    ```
    tensor([100,   1,   2,   3,   4,   5])
    tensor([[100,   1,   2],
            [  3,   4,   5]])
    ```

    å¯ä»¥çœ‹åˆ°ï¼Œä¿®æ”¹ reshaped_tensor ä¹Ÿä¼šå½±å“ original_tensorï¼Œå› ä¸ºå®ƒä»¬å…±äº«åº•å±‚æ•°æ®å­˜å‚¨ã€‚

    å¦‚æžœåŽŸå§‹å¼ é‡åœ¨å†…å­˜ä¸­ä¸æ˜¯è¿žç»­çš„ï¼Œview() å¯èƒ½ä¼šå¤±è´¥ï¼Œæ­¤æ—¶éœ€è¦ä½¿ç”¨ reshape()ï¼š

    ```py
    # è½¬ç½®æ“ä½œä¼šåˆ›å»ºä¸è¿žç»­çš„å¼ é‡
    non_contiguous = original_tensor.t()  # è½¬ç½®

    # å¯èƒ½ä¼šæŠ¥é”™
    reshaped = non_contiguous.view(2, 3)
    print('view reshaped: {}'.format(reshaped))

    # åº”è¯¥ä½¿ç”¨reshape()
    reshaped = non_contiguous.reshape(2, 3)  # åŒæ ·ä¹Ÿæ˜¯æµ…æ‹·è´
    print('reshape reshaped: {}'.format(reshaped))
    ```

    output:

    ```
    view reshaped: tensor([[0, 1, 2],
            [3, 4, 5]])
    reshape reshaped: tensor([[0, 1, 2],
            [3, 4, 5]])
    ```

    ç›®å‰çœ‹åˆ°ä½¿ç”¨ view ä¹Ÿæ²¡æœ‰æŠ¥é”™ï¼Œä¸æ¸…æ¥šä¸ºä»€ä¹ˆã€‚

    å¦‚æžœéœ€è¦æ·±æ‹·è´ï¼Œå¯ä»¥ä½¿ç”¨ clone() æ–¹æ³•ï¼š

    ```py
    # åˆ›å»ºçœŸæ­£çš„æ·±æ‹·è´
    deep_copy = original_tensor.view(2, 3).clone()

    # ä¿®æ”¹æ·±æ‹·è´ä¸ä¼šå½±å“åŽŸå§‹å¼ é‡
    deep_copy[0, 0] = 999
    print(original_tensor)  # ä¸ä¼šè¢«ä¿®æ”¹
    ```

    é¦–å…ˆ`.view()`ä¸€å®šæ˜¯æµ…æ‹·è´ã€‚å¯¹äºŽ`.reshape()`ï¼Œå¦‚æžœå¼ é‡æ˜¯ è¿žç»­çš„ï¼Œreshape() å†…éƒ¨ç›´æŽ¥è°ƒç”¨ view()ï¼›å¦‚æžœå¼ é‡æ˜¯ éžè¿žç»­çš„ï¼ˆä¾‹å¦‚ç»è¿‡ transposeï¼‰ï¼Œreshape() ä¼šå…ˆè°ƒç”¨ .contiguous()ï¼ŒæŠŠæ•°æ®æ•´ç†æˆæ ‡å‡†å¸ƒå±€ï¼ˆå¼€è¾Ÿæ–°å†…å­˜ã€å¤åˆ¶æ•°æ®ï¼‰ï¼Œæ­¤æ—¶ä¼šå‘ç”Ÿæ·±æ‹·è´ï¼Œç„¶åŽå†è°ƒç”¨ view()ã€‚

* permute å’Œ transpose éƒ½æ˜¯åªäº¤æ¢ç»´åº¦ï¼Œä¸æ”¹å˜åº•å±‚æ•°æ®ï¼Œæ‰€ä»¥ä¼šé€ æˆ tensor ä¸è¿žç»­

* å…³äºŽ`tensor.view()`ä¸Žå†…å­˜çš„è®¨è®º

    * view() åœ¨ PyTorch ä¸­åªæ˜¯æ”¹å˜å¼ é‡çš„ è§†å›¾ï¼Œä¸åšå®žé™…çš„æ•°æ®æ‹·è´ï¼Œå› æ­¤è¦æ±‚åº•å±‚å†…å­˜æ˜¯ è¿žç»­çš„ (contiguous)ã€‚å¦‚æžœåŽŸå§‹å¼ é‡ä¸æ˜¯è¿žç»­çš„ï¼ˆä¾‹å¦‚ç»è¿‡ transposeã€permute ç­‰æ“ä½œï¼‰ï¼Œç›´æŽ¥è°ƒç”¨ view() å°±ä¼šæŠ¥é”™ã€‚

    * reshape() æ›´çµæ´»ï¼šå®ƒä¼šå°è¯•è¿”å›žä¸€ä¸ª viewï¼Œä½†å¦‚æžœæ•°æ®åœ¨å†…å­˜ä¸­ä¸è¿žç»­ï¼Œå®ƒä¼šè‡ªåŠ¨åšä¸€æ¬¡æ‹·è´ï¼ŒæŠŠæ•°æ®æ•´ç†æˆè¿žç»­çš„ï¼Œå†è¿”å›žç»“æžœã€‚å› æ­¤ reshape() ä¸€å®šèƒ½æˆåŠŸï¼ˆåªè¦æ–°å½¢çŠ¶æ˜¯åˆæ³•çš„ï¼‰ã€‚

    example:

    ```py
    import torch

    # åˆ›å»ºä¸€ä¸ª 2x3 å¼ é‡
    a = torch.arange(6).reshape(2, 3)
    print("åŽŸå§‹ a:\n", a)

    # è½¬ç½®ï¼Œå¾—åˆ°éžè¿žç»­å†…å­˜çš„å¼ é‡
    b = a.t()   # transpose
    print("è½¬ç½® b:\n", b)
    print("b æ˜¯å¦è¿žç»­:", b.is_contiguous())  # False

    # å°è¯• view
    try:
        aaa = b.view(-1)
        print('aaa: {}'.format(aaa))
    except RuntimeError as e:
        print("view æŠ¥é”™:", e)

    # ä½¿ç”¨ reshape åˆ™æ²¡é—®é¢˜
    c = b.reshape(-1)
    print("reshape æˆåŠŸ:", c)
    ```

    output:

    ```
    åŽŸå§‹ a:
     tensor([[0, 1, 2],
            [3, 4, 5]])
    è½¬ç½® b:
     tensor([[0, 3],
            [1, 4],
            [2, 5]])
    b æ˜¯å¦è¿žç»­: False
    view æŠ¥é”™: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.
    reshape æˆåŠŸ: tensor([0, 3, 1, 4, 2, 5])
    ```
    
    PyTorch Tensor åº•å±‚ç”±ä¸¤ä¸ªå…³é”®éƒ¨åˆ†ç»„æˆï¼š

    1. Storageï¼ˆå­˜å‚¨åŒºï¼‰

        ä½¿ç”¨ä¸€å—è¿žç»­çš„å†…å­˜ï¼ˆ1D arrayï¼‰ï¼Œå­˜æ”¾æ‰€æœ‰å…ƒç´ ã€‚ä¸ä½¿ç”¨é“¾è¡¨æˆ–åˆ†æ•£å—å­˜å‚¨ã€‚

        å³ä½¿æ˜¯å¤šç»´å¼ é‡ï¼Œæœ¬è´¨ä¸Šè¿˜æ˜¯åœ¨ä¸€ç»´æ•°ç»„é‡Œã€‚

    2. Tensor å…ƒä¿¡æ¯ï¼šsize + stride

        * sizeï¼šæ¯ä¸€ç»´çš„é•¿åº¦ã€‚

        * strideï¼šæ¯ä¸€ç»´è·¨è¶Šçš„æ­¥é•¿ï¼ˆåœ¨å†…å­˜é‡Œéš”å¤šå°‘å…ƒç´ ç®—ä¸€æ­¥ï¼‰ã€‚

        ä¾‹å­ï¼š

        shape ä¸º (2, 3) çš„å¼ é‡ï¼Œstride = (3, 1)ã€‚

        å¦‚æžœæˆ‘ä»¬å¯¹å…¶è¿›è¡Œè½¬ç½®ï¼ˆtransposeï¼‰ï¼Œé‚£ä¹ˆ torch ä¼šå®žè¡Œä¸€ä¸ª trickï¼Œå³åªäº¤æ¢ç»´åº¦ä¿¡æ¯ï¼Œä¸æ”¹å˜åº•å±‚æ•°æ®ï¼Œæ­¤æ—¶ stride ä¼šå˜æˆ (1, 3)ï¼Œæˆ‘ä»¬é€šè¿‡ç´¢å¼•`arr[m][n]`å¯ä»¥æ­£ç¡®è®¿é—®åˆ°è½¬ç½®åŽçš„æ•°æ®ï¼Œä½†æ˜¯æ­¤æ—¶å®ƒå·²ç»ä¸å†æ˜¯å…ˆè¡ŒåŽåˆ—çš„å«ä¹‰äº†ï¼Œå› æ­¤ä¸è¿žç»­ã€‚

        å¦‚æžœæˆ‘ä»¬æ”¹å˜åº•å±‚æ•°æ®ï¼Œä½¿å®ƒæ˜¯è¿žç»­çš„ï¼Œé‚£ä¹ˆè½¬ç½®åŽçš„ tensorï¼Œshape ä¸º (3, 2)ï¼Œstride ä¸º (2, 1)ã€‚

        `stride[i]`è¡¨ç¤ºåœ¨ç¬¬ i ç»´ä¸Š ç´¢å¼•åŠ  1ï¼Œåœ¨åº•å±‚ 1D å­˜å‚¨é‡Œéœ€è¦ç§»åŠ¨å¤šå°‘ä¸ªå…ƒç´ ã€‚

    ä¸‹é¢çš„ä»£ç è§£é‡Šäº† torch ä¸­ transpose() çš„ trick:

    ```py
    import numpy as np

    class Arr:
        def __init__(self, arr, m: int, n: int):
            self.arr = arr
            self.shape = [m, n]
            self.stride = [n, 1]

        def view(self, m: int, n: int):
            self.shape = [m, n]
            self.stride = [n, 1]

        def transpose(self):
            self.shape = [self.shape[1], self.shape[0]]
            self.stride = [1, self.stride[0]]

        def get(self, i, j):
            return self.arr[i * self.stride[0] + j * self.stride[1]]

    def print_arr(arr: Arr):
        for i in range(arr.shape[0]):
            for j in range(arr.shape[1]):
                print('{}, '.format(arr.get(i, j)), end='')
            print()
        print()
        return

    def main():
        data = np.arange(3 * 4)
        arr = Arr(data, 3, 4)

        print('arr (3 x 4):')
        print_arr(arr)

        arr.view(4, 3)
        print('arr (4 x 3):')
        print_arr(arr)

        arr.view(3, 4)  # back to original state
        arr.transpose()
        print('arr transposed (4 x 3):')
        print_arr(arr)

        return

    if __name__ == '__main__':
        main()
    ```

    output:

    ```
    arr (3 x 4):
    0, 1, 2, 3, 
    4, 5, 6, 7, 
    8, 9, 10, 11, 

    arr (4 x 3):
    0, 1, 2, 
    3, 4, 5, 
    6, 7, 8, 
    9, 10, 11, 

    arr transposed (4 x 3):
    0, 4, 8, 
    1, 5, 9, 
    2, 6, 10, 
    3, 7, 11,
    ```

    å¦‚æžœæˆ‘ä»¬éœ€è¦å°†è¿™ç§éžè¿žç»­çš„åº•å±‚æ•°æ®å˜æˆè¿žç»­çš„ï¼Œé‚£ä¹ˆå¯ä»¥è°ƒç”¨`.contiguous()`æ–¹æ³•å°†å…¶å˜æˆè¿žç»­çš„ã€‚

### tensor è¿ç®— / tensor æ“ä½œ

* index_fill_

    'Val' value is filled with the elements of 'x' along with the order of indices given in the vector 'index'.

    syntax:

    ```py
    index_fill_(dim, index, val) â†’ Tensor
    ```

    è¿™ä¸ªå‡½æ•°ä¸­çš„`val`æ˜¯ä¸ª scalarã€‚

    å¯¹åº”çš„ out of place ç‰ˆæœ¬ï¼š

    `index_fill()`

    `index_put_()`, `index_put()`:

    This operation puts the value of 'val' into the self tensor using the indices of the given 'index'.

    syntax:

    ```py
    index_put_(indices, values, accumulate=False) â†’ Tensor
    ```

    å°† value æ”¾åˆ° indices æŒ‡å®šçš„ä½ç½®ã€‚è¿™é‡Œçš„ value æ˜¯ä¸ª vectorï¼Œindices åˆ™æ˜¯ tensor ä¸­è¦ä¿®æ”¹çš„æ•°æ®çš„ç´¢å¼•ï¼ˆå¯èƒ½æ˜¯å¤šç»´çš„ï¼‰ã€‚

    example:

    ```py
    #importing libraries
    import torch
     
    target=torch.zeros([4,4])
    indices = torch.LongTensor([[0,1],[1,2],[3,1],[1,0]])#indices to which values to be put
    value = torch.ones(indices.shape[0])
    #tuple of the index tensor is passed along with the value
    target.index_put_(tuple(indices.t()), value)
    ```

    output:

    ```
    tensor([[0., 1., 0., 0.],
           [1., 0., 1., 0.],
           [0., 0., 0., 0.],
           [0., 1., 0., 0.]])
    ```

    å¦‚æžœ`accumulate`ä¸º trueï¼Œé‚£ä¹ˆæ–°å…ƒç´ ä¼šå åŠ åˆ°æ—§å…ƒç´ ä¸Šã€‚

    `index_select()`:

    A tensor is returned with indices as mentioned, by selecting from the target tensor.

    syntax:

    ```py
    torch.index_select(input, dim, index, out=None) 
    ```
    
    é€‰å–æŒ‡å®šç»´åº¦çš„å‡ è¡Œ/å‡ åˆ—ã€‚

    è¿™ä¸ªæ“ä½œå¯ä»¥ç›´æŽ¥ç”¨`[:, [y_1, y_2], :]`è¿™ç§ç´¢å¼•æ–¹å¼å®Œæˆï¼Œæ„Ÿè§‰æ¯”è¾ƒé¸¡è‚‹ã€‚

* `squeeze()`

    ç§»é™¤æ‰€æœ‰é•¿åº¦ä¸º 1 çš„ç»´åº¦ï¼ˆæˆ–è€…åªç§»é™¤æŒ‡å®šç»´åº¦ï¼Œå¦‚æžœå…¶é•¿åº¦ä¸º 1ï¼‰ã€‚

    example:

    ```py
    # æŽ¥ä¸Šé¢çš„ä¾‹å­
    x = torch.randn(1, 4, 1, 2)
    print(f"Original shape: {x.shape}") # torch.Size([1, 4, 1, 2])

    y = x.squeeze() # ç§»é™¤æ‰€æœ‰é•¿åº¦ä¸º1çš„ç»´åº¦
    print(f"After squeeze(): {y.shape}") # torch.Size([4, 2])

    z = x.squeeze(0) # åªç§»é™¤ç¬¬0ç»´ï¼Œå¦‚æžœå…¶é•¿åº¦ä¸º1
    print(f"After squeeze(0): {z.shape}") # torch.Size([4, 1, 2])

    w = x.squeeze(2) # åªç§»é™¤ç¬¬2ç»´ï¼Œå¦‚æžœå…¶é•¿åº¦ä¸º1
    print(f"After squeeze(2): {w.shape}") # torch.Size([1, 4, 2])
    ```

* `unsqueeze()`

    åœ¨å¼ é‡çš„æŒ‡å®šç»´åº¦ä¸Šå¢žåŠ ä¸€ä¸ªé•¿åº¦ä¸º 1 çš„ç»´åº¦ã€‚è¿™ä¸ªæ“ä½œé€šå¸¸ä¹Ÿè¢«ç§°ä¸ºâ€œå‡ç»´â€ã€‚

    syntax:

    ```py
    torch.unsqueeze(input, dim) â†’ Tensor
    ```

    * input: è¾“å…¥å¼ é‡ã€‚

    * dim: ä¸€ä¸ªæ•´æ•°ï¼ŒæŒ‡å®šåœ¨å“ªä¸ªä½ç½®æ’å…¥æ–°çš„ç»´åº¦ã€‚è¿™ä¸ªæ–°ç»´åº¦çš„é•¿åº¦å°†ä¸º 1ã€‚

        dim çš„å–å€¼èŒƒå›´æ˜¯ [-input.dim()-1, input.dim()]ã€‚

        * æ­£ç´¢å¼•: ä»Žå‰å¾€åŽæ•°ï¼Œ0 è¡¨ç¤ºåœ¨æœ€å‰é¢æ’å…¥ã€‚

        * è´Ÿç´¢å¼•: ä»ŽåŽå¾€å‰æ•°ï¼Œ-1 è¡¨ç¤ºåœ¨æœ€åŽä¸€ä¸ªç»´åº¦ä¹‹åŽæ’å…¥ã€‚

    è¿™æ˜¯ä¸€ä¸ªâ€œè§†å›¾æ“ä½œâ€ï¼Œæ„å‘³ç€å®ƒé€šå¸¸ä¸ä¼šå¤åˆ¶åº•å±‚æ•°æ®ï¼Œè€Œåªæ˜¯æ”¹å˜äº†çœ‹å¾…æ•°æ®çš„â€œè§†è§’â€ï¼Œå› æ­¤æ•ˆçŽ‡å¾ˆé«˜ã€‚

    ä¾‹å¦‚ï¼š

    å¯¹äºŽä¸€ä¸ª 3 ç»´å¼ é‡ (C, H, W)ï¼š

    * dim=0 -> æ–°å½¢çŠ¶ä¸º (1, C, H, W)

    * dim=1 -> æ–°å½¢çŠ¶ä¸º (C, 1, H, W)

    * dim=-1 -> æ–°å½¢çŠ¶ä¸º (C, H, W, 1)

    * dim=-2 -> æ–°å½¢çŠ¶ä¸º (C, H, 1, W)

* torch ä¸­çš„`@`

    Python ä¸­çš„çŸ©é˜µä¹˜æ³•è¿ç®—ç¬¦ï¼ŒA @ B ç­‰ä»·äºŽ torch.matmul(A, B)ã€‚

    PyTorch é€šè¿‡å®žçŽ° Python çš„ç‰¹æ®Šæ–¹æ³•æ¥è‡ªå®šä¹‰è¿ç®—ç¬¦è¡Œä¸ºï¼š

    | è¿ç®—ç¬¦ | Python ç‰¹æ®Šæ–¹æ³• |
    | - | - |
    | `@` | `__matmul__`, `__rmatmul__` |
    | `+` | `__add__` |
    | `-` | `__sub__` |
    | `*` | `__mul__` |	
    | `/` | `__truediv__` |

* ä½¿ç”¨`random_()`å¯ä»¥å°†æ•°æ®åˆå§‹åŒ–ä¸ºéšæœºå€¼

    example:

    `target = torch.empty(2, dtype=t.long).random_(4)`

    åˆ›å»ºä¸€ä¸ª shape ä¸º`(2, )`çš„æ•°ç»„ï¼Œå°†å…¶æ•°æ®åˆå§‹åŒ–ä¸º`[0, 4)`çš„éšæœºå€¼ã€‚

* `np.pad()`

    np.pad() æ˜¯ NumPy ä¸­ç”¨äºŽæ•°ç»„å¡«å……ï¼ˆpaddingï¼‰çš„å‡½æ•°ï¼Œä¸»è¦ç”¨äºŽåœ¨æ•°ç»„è¾¹ç•Œæ‰©å±•æŒ‡å®šå®½åº¦çš„å…ƒç´ ã€‚

    syntax:

    ```py
    numpy.pad(array, pad_width, mode='constant', **kwargs)
    ```

    ä¸»è¦å‚æ•°ï¼š

    * arrayï¼šè¦å¡«å……çš„æ•°ç»„

    * pad_widthï¼šå¡«å……å®½åº¦ï¼Œæ ¼å¼ä¸º ((before_1, after_1), ..., (before_N, after_N))

        å¯¹äºŽå¤šç»´æ•°ç»„ï¼Œpad_width çš„æ¯ä¸ªå…ƒç»„å¯¹åº”ä¸€ä¸ªç»´åº¦

    * modeï¼šå¡«å……æ¨¡å¼ï¼Œé»˜è®¤ä¸º 'constant'

    * constant_valuesï¼šå½“æ¨¡å¼ä¸º 'constant' æ—¶ä½¿ç”¨çš„å¡«å……å€¼

    å¸¸ç”¨å¡«å……æ¨¡å¼

    * constantï¼šå¸¸æ•°å¡«å……ï¼ˆé»˜è®¤ï¼‰

    * edgeï¼šä½¿ç”¨è¾¹ç¼˜å€¼å¡«å……

    * linear_rampï¼šçº¿æ€§æ–œå¡å¡«å……

    * maximum/minimumï¼šä½¿ç”¨æ•°ç»„æœ€å¤§/æœ€å°å€¼å¡«å……

    * meanï¼šä½¿ç”¨æ•°ç»„å¹³å‡å€¼å¡«å……

    * medianï¼šä½¿ç”¨ä¸­ä½æ•°å¡«å……

    * reflect/symmetricï¼šåå°„/å¯¹ç§°å¡«å……

    example:

    ```py
    import numpy as np

    # 1. ä¸€ç»´æ•°ç»„å¸¸æ•°å¡«å……
    arr_1d = np.array([1, 2, 3])
    padded = np.pad(arr_1d, (2, 3), mode='constant', constant_values=0)
    # ç»“æžœï¼š[0 0 1 2 3 0 0 0]

    # 2. äºŒç»´æ•°ç»„ä¸åŒæ–¹å‘å¡«å……
    arr_2d = np.array([[1, 2], [3, 4]])
    # ä¸Šä¸‹å„å¡«å……1è¡Œï¼Œå·¦å³å„å¡«å……2åˆ—
    padded = np.pad(arr_2d, ((1, 1), (2, 2)), mode='constant', constant_values=0)

    # 3. ä½¿ç”¨ä¸åŒå¡«å……å€¼
    arr = np.array([1, 2, 3])
    # å·¦ä¾§å¡«å……5ï¼Œå³ä¾§å¡«å……10
    padded = np.pad(arr, (2, 3), mode='constant', constant_values=(5, 10))

    # 4. è¾¹ç¼˜å€¼å¡«å……
    arr = np.array([1, 2, 3, 4])
    padded = np.pad(arr, (2, 2), mode='edge')
    # ç»“æžœï¼š[1 1 1 2 3 4 4 4]

    # 5. åå°„å¡«å……
    arr = np.array([1, 2, 3, 4])
    padded = np.pad(arr, (2, 2), mode='reflect')
    # ç»“æžœï¼š[3 2 1 2 3 4 3 2]

    # 6. ä¸åŒç»´åº¦ä¸åŒå¡«å……å®½åº¦
    arr_2d = np.ones((3, 3))
    pad_width = ((1, 2), (3, 4))  # ç¬¬ä¸€ç»´ï¼šä¸Š1è¡Œä¸‹2è¡Œï¼Œç¬¬äºŒç»´ï¼šå·¦3åˆ—å³4åˆ—
    padded = np.pad(arr_2d, pad_width, mode='constant', constant_values=0)
    ```

    æ³¨ï¼š

    1. å¦‚æžœ`pad_width`åªå†™ä¸€ä»½ï¼Œé‚£ä¹ˆä¼šåœ¨æœ€å¤–å±‚ç»´åº¦ä¸Šè¢«å¹¿æ’­

        example:

        ```py
        import numpy as np

        arr = np.ones((3, 2))
        arr = np.pad(arr, (2, 3))
        print(arr)
        ```

        output:

        ```
        [[0. 0. 0. 0. 0. 0. 0.]
        [0. 0. 0. 0. 0. 0. 0.]
        [0. 0. 1. 1. 0. 0. 0.]
        [0. 0. 1. 1. 0. 0. 0.]
        [0. 0. 1. 1. 0. 0. 0.]
        [0. 0. 0. 0. 0. 0. 0.]
        [0. 0. 0. 0. 0. 0. 0.]
        [0. 0. 0. 0. 0. 0. 0.]]
        ```

        å¯ä»¥çœ‹åˆ°ï¼Œåœ¨è¡Œä¸Šæ˜¯å‰é¢æ·»åŠ ä¸¤è¡Œï¼ŒåŽé¢æ·»åŠ ä¸‰è¡Œï¼›åœ¨åˆ—ä¸Šæ˜¯å·¦è¾¹æ·»åŠ ä¸¤åˆ—ï¼Œå³è¾¹æ·»åŠ ä¸‰åˆ—ã€‚

        ç›¸å½“äºŽæŠŠ`(2, 3)`å¹¿æ’­æˆäº†`((2, 3), (2, 3))`ã€‚

        å¦‚æžœå¸Œæœ›è¢«å¹¿æ’­æˆ`((2, 2), (3, 3))`ï¼Œé‚£ä¹ˆå¯ä»¥å†™æˆ`((2, ), (3, ))`

    1. å¯¹äºŽ`constant_value`ï¼ŒåŽ padding çš„ä¼šè¦†ç›–å…ˆ padding çš„

        ```py
        import numpy as np

        arr = np.ones((3, 2))
        arr = np.pad(arr, ((2, ), (3, )), constant_values=((2, ), (3, )))
        print(arr)
        ```

        output:

        ```
        [[3. 3. 3. 2. 2. 3. 3. 3.]
        [3. 3. 3. 2. 2. 3. 3. 3.]
        [3. 3. 3. 1. 1. 3. 3. 3.]
        [3. 3. 3. 1. 1. 3. 3. 3.]
        [3. 3. 3. 1. 1. 3. 3. 3.]
        [3. 3. 3. 2. 2. 3. 3. 3.]
        [3. 3. 3. 2. 2. 3. 3. 3.]]
        ```

* tensor çš„ indexing, slicing, reshaping æ“ä½œ

    ```py
    import torch

    tensor = torch.tensor([[1, 2], [3, 4], [5, 6]])

    element = tensor[1, 0]
    print(f"Indexed Element (Row 1, Column 0): {element}")
    
    slice_tensor = tensor[:2, :]
    print(f"Sliced Tensor (First two rows): \n{slice_tensor}")

    reshaped_tensor = tensor.view(2, 3)
    print(f"Reshaped Tensor (2x3): \n{reshaped_tensor}")
    ```

    output:

    ```
    Indexed Element (Row 1, Column 0): 3
    Sliced Tensor (First two rows): 
    tensor([[1, 2],
            [3, 4]])
    Reshaped Tensor (2x3): 
    tensor([[1, 2, 3],
            [4, 5, 6]])
    ```

* tensor çš„ Broadcasting å’Œ Matrix Multiplication æ“ä½œ

    ```py
    import torch

    tensor_a = torch.tensor([[1, 2, 3], [4, 5, 6]])
    print('tensor a shape: {}'.format(tensor_a.shape))

    tensor_b = torch.tensor([[10, 20, 30]]) 
    print('tensor b shape: {}'.format(tensor_b.shape))

    broadcasted_result = tensor_a + tensor_b 
    print(f"Broadcasted Addition Result: \n{broadcasted_result}")

    matrix_multiplication_result = torch.matmul(tensor_a, tensor_a.T)
    print(f"Matrix Multiplication Result (tensor_a * tensor_a^T): \n{matrix_multiplication_result}")
    ```

    output:

    ```
    tensor a shape: torch.Size([2, 3])
    tensor b shape: torch.Size([1, 3])
    Broadcasted Addition Result: 
    tensor([[11, 22, 33],
            [14, 25, 36]])
    Matrix Multiplication Result (tensor_a * tensor_a^T): 
    tensor([[14, 32],
            [32, 77]])
    ```

* `index_add()`

    It is the out-of place version of the function `index_add_()`.

    example:

    ```py
    import torch

    y = torch.ones(5,5)
    index2 = torch.tensor([0,1,1,1,2])
    ten = torch.randn(5,5)

    print("Indexed Matrix:\n",y.index_add(1,index2,ten))
    print ("Printing Indexed Matrix again:\n",y)
    ```

    output:

    ```
    Indexed Matrix:
     tensor([[ 1.1614,  2.1703,  1.5247,  1.0000,  1.0000],
            [-0.2930,  4.1282,  0.3124,  1.0000,  1.0000],
            [ 0.5624,  0.3906,  3.0302,  1.0000,  1.0000],
            [ 1.7235,  2.3990,  2.5070,  1.0000,  1.0000],
            [ 1.9170,  1.0716, -0.3112,  1.0000,  1.0000]])
    Printing Indexed Matrix again:
     tensor([[1., 1., 1., 1., 1.],
            [1., 1., 1., 1., 1.],
            [1., 1., 1., 1., 1.],
            [1., 1., 1., 1., 1.],
            [1., 1., 1., 1., 1.]])
    ```

    å¯ä»¥çœ‹å‡º`index_add()`ä¸ä¿®æ”¹åŽŸ tensor çš„æ•°æ®ã€‚

* Index-based Operation

    * `index_add_()`

        Adds the given tensor elements to the self tensor along the order given in the matrix.

        syntax:

        ```py
        index_add_(dim, index, tensor) ---> Tensor
        ```

        params:

        * dim: dimension along which index to add. '0' stands for column and '1' stands for row.

        * index: indices of the tensor to select from. It can be LongTensor or IntTensor.

        * tensor: tensor containing the values to add.

        example:

        ```py
        import torch

        x = torch.zeros(5,5)
        te = torch.tensor([[1,3,5,7,9], [1,3,5,7,9], [1,3,5,7,9]], dtype=torch.float32)
        print('te shape: {}\n'.format(te.shape))
        index0 = torch.tensor([0, 2, 4])

        x.index_add_(0, index0, te) #adding tensor te to x along row of the given order
        print('x:\n{}'.format(x))
        ```

        output:

        ```
        te shape: torch.Size([3, 5])

        x:
        tensor([[1., 3., 5., 7., 9.],
                [0., 0., 0., 0., 0.],
                [1., 3., 5., 7., 9.],
                [0., 0., 0., 0., 0.],
                [1., 3., 5., 7., 9.]])
        ```

        å¯ä»¥çœ‹å‡ºï¼Œæ˜¯è®©`te`ä¸­çš„ä¸‰è¡Œæ•°æ®åˆ†åˆ«å åŠ åˆ°`x`çš„`[0, 2, 4]`è¡Œä¸Šã€‚

        example 2:

        ```py
        import torch

        y = torch.ones(5, 5) # unit vector
        index2 = torch.tensor([0, 1, 1, 1, 2])
        ten = torch.randn(1, 5)

        # adding values to y along the column with given order
        y.index_add_(1, index2, ten)
        print('y is: {}'.format(y))
        ```

        output:

        ```
        Traceback (most recent call last):
          File "/home/hlc/Documents/Projects/torch_test/main.py", line 8, in <module>
            y.index_add_(1, index2, ten)
        RuntimeError: source tensor shape must match self tensor shape, excluding the specified dimension. Got self.shape = [5, 5] source.shape = [1, 5]
        ```

        å¯ä»¥çœ‹å‡ºå¹¶æ²¡æœ‰å‘ç”Ÿ broadcastingã€‚

        å¯ä»¥æ”¹æˆè¿™æ ·ï¼š

        ```py
        import torch

        y = torch.ones(5,5) # unit vector
        index2 = torch.tensor([0, 1, 1, 1, 2])
        ten = torch.randn(1, 5)
        ten = ten.expand(5, 5)
        print('ten is: {}'.format(ten))

        # adding values to y along the column with given order
        y.index_add_(1, index2, ten)
        print('y is: {}'.format(y))
        ```

        output:

        ```
        ten is: tensor([[ 0.1083, -0.3369, -0.7591, -0.2532, -0.4060],
                [ 0.1083, -0.3369, -0.7591, -0.2532, -0.4060],
                [ 0.1083, -0.3369, -0.7591, -0.2532, -0.4060],
                [ 0.1083, -0.3369, -0.7591, -0.2532, -0.4060],
                [ 0.1083, -0.3369, -0.7591, -0.2532, -0.4060]])
        y is: tensor([[ 1.1083, -0.3493,  0.5940,  1.0000,  1.0000],
                [ 1.1083, -0.3493,  0.5940,  1.0000,  1.0000],
                [ 1.1083, -0.3493,  0.5940,  1.0000,  1.0000],
                [ 1.1083, -0.3493,  0.5940,  1.0000,  1.0000],
                [ 1.1083, -0.3493,  0.5940,  1.0000,  1.0000]])
        ```

        å¯ä»¥çœ‹å‡ºï¼Œ`[0, 1, 1, 1, 2]`è¡¨ç¤ºå°†`ten`ä¸­çš„äº”åˆ—åˆ†åˆ«å åŠ åˆ°`y`çš„ç¬¬ 0, 1, 1, 1, 2 åˆ—ã€‚

* Tensor ä¸­çš„è½¬ç½®ï¼ˆTransposeï¼‰

    è½¬ç½®æ˜¯ä¸€ç§æ”¹å˜å¼ é‡ç»´åº¦ï¼ˆè½´ï¼‰é¡ºåºçš„æ“ä½œã€‚

    çŸ©é˜µï¼ˆä¸€ä¸ª 2D å¼ é‡ï¼‰ï¼Œå®ƒçš„è½¬ç½®å°±æ˜¯æ²¿ç€ä¸»å¯¹è§’çº¿ç¿»è½¬çš„æ“ä½œã€‚å°†çŸ©é˜µ A çš„è¡Œå’Œåˆ—äº’æ¢ï¼Œå°±å¾—åˆ°äº†å®ƒçš„è½¬ç½® Aáµ€ã€‚

    å¦‚æžœåŽŸçŸ©é˜µ A çš„å½¢çŠ¶æ˜¯ (m, n)ï¼Œé‚£ä¹ˆè½¬ç½®åŽçš„çŸ©é˜µ Aáµ€ çš„å½¢çŠ¶å°±æ˜¯ (n, m)ã€‚

    å…ƒç´ çš„ä½ç½®å…³ç³»ä¸ºï¼šA[i, j] = Aáµ€[j, i]ã€‚

    å¯¹äºŽç»´åº¦å¤§äºŽ 2 çš„å¼ é‡ï¼ˆä¾‹å¦‚ 3Dã€4Dï¼‰ï¼Œè½¬ç½®æŒ‡ä»»æ„åœ°é‡æ–°æŽ’åˆ—å¼ é‡çš„æ‰€æœ‰ç»´åº¦ã€‚

    PyTorch ä¸­è½¬ç½®æ“ä½œæ˜¯ä¸€ç§â€œè§†å›¾æ“ä½œâ€ï¼Œç”±äºŽä¸å¤åˆ¶æ•°æ®ï¼ŒåŽŸå¼ é‡å’Œè½¬ç½®åŽçš„å¼ é‡å…±äº«åŒä¸€å—å†…å­˜ã€‚ä¿®æ”¹å…¶ä¸­ä¸€ä¸ªçš„å€¼ï¼Œå¦ä¸€ä¸ªä¹Ÿä¼šéšä¹‹æ”¹å˜ã€‚

    1. é»˜è®¤è½¬ç½®ï¼ˆ`.T` æˆ– `transpose()`ï¼‰

        åœ¨å¾ˆå¤šæ¡†æž¶ä¸­ï¼Œå¦‚æžœä¸æä¾›å‚æ•°ï¼Œ.T å±žæ€§ä¼šé»˜è®¤åè½¬æ‰€æœ‰ç»´åº¦çš„é¡ºåºã€‚

        `y = x.T`

        æ–°çš„ç»´åº¦é¡ºåºæ˜¯åŽŸé¡ºåºçš„åè½¬ï¼š`(2, 1, 0)`

        å› æ­¤ï¼Œè½¬ç½®åŽçš„å½¢çŠ¶ä¸ºï¼š`(original_shape[2], original_shape[1], original_shape[0]) = (4, 3, 2)`

    2. è‡ªå®šä¹‰è½¬ç½®ï¼ˆæŒ‡å®š perm å‚æ•°ï¼‰

        * example 1: äº¤æ¢æœ€åŽä¸¤ä¸ªç»´åº¦

            ```py
            # å‡è®¾ x.shape = (2, 3, 4)
            y = x.transpose(0, 2, 1) # æˆ–è€… x.permute(0, 2, 1) in PyTorch
            # æ–°çš„ç»´åº¦é¡ºåº: (0, 2, 1)
            # æ–°å½¢çŠ¶: (original_shape[0], original_shape[2], original_shape[1])
            #        = (2, 4, 3)
            ```

        * example 2: å¤æ‚çš„é‡æ–°æŽ’åˆ—

            ```py
            # å‡è®¾ x.shape = (2, 3, 4, 5)
            # æˆ‘ä»¬æƒ³è¦ä¸€ä¸ªæ–°çš„é¡ºåºï¼šå°†åŽŸæ¥çš„ç»´åº¦ 2 æ”¾åˆ°æœ€å‰é¢ï¼Œç„¶åŽæ˜¯ç»´åº¦ 0ï¼Œç»´åº¦ 3ï¼Œæœ€åŽæ˜¯ç»´åº¦ 1ã€‚
            perm = (2, 0, 3, 1)
            y = x.transpose(perm)
            # æ–°å½¢çŠ¶: (original_shape[2], original_shape[0], original_shape[3], original_shape[1])
            #        = (4, 2, 5, 3)
            ```

    numpy ä¸Ž torch çš„æŽ¥å£å‡½æ•°ï¼š

    * numpy

        ```py
        import numpy as np
        x = np.random.rand(2, 3, 4)
        y = x.transpose(0, 2, 1) # ä½¿ç”¨ transpose å‡½æ•°
        z = x.T # åè½¬æ‰€æœ‰ç»´åº¦
        ```

    * torch

        ```py
        import torch
        x = torch.randn(2, 3, 4)
        y = x.permute(0, 2, 1) # å¸¸ç”¨ permute å‡½æ•°
        z = x.transpose(1, 2)  # transpose é€šå¸¸ä¸€æ¬¡åªäº¤æ¢ä¸¤ä¸ªæŒ‡å®šç»´åº¦ï¼Œè¿™é‡Œæ˜¯äº¤æ¢ç»´åº¦1å’Œ2
        w = x.T # åè½¬æ‰€æœ‰ç»´åº¦
        ```

### æ•°æ®å¢žå¹¿ï¼Œæ•°æ®é¢„å¤„ç†

* torchvision.transforms ä¸­å¸¸ç”¨çš„ augmentation æ–¹æ³•ï¼š

    * å›¾åƒé¢„å¤„ç† & åŸºæœ¬å˜æ¢

        ```py
        # Resizeï¼šè°ƒæ•´å›¾åƒå°ºå¯¸
        transforms.Resize((256, 256))

        # CenterCrop / RandomCropï¼šä¸­å¿ƒ/éšæœºè£å‰ª
        transforms.RandomCrop(224)

        # Padï¼šè¾¹ç¼˜å¡«å……
        transforms.Pad(50, fill=255)
        ```

    * é¢œè‰² & äº®åº¦å˜æ¢

        ```py
        # ColorJitterï¼šéšæœºè°ƒæ•´äº®åº¦ã€å¯¹æ¯”åº¦ã€é¥±å’Œåº¦å’Œè‰²è°ƒ
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)

        # Grayscale / RandomGrayscaleï¼šè½¬ç°åº¦å›¾
        transforms.RandomGrayscale(p=0.1)

        # RandomAdjustSharpness / RandomAutocontrastï¼šè°ƒæ•´é”åº¦ã€è‡ªåŠ¨å¯¹æ¯”åº¦
        ```

    * å‡ ä½•å˜æ¢

        ```py
        # RandomHorizontalFlip / RandomVerticalFlipï¼šéšæœºæ°´å¹³/åž‚ç›´ç¿»è½¬
        transforms.RandomHorizontalFlip(p=0.5)

        # RandomRotationï¼šéšæœºæ—‹è½¬
        transforms.RandomRotation(degrees=30)

        # RandomAffineï¼šéšæœºä»¿å°„å˜æ¢ï¼ˆå¹³ç§»ã€æ—‹è½¬ã€ç¼©æ”¾ã€å‰ªåˆ‡ï¼‰
        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1))

        # RandomPerspectiveï¼šéšæœºé€è§†å˜æ¢
        ```

    * æ¨¡ç³Š & å™ªå£°

        ```py
        # GaussianBlurï¼šé«˜æ–¯æ¨¡ç³Š
        transforms.GaussianBlur(kernel_size=5)

        # RandomErasingï¼šéšæœºæ“¦é™¤ï¼ˆCutOutï¼‰
        transforms.RandomErasing(p=0.5)
        ```

    * æ ‡å‡†åŒ– & å¼ é‡è½¬æ¢

        ```py
        # ToTensorï¼šå°†PILå›¾åƒæˆ–NumPyæ•°ç»„è½¬æ¢ä¸ºå¼ é‡ï¼Œå¹¶ç¼©æ”¾åˆ° [0,1]
        transforms.ToTensor()

        # Normalizeï¼šæ ‡å‡†åŒ–ï¼ˆå‡å‡å€¼ã€é™¤æ ‡å‡†å·®ï¼‰
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ```

    * ç»„åˆå˜æ¢

        ä½¿ç”¨ Compose å°†å¤šä¸ªå˜æ¢ç»„åˆï¼š

        ```py
        transform = transforms.Compose([
            transforms.Resize(256),
            transforms.RandomCrop(224),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        ```

* `ToTensor()`

    1. æ•°æ®ç±»åž‹è½¬æ¢ï¼šToTensor() å°† PIL Image æˆ– numpy.ndarray è½¬æ¢ä¸º PyTorch Tensorï¼ŒåŽç»­çš„ transforms éƒ½éœ€è¦åœ¨ Tensor ä¸Šæ“ä½œ

    2. é€šé“é¡ºåºï¼šå°† HÃ—WÃ—C è½¬æ¢ä¸º CÃ—HÃ—Wï¼Œç¬¦åˆ PyTorch çš„æœŸæœ›æ ¼å¼

    3. æ•°å€¼èŒƒå›´ï¼šå°† [0, 255] çš„æ•´æ•°æˆ– [0, 1] çš„æµ®ç‚¹æ•°è½¬æ¢ä¸º [0.0, 1.0] çš„æµ®ç‚¹æ•°

    å˜æ¢å‰åŽæ•°æ® shape å¯¹æ¯”ï¼š

    ```py
    # å¯¹äºŽ RGB å›¾åƒ
    (H, W, C) = (224, 224, 3)
    # å˜æ¢åŽ
    (C, H, W) = (3, 224, 224)

    # å¯¹äºŽç°åº¦å›¾åƒ  
    (H, W) = (224, 224)  # æˆ– (H, W, 1)
    # å˜æ¢åŽ
    (1, H, W) = (1, 224, 224)
    ```

* `transforms.Normalize()`

    example:

    `transforms.Normalize((0.5,), (0.5,))`ä½œç”¨å¦‚ä¸‹ï¼š

    ```py
    # å¯¹äºŽæ¯ä¸ªåƒç´ å€¼ï¼š
    normalized_pixel = (pixel - mean) / std

    # å…·ä½“åˆ°ä½ çš„ä¾‹å­ï¼š
    normalized_pixel = (pixel - 0.5) / 0.5
    ```

    å¦‚æžœ RGB ä¸‰ä¸ªé€šé“çš„ mean å’Œ std ç›¸åŒï¼Œé‚£ä¹ˆå¯ä»¥å†™æˆï¼š

    ```py
    transforms.Normalize(mean, std)
    ```

    å¦‚æžœæ˜¯å¤šé€šé“å›¾åƒï¼Œé‚£ä¹ˆå¯ä»¥å†™æˆï¼š

    ```py
    # RGB å›¾åƒå½’ä¸€åŒ–
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5],  # R, G, B é€šé“çš„å‡å€¼
                             std=[0.5, 0.5, 0.5])   # R, G, B é€šé“çš„æ ‡å‡†å·®
    ])
    ```

    ä¸ºä»€ä¹ˆè¦å½’ä¸€åŒ–ï¼Ÿ

    * è®­ç»ƒç¨³å®šæ€§ï¼šå°†æ•°æ®ç¼©æ”¾åˆ°ç›¸ä¼¼çš„èŒƒå›´ï¼Œé¿å…æ¢¯åº¦çˆ†ç‚¸

    * æ”¶æ•›é€Ÿåº¦ï¼šå¸®åŠ©ä¼˜åŒ–å™¨æ›´å¿«æ”¶æ•›

    * æ¨¡åž‹æ€§èƒ½ï¼šå¾ˆå¤šæ¨¡åž‹å‡è®¾è¾“å…¥æ•°æ®æ˜¯é›¶å‡å€¼çš„

    * æ•°å€¼ç²¾åº¦ï¼šåœ¨ [-1, 1] èŒƒå›´å†…è®¡ç®—æ›´ç¨³å®š

* åœ¨ transform æ—¶ï¼Œnumpy åªèƒ½å…ˆ to tensorï¼Œå† resizeï¼Œä¸èƒ½å…ˆ resizeã€‚PIL å›¾åƒæ—¢å¯ä»¥å…ˆ resizeï¼Œä¹Ÿå¯ä»¥å…ˆ to tensor

    * numpy ndarray åªèƒ½å…ˆ to tensor;

        ```py
        from torchvision import transforms
        import numpy as np

        img = np.random.random((256, 256))

        trans = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((512, 512))
        ])

        img_trans = trans(img)

        print("img shape: {}".format(img.shape))
        print("img trans shape: {}".format(img_trans.shape))
        ```

        output:

        ```
        img shape: (256, 256)
        img trans shape: torch.Size([1, 512, 512])
        ```

        ä¸‰ç»´çš„æ•°æ®ä¹Ÿå¯ä»¥å¤„ç†ï¼š

        `img = np.random.random((256, 256, 3))`

        output:

        ```
        img shape: (256, 256, 3)
        img trans shape: torch.Size([3, 512, 512])
        ```

        å¦‚æžœæˆ‘ä»¬è®¾ç½®å…ˆ resizeï¼Œé‚£ä¹ˆä¼šæŠ¥é”™ï¼š

        ```py
        trans = transforms.Compose([
            transforms.Resize((512, 512)),
            transforms.ToTensor()
        ])
        ```

        output:

        ```
        ...
          File "/home/hlc/miniconda3/envs/torch/lib/python3.10/site-packages/torchvision/transforms/_functional_pil.py", line 31, in get_dimensions
            raise TypeError(f"Unexpected type {type(img)}")
        TypeError: Unexpected type <class 'numpy.ndarray'>
        ```

    * PIL å›¾ç‰‡æ—¢å¯ä»¥å…ˆ resizeï¼Œä¹Ÿå¯ä»¥å…ˆ to tensor:

        ```py
        from torchvision import transforms
        from PIL import Image

        img = Image.open('../example.jpg')

        trans = transforms.Compose([
            transforms.Resize((512, 512)),
            transforms.ToTensor()
        ])

        img_trans = trans(img)

        # print("img shape: {}".format(img.shape))  # PIL Image object has no shape attribute
        print("img trans shape: {}".format(img_trans.shape))
        ```

        output:

        ```
        img trans shape: torch.Size([3, 512, 512])
        ```

        å…ˆ to tensor ä¹Ÿæ˜¯å¯ä»¥çš„ï¼š

        ```py
        trans = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((512, 512))
        ])
        ```

        output:

        ```
        img trans shape: torch.Size([3, 512, 512])
        ```

    å¦‚æžœå…ˆåšäº† to tensorï¼Œé‚£ä¹ˆåŽç»­æ“ä½œä¼šåœ¨ GPU é‡Œå®Œæˆï¼ˆæ˜¯ CPU å§ï¼Ÿï¼‰ã€‚å¦‚æžœå…ˆåš resizeï¼Œé‚£ä¹ˆ resize æ“ä½œä¼šè°ƒç”¨ PIL æä¾›çš„ resize å‡½æ•°ã€‚

* PyTorch Functional Transforms for Computer Vision

    Most of the functional transforms accept both PIL images and tensor images. A tensor image is a tensor with shape (C, H, W),

    if the input is a PIL image output is also a PIL image and the same for Tensor image.

    * `adjust_brightness()`

        adjusts the brightness of an image. It accepts both PIL image and Tensor Image.

        syntax:

        ```py
        torchvision.transforms.functional.adjust_brightness(
            img: Tensor,
            brightness_factor: float
        ) -> Tensor
        ```

        * img (Tensor): è¾“å…¥å›¾åƒï¼Œå½¢çŠ¶ä¸º (..., H, W) æˆ– (C, H, W) æˆ– (H, W)

        * brightness_factor is any non-negative floating-point number:

            * brightness_factor = 1, the original image.

            * brightness_factor < 1, a darker output image.

            * brightness_factor > 1, a brighter output image.

        example:

        ```py
        import torchvision.transforms.functional as F
        import torch
        from PIL import Image

        image = Image.open('nature.jpg')

        output = F.adjust_brightness(image, brightness_factor=3.0)
        output.show()
        ```

        æ³¨æ„äº‹é¡¹ï¼š

        * äº®åº¦è°ƒæ•´æ˜¯é€šè¿‡å°†æ¯ä¸ªåƒç´ å€¼ä¹˜ä»¥ brightness_factor å®žçŽ°çš„

        * ç»“æžœä¼šè¢«è£å‰ªåˆ°å›¾åƒçš„åŽŸå§‹å€¼èŒƒå›´å†…ï¼ˆé€šå¸¸æ˜¯ [0, 1]ï¼‰

        * å¦‚æžœè¾“å…¥æ˜¯ PIL å›¾åƒï¼ŒF.adjust_brightness() çš„è¾“å‡ºæ˜¯ Tensorï¼Œè€Œä¸æ˜¯ PIL å›¾åƒã€‚

        ä¸Ž``transforms.Compose`åˆç”¨çš„ä¾‹å­ï¼š

        ```py
        transform = transforms.Compose([
            transforms.ToTensor(),  # PIL -> Tensor
            lambda x: F.adjust_brightness(x, brightness_factor=1.5),
            transforms.ToPILImage()  # Tensor -> PIL
        ])
        ```

* image augmentation

    ```py
    import torchvision.transforms as transforms
    from PIL import Image

    image = Image.open('example.jpg')

    transform = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor()
    ])

    augmented_image = transform(image)
    print("Augmented Image Shape:", augmented_image.shape)
    ```

    output:

    ```
    Augmented Image Shape: torch.Size([3, 500, 500])
    ```

### å¯è§†åŒ– visualization

* åœ¨`fig, axes = subplots()`æ—¶ï¼Œå¦‚æžœæ˜¯ä¸€è¡Œæˆ–è€…ä¸€åˆ—ï¼Œé‚£ä¹ˆ`axes`æ˜¯ä¸€ç»´çš„ï¼Œå¦‚æžœæ˜¯å¤šè¡Œå¤šåˆ—ï¼Œ`axes`æ˜¯äºŒç»´çš„ã€‚

* matplotlib ç”» 3d surface çš„ example

    ```py
    import numpy as np
    import matplotlib.pyplot as plt
    from mpl_toolkits.mplot3d import Axes3D
    import matplotlib.font_manager as fm

    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['Noto Sans CJK JP']  # ä½¿ç”¨é»‘ä½“
    plt.rcParams['axes.unicode_minus'] = False    # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

    # åˆ›å»ºæ•°æ®
    x = np.linspace(-5, 5, 100)
    y = np.linspace(-5, 5, 100)
    X, Y = np.meshgrid(x, y)
    Z = np.sin(np.sqrt(X**2 + Y**2))

    # åˆ›å»ºå›¾å½¢
    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')

    # ç»˜åˆ¶æ›²é¢
    surf = ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)

    # æ·»åŠ é¢œè‰²æ¡
    fig.colorbar(surf)

    # è®¾ç½®æ ‡ç­¾ - çŽ°åœ¨ä¸­æ–‡å¯ä»¥æ­£å¸¸æ˜¾ç¤º
    ax.set_xlabel('Xè½´')
    ax.set_ylabel('Yè½´')
    ax.set_zlabel('Zè½´')
    ax.set_title('3Dæ›²é¢å›¾ç¤ºä¾‹')

    plt.show()
    ```

* Axes3D

    æ¨¡å—ï¼š`mpl_toolkits.mplot3d`

    åŸºæœ¬åŠŸèƒ½ routineï¼š

    1. åˆ›å»ºä¸‰ç»´åæ ‡è½´

        ä½¿ç”¨ projection='3d' å‚æ•°å°†ä¸€ä¸ªæ™®é€šçš„äºŒç»´åæ ‡è½´è½¬æ¢ä¸ºä¸‰ç»´åæ ‡è½´ã€‚

        ```py
        import matplotlib.pyplot as plt
        from mpl_toolkits.mplot3d import Axes3D  # è™½ç„¶æ˜¾å¼å¯¼å…¥æœ‰æ—¶ä¸éœ€è¦ï¼Œä½†å»ºè®®ä¿ç•™ä»¥ç¡®ä¿çŽ¯å¢ƒæ­£å¸¸

        # åˆ›å»ºå›¾å½¢å’Œä¸‰ç»´åæ ‡è½´
        fig = plt.figure()
        ax = fig.add_subplot(111, projection='3d')  # 111 è¡¨ç¤º 1x1 ç½‘æ ¼çš„ç¬¬1ä¸ªå­å›¾

        # åœ¨è¾ƒæ–°çš„ Matplotlib ç‰ˆæœ¬ä¸­ï¼Œä¹Ÿå¯ä»¥è¿™æ ·åˆ›å»ºï¼š
        # fig, ax = plt.subplots(subplot_kw={'projection': '3d'})
        ```

    2. åŸºæœ¬ä¸‰ç»´ç»˜å›¾æ–¹æ³•

        åˆ›å»ºäº† Axes3D å¯¹è±¡ï¼ˆé€šå¸¸å‘½åä¸º axï¼‰åŽï¼Œä½ å¯ä»¥ä½¿ç”¨ç±»ä¼¼äºŒç»´ç»˜å›¾çš„æ–¹æ³•ï¼Œä½†å®ƒä»¬æŽ¥å—ä¸‰ç»´åæ ‡ï¼ˆx, y, zï¼‰ä½œä¸ºå‚æ•°ã€‚

        * ä¸‰ç»´æ•£ç‚¹å›¾ (Scatter Plot)

            ä½¿ç”¨ `.scatter(xs, ys, zs)` æ–¹æ³•ã€‚

            ```py
            import numpy as np

            # ç”Ÿæˆéšæœºæ•°æ®
            n = 100
            x = np.random.rand(n)
            y = np.random.rand(n)
            z = np.random.rand(n)

            ax.scatter(x, y, z, c=z, cmap='viridis', marker='o') # c=z è¡¨ç¤ºç”¨ z å€¼æ˜ å°„é¢œè‰²
            ax.set_xlabel('X Label')
            ax.set_ylabel('Y Label')
            ax.set_zlabel('Z Label')
            plt.show()
            ```

        * ä¸‰ç»´çº¿å›¾ (Line Plot)

            ä½¿ç”¨ .plot(xs, ys, zs) æ–¹æ³•ã€‚

            ```py
            # ç”Ÿæˆèžºæ—‹çº¿æ•°æ®
            theta = np.linspace(-4 * np.pi, 4 * np.pi, 100)
            z = np.linspace(-2, 2, 100)
            r = z**2 + 1
            x = r * np.sin(theta)
            y = r * np.cos(theta)

            ax.plot(x, y, z, label='3D Curve', linewidth=2)
            ax.legend()
            plt.show()
            ```

        * ä¸‰ç»´æ›²é¢å›¾ (Surface Plot)

            ä½¿ç”¨ .plot_surface(X, Y, Z) æ–¹æ³•ã€‚æ³¨æ„ï¼š X, Y, Z å¿…é¡»æ˜¯äºŒç»´ç½‘æ ¼æ•°æ®ã€‚

            ```py
            # åˆ›å»ºç½‘æ ¼æ•°æ®
            x = np.linspace(-5, 5, 50)
            y = np.linspace(-5, 5, 50)
            X, Y = np.meshgrid(x, y)
            Z = np.sin(np.sqrt(X**2 + Y**2))  # è®¡ç®—æ¯ä¸ªç½‘æ ¼ç‚¹ä¸Šçš„ Z å€¼ï¼ˆä¸€ä¸ªæ›²é¢ï¼‰

            # ç»˜åˆ¶æ›²é¢
            surf = ax.plot_surface(X, Y, Z, cmap='coolwarm', alpha=0.8)

            # æ·»åŠ é¢œè‰²æ¡
            fig.colorbar(surf, ax=ax, shrink=0.5)
            plt.show()
            ```

        * ä¸‰ç»´çº¿æ¡†å›¾ (Wireframe Plot)

            ä½¿ç”¨ .plot_wireframe(X, Y, Z) æ–¹æ³•ï¼Œç±»ä¼¼äºŽæ›²é¢å›¾ä½†åªæ˜¾ç¤ºç½‘æ ¼çº¿ã€‚

            ```py
            ax.plot_wireframe(X, Y, Z, color='black', linewidth=0.5)
            plt.show()
            ```

        * ä¸‰ç»´æŸ±çŠ¶å›¾ (Bar Plot)

            ä½¿ç”¨ .bar3d(x, y, z, dx, dy, dz) æ–¹æ³•ã€‚

            * x, y, z: æŸ±å­çš„åº•éƒ¨åæ ‡ã€‚

            * dx, dy, dz: æŸ±å­åœ¨ x, y, z æ–¹å‘ä¸Šçš„é•¿åº¦ï¼ˆå®½åº¦ã€æ·±åº¦ã€é«˜åº¦ï¼‰ã€‚

            ```py
            # å®šä¹‰æŸ±å­çš„ä½ç½®å’Œå¤§å°
            x_pos = [0, 1, 2]
            y_pos = [0, 1, 2]
            z_pos = np.zeros(3)  # æ‰€æœ‰æŸ±å­ä»Ž z=0 å¼€å§‹

            dx = dy = 0.5 * np.ones(3)  # æ‰€æœ‰æŸ±å­çš„å®½åº¦å’Œæ·±åº¦éƒ½æ˜¯ 0.5
            dz = [1, 2, 3]              # ä¸‰ä¸ªæŸ±å­çš„é«˜åº¦åˆ†åˆ«ä¸º 1, 2, 3

            ax.bar3d(x_pos, y_pos, z_pos, dx, dy, dz, color=['r', 'g', 'b'], alpha=0.7)
            plt.show()
            ```

    3. è‡ªå®šä¹‰è§†å›¾

        è°ƒæ•´ä¸‰ç»´å›¾å½¢çš„è§†è§’ï¼š

        ```py
        # è®¾ç½®è§†è§’ (ä»°è§’, æ–¹ä½è§’)
        ax.view_init(elev=30,  azim=45)  # elev: ä»°è§’ï¼ˆä¸Šä¸‹çœ‹ï¼‰, azim: æ–¹ä½è§’ï¼ˆå·¦å³è½¬ï¼‰

        # è®¾ç½®åæ ‡è½´æ¯”ä¾‹ï¼ˆä½¿å…¶ç­‰æ¯”ä¾‹æ˜¾ç¤ºï¼Œé¿å…å›¾å½¢æ‰­æ›²ï¼‰
        ax.set_box_aspect([1, 1, 1])  # [x, y, z] æ–¹å‘çš„æ¯”ä¾‹
        ```

    example:

    ```py
    import matplotlib.pyplot as plt
    import numpy as np

    # 1. åˆ›å»ºå›¾å½¢å’Œä¸‰ç»´åæ ‡è½´
    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')

    # 2. ç”Ÿæˆå¹¶ç»˜åˆ¶æ•°æ®ï¼ˆä¸€ä¸ªæ›²é¢å’Œä¸€æ¡æ›²çº¿ï¼‰
    # æ›²é¢æ•°æ®
    x = np.linspace(-5, 5, 50)
    y = np.linspace(-5, 5, 50)
    X, Y = np.meshgrid(x, y)
    Z_surf = np.sin(np.sqrt(X**2 + Y**2))
    ax.plot_surface(X, Y, Z_surf, cmap='viridis', alpha=0.7)

    # æ›²çº¿æ•°æ®ï¼ˆä¸€æ¡èžºæ—‹çº¿ï¼‰
    theta = np.linspace(0, 6*np.pi, 100)
    z_line = np.linspace(0, 2, 100)
    x_line = np.cos(theta)
    y_line = np.sin(theta)
    ax.plot(x_line, y_line, z_line, 'r-', linewidth=3, label='Spiral')

    # 3. è®¾ç½®æ ‡ç­¾ã€æ ‡é¢˜å’Œå›¾ä¾‹
    ax.set_xlabel('X Axis')
    ax.set_ylabel('Y Axis')
    ax.set_zlabel('Z Axis')
    ax.set_title('3D Surface and Line Plot')
    ax.legend()

    # 4. è°ƒæ•´è§†è§’
    ax.view_init(elev=20, azim=35)

    plt.tight_layout()
    plt.show()
    ```

    Axes3D çš„åŸºæœ¬ç”¨æ³•å¯ä»¥æ¦‚æ‹¬ä¸ºï¼š

    1. åˆ›å»ºï¼šé€šè¿‡ fig.add_subplot(projection='3d') åˆ›å»ºã€‚

    2. ç»˜å›¾ï¼šä½¿ç”¨ä¸ŽäºŒç»´ç»˜å›¾ç±»ä¼¼çš„æ–¹æ³•ï¼ˆå¦‚ plot, scatterï¼‰ï¼Œä½†ä¼ å…¥ä¸‰ä¸ªåæ ‡å‚æ•°ï¼ˆx, y, zï¼‰ã€‚å¯¹äºŽæ›²é¢å’Œçº¿æ¡†å›¾ï¼Œéœ€è¦äºŒç»´ç½‘æ ¼æ•°æ®ã€‚

    3. å®šåˆ¶ï¼šä½¿ç”¨ set_xlabel, view_init ç­‰æ–¹æ³•å®šåˆ¶åæ ‡è½´å’Œè§†å›¾ã€‚

    4. æ˜¾ç¤ºï¼šæœ€åŽç”¨ plt.show() æ˜¾ç¤ºå›¾å½¢ã€‚

### ç¨€ç–çŸ©é˜µ

* ç¨€ç–çŸ©é˜µä¹˜æ³•

    åŠ é€Ÿç®—æ³•ç®€è¿°ï¼ˆä»¥ CSR x CSC ä¸ºä¾‹ï¼‰ï¼š

    1. å¤–å±‚å¾ªçŽ¯ï¼šéåŽ†çŸ©é˜µAçš„æ¯ä¸€è¡Œ iï¼ˆåˆ©ç”¨CSRçš„ row_ptrï¼‰ã€‚

    2. ä¸­å±‚å¾ªçŽ¯ï¼šå¯¹äºŽAçš„ç¬¬ i è¡Œï¼ŒéåŽ†è¯¥è¡Œçš„æ¯ä¸€ä¸ªéžé›¶å…ƒç´  A(i,k)ï¼ˆåˆ©ç”¨CSRçš„ col_indices å’Œ valuesï¼‰ã€‚è¿™ä¸ª k æ˜¯Açš„åˆ—å·ï¼ŒåŒæ—¶ä¹Ÿæ˜¯Bçš„è¡Œå·ã€‚

    3. å†…å±‚å¾ªçŽ¯ï¼šå¯¹äºŽæ¯ä¸ª kï¼Œæ‰¾åˆ°çŸ©é˜µBçš„ç¬¬ k è¡Œï¼ˆå³CSCæ ¼å¼ä¸‹çš„ç¬¬ k åˆ—ï¼‰ã€‚éåŽ†Bçš„ç¬¬ k è¡Œä¸Šçš„æ¯ä¸€ä¸ªéžé›¶å…ƒç´  B(k,j)ï¼ˆåˆ©ç”¨CSCçš„ row_indices å’Œ valuesï¼‰ã€‚

    4. ç´¯åŠ ï¼šå°†ä¹˜ç§¯ A(i,k) * B(k,j) ç´¯åŠ åˆ°ç»“æžœçŸ©é˜µ C(i,j) ä¸Šã€‚

    æˆ‘ä»¬åªå¤„ç†é‚£äº›å¯èƒ½äº§ç”Ÿéžé›¶ç»“æžœçš„è®¡ç®—ã€‚

* `scipy.sparse.csr_matrix`

    Compressed Sparse Row matrix

    æ˜¯ SciPy åº“ä¸­ç”¨äºŽè¡¨ç¤ºç¨€ç–çŸ©é˜µçš„ä¸€ç§æ•°æ®ç»“æž„ã€‚å®ƒä¸“é—¨ç”¨äºŽé«˜æ•ˆåœ°å­˜å‚¨å’Œæ“ä½œé‚£äº›å¤§éƒ¨åˆ†å…ƒç´ ä¸ºé›¶çš„çŸ©é˜µã€‚

    CSR æ ¼å¼åªå­˜å‚¨éžé›¶å…ƒç´ çš„å€¼åŠå…¶ä½ç½®ï¼Œæžå¤§åœ°èŠ‚çœäº†å†…å­˜å’Œè®¡ç®—æ—¶é—´ã€‚

    é€‚ç”¨åœºæ™¯:

    * è¯è¢‹æ¨¡åž‹ï¼ˆBag-of-Wordsï¼‰ä¸­çš„æ–‡æ¡£-è¯é¡¹çŸ©é˜µ

    * å›¾çš„é‚»æŽ¥çŸ©é˜µ

    * æœ‰é™å…ƒåˆ†æžä¸­çš„åˆšåº¦çŸ©é˜µ

    CSR æ ¼å¼é€šè¿‡ä¸‰ä¸ªä¸€ç»´æ•°ç»„æ¥è¡¨ç¤ºæ•´ä¸ªçŸ©é˜µï¼š

    1. dataï¼šå­˜å‚¨æ‰€æœ‰éžé›¶å…ƒç´ çš„å€¼ã€‚

    2. indicesï¼šå­˜å‚¨æ¯ä¸ªéžé›¶å…ƒç´ æ‰€åœ¨çš„åˆ—ç´¢å¼•ã€‚

    3. indptrï¼ˆç´¢å¼•æŒ‡é’ˆï¼‰ï¼šå­˜å‚¨æ¯ä¸€è¡Œç¬¬ä¸€ä¸ªéžé›¶å…ƒç´ åœ¨ data å’Œ indices ä¸­çš„èµ·å§‹ä½ç½®ã€‚

    è¿™ç§ç»“æž„ä½¿å¾—æŒ‰è¡Œè®¿é—®å’Œæ“ä½œï¼ˆå¦‚çŸ©é˜µ-å‘é‡ä¹˜æ³•ï¼‰éžå¸¸é«˜æ•ˆã€‚

* `csc_matrix`å¸¸ç”¨å±žæ€§å’Œæ“ä½œ

    * æŸ¥çœ‹çŸ©é˜µä¿¡æ¯

        ```py
        print(sparse_matrix.shape)   # çŸ©é˜µå½¢çŠ¶: (3, 4)
        print(sparse_matrix.nnz)     # éžé›¶å…ƒç´ ä¸ªæ•°: 4
        print(sparse_matrix.dtype)   # æ•°æ®ç±»åž‹: int64
        print(sparse_matrix.has_sorted_indices) # ç´¢å¼•æ˜¯å¦å·²æŽ’åº: True
        ```

    * è½¬æ¢æ ¼å¼

        ```py
        # è½¬æ¢ä¸ºå…¶ä»–ç¨€ç–æ ¼å¼
        csc_matrix = sparse_matrix.tocsc() # è½¬ä¸ºCSCæ ¼å¼ï¼ˆæŒ‰åˆ—åŽ‹ç¼©ï¼Œåˆ—æ“ä½œå¿«ï¼‰
        coo_matrix = sparse_matrix.tocoo() # è½¬ä¸ºCOOæ ¼å¼ï¼ˆåæ ‡æ ¼å¼ï¼Œæž„å»ºå¿«ï¼‰

        # è½¬æ¢ä¸ºå¯†é›†NumPyæ•°ç»„
        dense_array = sparse_matrix.toarray()
        ```

    * æ•°å­¦è¿ç®—

        ```py
        # æ ‡é‡è¿ç®—
        result = sparse_matrix * 2   # æ‰€æœ‰éžé›¶å…ƒç´ ä¹˜ä»¥2

        # çŸ©é˜µè¿ç®—ï¼ˆç»“æžœé€šå¸¸ä¹Ÿæ˜¯ç¨€ç–çŸ©é˜µï¼‰
        vector = np.array([1, 2, 3, 4])
        result_vector = sparse_matrix.dot(vector) # çŸ©é˜µ-å‘é‡ä¹˜æ³•

        other_sparse_matrix = csr_matrix([[1], [0], [1], [0]])
        result_matrix = sparse_matrix.dot(other_sparse_matrix) # çŸ©é˜µ-çŸ©é˜µä¹˜æ³•
        ```

        csr_matrix æ”¯æŒå¤§å¤šæ•°å¸¸è§çš„çŸ©é˜µè¿ç®—ã€‚

    * åˆ‡ç‰‡å’Œç´¢å¼•

        ```py
        # èŽ·å–ç¬¬1è¡Œï¼ˆè¿”å›žä¸€ä¸ª1xNçš„CSRçŸ©é˜µï¼‰
        row_1 = sparse_matrix[1, :]

        # èŽ·å–ç¬¬2åˆ—ï¼ˆæ•ˆçŽ‡è¾ƒä½Žï¼Œè€ƒè™‘ç”¨CSCæ ¼å¼åšåˆ—æ“ä½œï¼‰
        col_2 = sparse_matrix[:, 2]
        ```

        å¯¹ CSR çŸ©é˜µè¿›è¡Œåˆ‡ç‰‡é€šå¸¸ä¸å¦‚å¯¹å¯†é›†çŸ©é˜µé«˜æ•ˆï¼Œå°¤å…¶æ˜¯åˆ—åˆ‡ç‰‡ã€‚

* `csr_matrix`çš„åˆ›å»ºæ–¹æ³•

    * ä»Žå¯†é›†çŸ©é˜µï¼ˆDense Arrayï¼‰åˆ›å»º

        ä»Žä¸€ä¸ªæ™®é€šçš„ 2D NumPy æ•°ç»„æˆ–åˆ—è¡¨çš„åˆ—è¡¨åˆ›å»ºã€‚

        ```py
        import numpy as np
        from scipy.sparse import csr_matrix

        dense_matrix = np.array([[1, 0, 0, 0],
                                 [0, 0, 2, 0],
                                 [0, 3, 0, 4]])
                                 
        sparse_matrix = csr_matrix(dense_matrix)
        print(sparse_matrix)
        print(sparse_matrix.toarray()) # è½¬å›žå¯†é›†çŸ©é˜µæŸ¥çœ‹
        ```

        output:

        ```
        <Compressed Sparse Row sparse matrix of dtype 'int64'
        	with 4 stored elements and shape (3, 4)>
          Coords	Values
          (0, 0)	1
          (1, 2)	2
          (2, 1)	3
          (2, 3)	4
        [[1 0 0 0]
         [0 0 2 0]
         [0 3 0 4]]
        ```

    * ä½¿ç”¨ (data, (row, col)) åæ ‡æ ¼å¼åˆ›å»º

        æ˜Žç¡®æŒ‡å®šæ¯ä¸ªéžé›¶å…ƒç´ çš„å€¼åŠå…¶æ‰€åœ¨çš„è¡Œå’Œåˆ—åæ ‡

        ```py
        import numpy as np
        from scipy.sparse import csr_matrix

        # æ•°æ®ï¼š [1, 2, 3, 4]
        # è¡Œç´¢å¼•ï¼š[0, 1, 2, 2] -> ç¬¬ä¸€ä¸ªå…ƒç´ åœ¨ç¬¬0è¡Œï¼Œç¬¬äºŒä¸ªåœ¨ç¬¬1è¡Œï¼Œç¬¬ä¸‰ã€å››ä¸ªåœ¨ç¬¬2è¡Œ
        # åˆ—ç´¢å¼•ï¼š[0, 2, 1, 3] -> ç¬¬ä¸€ä¸ªå…ƒç´ åœ¨ç¬¬0åˆ—ï¼Œç¬¬äºŒä¸ªåœ¨ç¬¬2åˆ—ï¼Œç¬¬ä¸‰ä¸ªåœ¨ç¬¬1åˆ—ï¼Œç¬¬å››ä¸ªåœ¨ç¬¬3åˆ—

        data = [1, 2, 3, 4]
        row = [0, 1, 2, 2]
        col = [0, 2, 1, 3]

        sparse_matrix = csr_matrix((data, (row, col)), shape=(3, 4))
        print(sparse_matrix.toarray())
        ```

        output:

        ```
        [[1 0 0 0]
         [0 0 2 0]
         [0 3 0 4]]
        ```

    * ä½¿ç”¨ (data, indices, indptr) ç›´æŽ¥åˆ›å»ºï¼ˆé«˜çº§ï¼‰

        ç›´æŽ¥ä½¿ç”¨ CSR æ ¼å¼çš„ä¸‰ä¸ªå†…éƒ¨æ•°ç»„æ¥åˆ›å»ºã€‚

        ```py
        # å‡è®¾çŸ©é˜µä¸ºï¼š
        # [[1, 0, 2, 0]
        #  [0, 0, 3, 4]
        #  [5, 0, 0, 6]]

        data = [1, 2, 3, 4, 5, 6]    # æ‰€æœ‰éžé›¶å€¼
        indices = [0, 2, 2, 3, 0, 3] # æ¯ä¸ªå€¼å¯¹åº”çš„åˆ—å·
        indptr = [0, 2, 4, 6]        # ç¬¬iè¡Œçš„éžé›¶å€¼èŒƒå›´æ˜¯ data[indptr[i]:indptr[i+1]]

        # indptr è§£é‡Šï¼š
        # ç¬¬0è¡Œï¼šæœ‰ indptr[1]-indptr[0] = 2 ä¸ªå…ƒç´ ï¼Œæ˜¯ data[0:2] -> [1,2]ï¼Œåˆ—å·ä¸º indices[0:2] -> [0,2]
        # ç¬¬1è¡Œï¼šæœ‰ indptr[2]-indptr[1] = 2 ä¸ªå…ƒç´ ï¼Œæ˜¯ data[2:4] -> [3,4]ï¼Œåˆ—å·ä¸º indices[2:4] -> [2,3]
        # ç¬¬2è¡Œï¼šæœ‰ indptr[3]-indptr[2] = 2 ä¸ªå…ƒç´ ï¼Œæ˜¯ data[4:6] -> [5,6]ï¼Œåˆ—å·ä¸º indices[4:6] -> [0,3]

        sparse_matrix = csr_matrix((data, indices, indptr), shape=(3, 4))
        print(sparse_matrix.toarray())
        # [[1 0 2 0]
        #  [0 0 3 4]
        #  [5 0 0 6]]
        ```

* COO

    COO æ˜¯ â€œCoordinate Formatâ€ çš„ç¼©å†™ï¼Œå³åæ ‡æ ¼å¼ã€‚å®ƒçš„è®¾è®¡ç†å¿µéžå¸¸ç›´è§‚ï¼šåˆ†åˆ«å­˜å‚¨éžé›¶å…ƒç´ æ‰€åœ¨çš„è¡Œç´¢å¼•ã€åˆ—ç´¢å¼•ä»¥åŠå…ƒç´ çš„å€¼ã€‚

    coo_matrix å°±æ˜¯ç”±è¿™ä¸‰ä¸ªç­‰é•¿çš„æ•°ç»„æž„æˆçš„ï¼š

    * dataï¼š å­˜å‚¨æ‰€æœ‰éžé›¶å…ƒç´ çš„å€¼ï¼Œä¾‹å¦‚ [5, 9, 1, 4]

    * rowï¼š å­˜å‚¨æ¯ä¸ªéžé›¶å…ƒç´ å¯¹åº”çš„è¡Œç´¢å¼•ï¼Œä¾‹å¦‚ [0, 1, 2, 2]

    * colï¼š å­˜å‚¨æ¯ä¸ªéžé›¶å…ƒç´ å¯¹åº”çš„åˆ—ç´¢å¼•ï¼Œä¾‹å¦‚ [2, 0, 1, 2]

    COO æ ¼å¼æœ¬èº«å¹¶ä¸é€‚åˆç›´æŽ¥è¿›è¡ŒçŸ©é˜µä¹˜æ³•ã€åŠ æ³•ç­‰ç§‘å­¦è®¡ç®—ã€‚å®ƒçš„ä¸»è¦èŒè´£æ˜¯ä½œä¸ºä¸€ç§é«˜æ•ˆçš„æž„å»ºæ ¼å¼ã€‚

    ä¸€æ—¦ç”¨ COO æ ¼å¼æž„å»ºå¥½çŸ©é˜µï¼Œä½ å¯ä»¥éžå¸¸å¿«é€Ÿåœ°å°†å®ƒè½¬æ¢ä¸ºå…¶ä»–æ›´é€‚åˆè®¡ç®—çš„æ ¼å¼ï¼Œä¾‹å¦‚ï¼š

    * CSR (Compressed Sparse Row)ï¼š ç”¨äºŽé«˜æ•ˆçš„çŸ©é˜µè¿ç®—ï¼ˆå¦‚ä¹˜æ³•ï¼‰ã€‚

    * CSC (Compressed Sparse Column)ï¼š ç”¨äºŽé«˜æ•ˆçš„åˆ—æ“ä½œå’Œæ±‚è§£çº¿æ€§æ–¹ç¨‹ç»„ã€‚

    coo_matrix çš„ tocsr() å’Œ tocsc() æ–¹æ³•å°±æ˜¯ç”¨æ¥åšè¿™ä¸ªè½¬æ¢çš„ã€‚

    example:

    ```py
    import numpy as np
    from scipy.sparse import coo_matrix

    # 1. åˆ›å»º COO çŸ©é˜µçš„ä¸‰å¤§æ ¸å¿ƒæ•°ç»„
    data = np.array([5, 9, 1, 4])    # éžé›¶å…ƒç´ çš„å€¼
    row  = np.array([0, 1, 2, 2])    # è¿™äº›å…ƒç´ çš„è¡Œç´¢å¼•
    col  = np.array([2, 0, 1, 2])    # è¿™äº›å…ƒç´ çš„åˆ—ç´¢å¼•

    # 2. åˆ›å»º COO çŸ©é˜µ
    # å‚æ•° shape æŒ‡å®šçŸ©é˜µçš„æ€»å¤§å°ï¼Œè¿™é‡Œæ˜¯ä¸€ä¸ª 3x3 çš„çŸ©é˜µ
    coo_sparse_matrix = coo_matrix((data, (row, col)), shape=(3, 3))

    # 3. æŸ¥çœ‹çŸ©é˜µï¼ˆè½¬æ¢ä¸ºç¨ å¯†çŸ©é˜µæ˜¾ç¤ºï¼Œä¾¿äºŽè§‚å¯Ÿï¼‰
    print("COOçŸ©é˜µï¼ˆä»¥ç¨ å¯†å½¢å¼æ˜¾ç¤ºï¼‰:")
    print(coo_sparse_matrix.toarray())

    # è¾“å‡ºç»“æžœï¼š
    # [[0 0 5]
    #  [9 0 0]
    #  [0 1 4]]

    # 4. è½¬æ¢ä¸º CSR æ ¼å¼ä»¥è¿›è¡Œé«˜æ•ˆè¿ç®—
    csr_sparse_matrix = coo_sparse_matrix.tocsr()
    print("\nå·²è½¬æ¢ä¸ºCSRæ ¼å¼ã€‚")
    ```

* `Eigen::SparseMatrix`

    Eigen::SparseMatrix æ˜¯ Eigen åº“ä¸­ç”¨äºŽè¡¨ç¤ºå’Œæ“ä½œç¨€ç–çŸ©é˜µçš„æ¨¡æ¿ç±»ã€‚

    install:

    `sudo apt install libeigen3-dev`

    å¤´æ–‡ä»¶è¢«å®‰è£…åœ¨ï¼š`/usr/include/eigen3`

    è¿™ä¼¼ä¹Žæ˜¯ä¸€ä¸ª header-only çš„æ¨¡æ¿åº“ï¼Œæ‰€ä»¥æ²¡æœ‰åº“æ–‡ä»¶ã€‚

    example:

    ```cpp
    #include <eigen3/Eigen/Sparse>
    #include <cstdio>
    #include <vector>

    int main() {
        // åˆ›å»ºç¨€ç–çŸ©é˜µ
        Eigen::SparseMatrix<double> mat(1000, 1000);

        // ä½¿ç”¨ triplet æ’å…¥éžé›¶å…ƒç´ 
        std::vector<Eigen::Triplet<double>> triplets;
        triplets.push_back({0, 0, 3.14});  // (è¡Œ, åˆ—, å€¼)
        triplets.push_back({1, 2, 2.71});

        mat.setFromTriplets(triplets.begin(), triplets.end());

        // ç¨€ç–çŸ©é˜µè¿ç®—
        Eigen::SparseMatrix<double> mat2 = mat * mat.transpose();
        
        return 0;
    }
    ```

    å½“çŸ©é˜µå¯†åº¦ < 5% æ—¶ï¼ŒEigen::SparseMatrix åœ¨å†…å­˜å’Œè®¡ç®—æ•ˆçŽ‡ä¸Šæ˜¾è‘—ä¼˜äºŽç¨ å¯†çŸ©é˜µã€‚

* `scipy.sparse.lil_matrix`

    scipy.sparse.lil_matrix æ˜¯ SciPy ä¸­ç”¨äºŽå­˜å‚¨ç¨€ç–çŸ©é˜µçš„ä¸€ç§æ•°æ®ç»“æž„ï¼Œç‰¹åˆ«é€‚ç”¨äºŽé€æ­¥æž„å»ºå’Œä¿®æ”¹ç¨€ç–çŸ©é˜µçš„åœºæ™¯ã€‚

    LIL (List of Lists) æ ¼å¼å°†ç¨€ç–çŸ©é˜µå­˜å‚¨ä¸ºï¼š

    * è¡Œåˆ—è¡¨ï¼šæ¯ä¸ªå…ƒç´ å¯¹åº”çŸ©é˜µçš„ä¸€è¡Œ

    * æ¯è¡Œå­˜å‚¨ï¼šä¸¤ä¸ªåˆ—è¡¨ï¼Œåˆ†åˆ«å­˜å‚¨éžé›¶å…ƒç´ çš„åˆ—ç´¢å¼•å’Œå€¼

    è¿™ç§ç»“æž„ä½¿å¾—æŒ‰è¡Œæ“ä½œï¼ˆæ·»åŠ ã€åˆ é™¤ã€ä¿®æ”¹å…ƒç´ ï¼‰éžå¸¸é«˜æ•ˆã€‚

    **åŸºæœ¬ç”¨æ³•:**

    * åˆ›å»º LIL çŸ©é˜µ

        ```py
        import numpy as np
        from scipy.sparse import lil_matrix

        # æ–¹æ³•1ï¼šæŒ‡å®šå½¢çŠ¶åˆ›å»ºç©ºçŸ©é˜µ
        matrix = lil_matrix((3, 3))  # 3x3 çŸ©é˜µ

        # æ–¹æ³•2ï¼šä»Žç¨ å¯†æ•°ç»„åˆ›å»º
        dense_array = np.array([[1, 0, 0], [0, 0, 2], [0, 3, 0]])
        matrix = lil_matrix(dense_array)

        # æ–¹æ³•3ï¼šä»Žå…¶ä»–ç¨€ç–æ ¼å¼è½¬æ¢
        from scipy.sparse import csr_matrix
        csr_mat = csr_matrix((3, 3))
        lil_mat = csr_mat.tolil()
        ```

    * å…ƒç´ èµ‹å€¼å’Œä¿®æ”¹

        ```py
        # åˆ›å»º 3x3 çŸ©é˜µ
        matrix = lil_matrix((3, 3))

        # é€ä¸ªå…ƒç´ èµ‹å€¼
        matrix[0, 0] = 1
        matrix[1, 2] = 2
        matrix[2, 1] = 3

        # æ‰¹é‡èµ‹å€¼
        matrix[0, [1, 2]] = [4, 5]  # ç¬¬0è¡Œï¼Œç¬¬1ã€2åˆ—
        matrix[[1, 2], 0] = [6, 7]  # ç¬¬1ã€2è¡Œï¼Œç¬¬0åˆ—

        print(matrix.toarray())
        # è¾“å‡ºï¼š
        # [[1. 4. 5.]
        #  [6. 0. 2.]
        #  [7. 3. 0.]]
        ```

    * è®¿é—®çŸ©é˜µæ•°æ®

        ```py
        # è®¿é—®å•ä¸ªå…ƒç´ 
        print(matrix[0, 0])  # 1.0

        # è®¿é—®æ•´è¡Œ
        print(matrix[0].toarray())  # [[1. 4. 5.]]

        # èŽ·å–éžé›¶å…ƒç´ ä¿¡æ¯
        print("è¡ŒæŒ‡é’ˆ:", matrix.rows)     # æ¯è¡Œçš„åˆ—ç´¢å¼•åˆ—è¡¨
        print("æ•°æ®å€¼:", matrix.data)     # æ¯è¡Œçš„æ•°å€¼åˆ—è¡¨

        # è½¬æ¢ä¸ºç¨ å¯†æ•°ç»„
        dense = matrix.toarray()
        ```

    * å®žé™…åº”ç”¨ç¤ºä¾‹

        ```py
        # ç¤ºä¾‹ï¼šæž„å»ºé‚»æŽ¥çŸ©é˜µ
        n_nodes = 5
        adj_matrix = lil_matrix((n_nodes, n_nodes))

        # æ·»åŠ è¾¹ï¼ˆæ— å‘å›¾ï¼‰
        edges = [(0, 1), (1, 2), (2, 3), (3, 4), (0, 4)]
        for i, j in edges:
            adj_matrix[i, j] = 1
            adj_matrix[j, i] = 1  # æ— å‘å›¾å¯¹ç§°

        print("é‚»æŽ¥çŸ©é˜µ:")
        print(adj_matrix.toarray())

        # è½¬æ¢ä¸ºå…¶ä»–æ ¼å¼è¿›è¡Œé«˜æ•ˆè¿ç®—
        csr_adj = adj_matrix.tocsr()  # è½¬æ¢ä¸ºCSRæ ¼å¼è¿›è¡ŒçŸ©é˜µè¿ç®—
        ```

    * æ ¼å¼è½¬æ¢

        ```py
        # è½¬æ¢ä¸ºå…¶ä»–ç¨€ç–æ ¼å¼
        csr_matrix = matrix.tocsr()   # åŽ‹ç¼©ç¨€ç–è¡Œæ ¼å¼ï¼ˆé«˜æ•ˆè®¡ç®—ï¼‰
        csc_matrix = matrix.tocsc()   # åŽ‹ç¼©ç¨€ç–åˆ—æ ¼å¼ï¼ˆé«˜æ•ˆåˆ—æ“ä½œï¼‰
        coo_matrix = matrix.tocoo()   # åæ ‡æ ¼å¼ï¼ˆå¿«é€Ÿæž„å»ºï¼‰

        # è½¬æ¢å›žç¨ å¯†çŸ©é˜µ
        dense_matrix = matrix.toarray()
        ```

    **ä½¿ç”¨å»ºè®®**

    * æž„å»ºé˜¶æ®µï¼šä½¿ç”¨ LIL æ ¼å¼è¿›è¡Œé¢‘ç¹çš„å…ƒç´ ä¿®æ”¹

    * è®¡ç®—é˜¶æ®µï¼šè½¬æ¢ä¸º CSR/CSC æ ¼å¼è¿›è¡Œæ•°å­¦è¿ç®—

    * å†…å­˜æ•æ„Ÿï¼šå¯¹äºŽè¶…å¤§çŸ©é˜µï¼Œè€ƒè™‘ä½¿ç”¨ COO æ ¼å¼

### æ•°æ®é›†èŽ·å–ã€åˆ’åˆ†ä¸ŽåŠ è½½

* DataLoader ä¸­çš„ sampler

    sampler åªè´Ÿè´£ç”Ÿæˆç´¢å¼•ï¼Œdataloader åˆ™æŒ‰ç…§ç´¢å¼•ç”Ÿæˆ batchã€‚ä¼ªä»£ç æè¿°è¿™ä¸ªè¿‡ç¨‹ï¼š

    ```py
    # ä¼ªä»£ç ï¼Œè§£é‡Š dataloader å†…éƒ¨é€»è¾‘
    for epoch in range(...):
        for batch_indices in sampler: # é‡‡æ ·å™¨ç”Ÿæˆä¸€ä¸ªbatchçš„ç´¢å¼•åˆ—è¡¨ï¼Œå¦‚ [3, 1, 4, 9]
            batch_data = [dataset[i] for i in batch_indices] # æ ¹æ®ç´¢å¼•ä»Žæ•°æ®é›†ä¸­èŽ·å–æ•°æ®
            # ... åŽç»­çš„ collate ç­‰æ“ä½œ
            yield batch_data
    ```

    é»˜è®¤çš„ sampler æœ‰`SequentialSampler`å’Œ`RandomSampler`ã€‚

    package ä¸Žä½¿ç”¨æ–¹æ³•ï¼š

    ```py
    import torch
    from torch.utils.data import DataLoader, SequentialSampler, TensorDataset

    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ•°æ®é›†
    data = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])
    labels = torch.tensor([0, 1, 0, 1, 0])
    dataset = TensorDataset(data, labels)

    # ä½¿ç”¨ SequentialSampler
    sequential_sampler = SequentialSampler(dataset)
    dataloader = DataLoader(dataset, batch_size=2, sampler=sequential_sampler)

    # éåŽ† DataLoader
    for batch_idx, (batch_data, batch_labels) in enumerate(dataloader):
        print(f"Batch {batch_idx}:")
        print(f"  Data: {batch_data}")
        print(f"  Labels: {batch_labels}")
        print("---")
    ```

* dataset ä¼¼ä¹Žæ”¯æŒ slice è®¿é—®

    ```py
    my_dataset = MyDataset()
    print(my_dataset[:3])
    ```

    output:

    ```
    [0, 1, 2]
    ```

* èŽ·å– hugging face çš„ imdb æ•°æ®é›†

    ```py
    from datasets import load_dataset
    dataset = load_dataset('imdb')
    print(dataset['train'][0])
    ```

    æ•°æ®ä¼šè¢«ä¸‹è½½åˆ°`~/.cache/huggingface/datasets`ä¸­ã€‚imdb æ•°æ®é›†å¤§å°ä¸º 128 Mã€‚

* hugging face ä¸­çš„æ•°æ®é›†

    <https://huggingface.co/datasets>

    ä½¿ç”¨ python ä»£ç æŸ¥è¯¢ï¼š

    ```py
    from huggingface_hub import list_datasets

    # è¿™æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œè¦èŽ·å–æ€»æ•°éœ€è¦å°†å…¶è½¬æ¢ä¸ºåˆ—è¡¨ï¼Œä½†å¯¹äºŽæ•°ä¸‡ä¸ªæ•°æ®é›†è¿™ä¼šå¾ˆæ…¢ä¸”è€—å†…å­˜ã€‚
    # all_datasets = list(list_datasets())
    # print(f"Total datasets: {len(all_datasets)}")

    # æ›´é«˜æ•ˆçš„æ–¹æ³•æ˜¯ä½¿ç”¨åˆ†é¡µå¹¶è®¡æ•°ï¼ˆä½†ä¾ç„¶éœ€è¦éåŽ†æ‰€æœ‰æ•°æ®é›†ï¼‰
    count = 0
    for ds in list_datasets():
        count += 1
    print(f"Total datasets: {count}") # æ³¨æ„ï¼šè¿™ä¼šè¿è¡Œä¸€æ®µæ—¶é—´ï¼Œå› ä¸ºè¦éåŽ†æ•°ä¸‡ä¸ªæ•°æ®é›†
    ```

    å¸¸è§çš„NLPä»»åŠ¡å’Œç›¸å…³æ•°æ®é›†:

    * æ–‡æœ¬åˆ†ç±»ï¼ˆå¦‚æƒ…æ„Ÿåˆ†æžã€ä¸»é¢˜åˆ†ç±»ï¼‰ï¼šimdb, ag_news, yelp_review_full

    * é—®ç­”ï¼ˆQuestion Answeringï¼‰ï¼šsquad, natural_questions

    * æ–‡æœ¬æ‘˜è¦ï¼ˆSummarizationï¼‰ï¼šcnn_dailymail, xsum

    * æ–‡æœ¬ç”Ÿæˆï¼ˆText Generationï¼‰ï¼šwikitext-2, story_cloze

    * æœºå™¨ç¿»è¯‘ï¼ˆTranslationï¼‰ï¼šwmt14, wmt16, opus_books

    * å‘½åå®žä½“è¯†åˆ«ï¼ˆNamed Entity Recognition, NERï¼‰ï¼šconll2003, wnut_17

    * è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆSemantic Textual Similarityï¼‰ï¼šstsb_multi_mt

    * è‡ªç„¶è¯­è¨€æŽ¨ç†ï¼ˆNatural Language Inferenceï¼‰ï¼šmnli, snli

    * æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼ˆç”¨äºŽè®­ç»ƒChatæ¨¡åž‹ï¼‰ï¼šalpaca, dolly-15k

    ä½¿ç”¨ä»£ç æŒ‰æ ‡ç­¾ç­›é€‰:

    ```py
    from huggingface_hub import list_datasets

    # æŸ¥æ‰¾æ‰€æœ‰æ‰“ä¸Š "text-classification" æ ‡ç­¾çš„æ•°æ®é›†
    nlp_datasets = list(list_datasets(filter="task_categories:text-classification"))
    print(f"Number of text-classification datasets: {len(list(nlp_datasets))}")

    # æ‚¨å¯ä»¥å°è¯•å…¶ä»–æ ‡ç­¾ï¼Œå¦‚ "text-generation", "question-answering", "translation" ç­‰ã€‚
    ```

* `nn.MSELoss()`

    Mean Squared Errorï¼ˆå‡æ–¹è¯¯å·®ï¼‰, è¡¡é‡æ¨¡åž‹é¢„æµ‹å€¼ $\hat{y}$ ä¸ŽçœŸå®žå€¼ $y$ ä¹‹é—´å·®çš„å¹³æ–¹çš„å¹³å‡å€¼ã€‚

    å…¬å¼ï¼š

    $L = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2$

    å…¶ä¸­ï¼š

    * $L$ æ˜¯æœ€ç»ˆçš„æŸå¤±å€¼ï¼ˆä¸€ä¸ªæ ‡é‡ï¼‰ã€‚

    * $N$ æ˜¯æ ·æœ¬çš„æ•°é‡ï¼ˆæˆ–è€…æ˜¯éœ€è¦è®¡ç®—æŸå¤±çš„å…ƒç´ çš„æ€»ä¸ªæ•°ï¼‰ã€‚

    * $y_i$ æ˜¯ç¬¬ $i$ ä¸ªæ•°æ®çš„çœŸå®žå€¼ï¼ˆground truthï¼‰ã€‚

    * $\hat{y}_i$ æ˜¯æ¨¡åž‹å¯¹ç¬¬ $i$ ä¸ªæ•°æ®çš„é¢„æµ‹å€¼ï¼ˆpredictionï¼‰ã€‚

    * $\sum_{i=1}^{N}$ è¡¨ç¤ºå¯¹æ‰€æœ‰ $N$ ä¸ªæ•°æ®ç‚¹çš„å·®å€¼å¹³æ–¹è¿›è¡Œæ±‚å’Œã€‚

    å¹³æ–¹çš„ä½œç”¨ï¼š

    * æ¶ˆé™¤æ­£è´Ÿè¯¯å·®ç›¸äº’æŠµæ¶ˆçš„é—®é¢˜ï¼ˆä¾‹å¦‚ï¼Œ-2 å’Œ +2 çš„è¯¯å·®å¦‚æžœç›´æŽ¥ç›¸åŠ ä¼šå˜æˆ 0ï¼Œä½†è¿™æ˜¾ç„¶ä¸å¯¹ï¼‰ã€‚

    * æ”¾å¤§è¾ƒå¤§è¯¯å·®çš„è´¡çŒ®ã€‚è¯¯å·®è¶Šå¤§ï¼Œå¹³æ–¹åŽçš„æƒ©ç½šè¶Šå¤§ï¼Œè¿™ä½¿å¾—æ¨¡åž‹ä¼šå¯¹å¤§çš„é”™è¯¯æ›´åŠ æ•æ„Ÿã€‚

    PyTorch çš„ nn.MSELoss è¿˜æä¾›äº†ä¸€ä¸ªé‡è¦çš„å‚æ•° reductionï¼Œå®ƒå¯ä»¥æ”¹å˜è®¡ç®—æœ€ç»ˆæŸå¤±çš„æ–¹å¼ï¼š

    * `reduction='mean'` (é»˜è®¤å€¼): è®¡ç®—æ‰€æœ‰å…ƒç´ å¹³æ–¹å·®çš„å¹³å‡å€¼ã€‚ $\rightarrow L = \frac{1}{N} \sum (y_i - \hat{y}_i)^2$

    * `reduction='sum'`: è®¡ç®—æ‰€æœ‰å…ƒç´ å¹³æ–¹å·®çš„æ€»å’Œã€‚ $\rightarrow L = \sum (y_i - \hat{y}_i)^2$

    * `reduction='none'`: ä¸è¿›è¡Œæ±‡æ€»ï¼ˆsum æˆ– meanï¼‰ï¼Œç›´æŽ¥è¿”å›žä¸€ä¸ªä¸Žè¾“å…¥å½¢çŠ¶ç›¸åŒçš„ã€æ¯ä¸ªä½ç½®éƒ½æ˜¯ä¸€ä¸ªå¹³æ–¹å·®çš„æŸå¤±å¼ é‡ã€‚ $\rightarrow L_i = (y_i - \hat{y}_i)^2$

    example:

    ```py
    import torch
    import torch.nn as nn

    # 1. åˆ›å»ºæŸå¤±å‡½æ•°å®žä¾‹
    # reduction å¯ä»¥æ˜¯ 'mean', 'sum', 'none'
    criterion = nn.MSELoss() # é»˜è®¤ reduction='mean'
    # criterion = nn.MSELoss(reduction='sum')
    # criterion = nn.MSELoss(reduction='none')

    # 2. å‡†å¤‡ç¤ºä¾‹æ•°æ®
    # å‡è®¾æˆ‘ä»¬æœ‰4ä¸ªæ ·æœ¬çš„é¢„æµ‹å€¼å’ŒçœŸå®žå€¼
    predictions = torch.tensor([3.0, 5.0, 2.5, 4.0])
    targets = torch.tensor([2.5, 4.8, 2.0, 3.8])

    # 3. è®¡ç®—æŸå¤±
    loss = criterion(predictions, targets)

    print(f"Predictions: {predictions}")
    print(f"Targets:     {targets}")
    print(f"MSE Loss:    {loss.item()}")
    ```

    output:

    ```
    Predictions: tensor([3.0000, 5.0000, 2.5000, 4.0000])
    Targets:     tensor([2.5000, 4.8000, 2.0000, 3.8000])
    MSE Loss:    0.14499999582767487
    ```

    æ‰‹åŠ¨ä»£ç å®žçŽ°ï¼š

    ```py
    def my_mse_loss(pred, targ, reduction='mean'):
        # 1. è®¡ç®—æ‰€æœ‰å…ƒç´ çš„å¹³æ–¹å·®
        squared_diff = (pred - targ) ** 2
        
        # 2. æ ¹æ® reduction å‚æ•°è¿›è¡Œæ±‡æ€»
        if reduction == 'mean':
            loss = torch.mean(squared_diff)
        elif reduction == 'sum':
            loss = torch.sum(squared_diff)
        elif reduction == 'none':
            loss = squared_diff
        else:
            raise ValueError("reduction must be 'mean', 'sum', or 'none'")
        return loss

    # ä½¿ç”¨æˆ‘ä»¬è‡ªå·±å®žçŽ°çš„å‡½æ•°
    my_loss_mean = my_mse_loss(predictions, targets, 'mean')
    my_loss_sum = my_mse_loss(predictions, targets, 'sum')
    my_loss_none = my_mse_loss(predictions, targets, 'none')

    print(f"Manual MSE Loss (mean): {my_loss_mean.item()}")
    print(f"Manual MSE Loss (sum):  {my_loss_sum.item()}")
    print(f"Manual MSE Loss (none): {my_loss_none}")
    ```

* torch dataset and dataloader

    ```py
    import torch
    from torch.utils.data import Dataset, DataLoader

    class MyDataset(Dataset):
        def __init__(self):
            self.data = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
            self.labels = torch.tensor([0, 1, 0])

        def __len__(self):
            return len(self.data)

        def __getitem__(self, idx):
            return self.data[idx], self.labels[idx]

    dataset = MyDataset()
    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)

    for batch in dataloader:
        print("Batch Data:", batch[0])  
        print("Batch Labels:", batch[1])
    ```

    output:

    ```
    Batch Data: tensor([[1., 2.],
            [3., 4.]])
    Batch Labels: tensor([0, 1])
    Batch Data: tensor([[5., 6.]])
    Batch Labels: tensor([0])
    ```

* IMDb ç”µå½±è¯„è®ºæ•°æ®é›†

    res: <http://ai.stanford.edu/~amaas/data/sentiment/>

    IMDb æ•°æ®é›†æ˜¯ä¸€ä¸ªç”¨äºŽäºŒå…ƒæƒ…æ„Ÿåˆ†ç±»çš„ç»å…¸åŸºå‡†æ•°æ®é›†ã€‚å®ƒåŒ…å«æ¥è‡ªäº’è”ç½‘ç”µå½±æ•°æ®åº“ï¼ˆIMDbï¼‰çš„ 50,000 æ¡é«˜åº¦æžåŒ–çš„ç”µå½±è¯„è®ºã€‚

    å†…å®¹ï¼š æ¯æ¡è¯„è®ºéƒ½è¢«æ ‡è®°ä¸º æ­£é¢ï¼ˆpositiveï¼‰ æˆ– è´Ÿé¢ï¼ˆnegativeï¼‰ã€‚

    è§„æ¨¡ï¼š æ•°æ®é›†é€šå¸¸è¢«åˆ†ä¸º 25,000 æ¡å¸¦æ ‡ç­¾çš„è®­ç»ƒè¯„è®ºå’Œ 25,000 æ¡æµ‹è¯•è¯„è®ºã€‚æ­¤å¤–ï¼Œè¿˜æœ‰ 50,000 æ¡æ— æ ‡ç­¾çš„é¢å¤–è¯„è®ºï¼ˆåœ¨æ­¤ä»»åŠ¡ä¸­é€šå¸¸ä¸ä½¿ç”¨ï¼‰ã€‚

    ä»»åŠ¡ï¼š æ ¹æ®è¯„è®ºæ–‡æœ¬é¢„æµ‹å…¶æƒ…æ„Ÿæžæ€§ï¼ˆæ­£é¢/è´Ÿé¢ï¼‰ã€‚è¿™æ˜¯ä¸€ä¸ªå…¸åž‹çš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ã€‚

    explore example:

    ```py
    from datasets import load_dataset
    import numpy as np

    # 1. åŠ è½½ IMDb æ•°æ®é›†
    imdb_dataset = load_dataset("imdb")

    # 2. æŽ¢ç´¢æ•°æ®é›†ç»“æž„
    print("æ•°æ®é›†ç»“æž„:", imdb_dataset)
    print("\nè®­ç»ƒé›†ç‰¹å¾:", imdb_dataset["train"].features)
    print("\næµ‹è¯•é›†ç¬¬ä¸€æ¡æ ·æœ¬:", imdb_dataset["test"][0])

    # 3. æŸ¥çœ‹ä¸€äº›åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    # æŸ¥çœ‹è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„å¤§å°
    print(f"\nè®­ç»ƒé›†å¤§å°: {len(imdb_dataset['train'])}")
    print(f"æµ‹è¯•é›†å¤§å°: {len(imdb_dataset['test'])}")

    # æŸ¥çœ‹æ ‡ç­¾åˆ†å¸ƒ
    train_labels = imdb_dataset["train"]["label"]
    test_labels = imdb_dataset["test"]["label"]

    print(f"\nè®­ç»ƒé›† - æ­£é¢è¯„è®º: {np.sum(train_labels)}, è´Ÿé¢è¯„è®º: {len(train_labels) - np.sum(train_labels)}")
    print(f"æµ‹è¯•é›† - æ­£é¢è¯„è®º: {np.sum(test_labels)}, è´Ÿé¢è¯„è®º: {len(test_labels) - np.sum(test_labels)}")

    # 4. éšæœºæŸ¥çœ‹å‡ æ¡æ ·æœ¬
    def show_samples(dataset, split="train", num_samples=3):
        sampled_data = dataset[split].shuffle(seed=42).select(range(num_samples))
        for i in range(num_samples):
            print(f"\n--- æ ·æœ¬ {i+1} ---")
            print(f"æ–‡æœ¬é¢„è§ˆ: {sampled_data[i]['text'][:200]}...") # åªæ‰“å°å‰200ä¸ªå­—ç¬¦
            print(f"æ ‡ç­¾: {sampled_data[i]['label']} ({'æ­£é¢' if sampled_data[i]['label'] == 1 else 'è´Ÿé¢'})")

    show_samples(imdb_dataset, "train")
    ```

* `torch.utils.data`

    There are two types of datasets:

    * map-style datasets: This data set provides two functions  `__getitem__( )`, `__len__( )` that returns the indices of the sample data referred to and the numbers of samples respectively. In the example, we will use this type of dataset.

    * iterable-style datasets: Datasets that can be represented in a set of iterable data samples, for this we use `__iter__( )` function.

    Dataloader syntax:

    ```py
    DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None, *, prefetch_factor=2, persistent_workers=False)
    ```

    example:

    ```py
    # importing libraries
    import torch
    import torchvision
    from torch.utils.data import Dataset, DataLoader
    import numpy as np
    import math

    # class to represent dataset
    class HeartDataSet():

        def __init__(self):
          
            # loading the csv file from the folder path
            data1 = np.loadtxt('heart.csv', delimiter=',',
                               dtype=np.float32, skiprows=1)
            
            # here the 13th column is class label and rest 
            # are features
            self.x = torch.from_numpy(data1[:, :13])
            self.y = torch.from_numpy(data1[:, [13]])
            self.n_samples = data1.shape[0] 
        
        # support indexing such that dataset[i] can 
        # be used to get i-th sample
        def __getitem__(self, index):
            return self.x[index], self.y[index]
          
        # we can call len(dataset) to return the size
        def __len__(self):
            return self.n_samples


    dataset = HeartDataSet()

    # get the first sample and unpack
    first_data = dataset[0]
    features, labels = first_data
    print(features, labels)
    ```

    output:

    ```
    tensor([ 63.0000,   1.0000,   3.0000, 145.0000, 233.0000,   1.0000,   0.0000,
            150.0000,   0.0000,   2.3000,   0.0000,   0.0000,   1.0000]) tensor([1.])
    ```

    dataloader example:

    ```py
    # Loading whole dataset with DataLoader
    # shuffle the data, which is good for training
    dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=True)

    # total samples of data and number of iterations performed
    total_samples = len(dataset)
    n_iterations = total_samples//4
    print(total_samples, n_iterations)
    for i, (targets, labels) in enumerate(dataloader):
        print(targets, labels)
    ```

    traning example:

    ```py
    num_epochs = 2

    for epoch in range(num_epochs):
        for i, (inputs, labels) in enumerate(dataloader):

            # here: 303 samples, batch_size = 4, n_iters=303/4=75 iterations
            # Run our training process
            if (i+1) % 5 == 0:
                print(f'Epoch: {epoch+1}/{num_epochs}, Step {i+1}/{n_iterations}|\
                    Inputs {inputs.shape} | Labels {labels.shape}')
    ```

* CIFAR-10

    This contains 60,000 32x32 color images in 10 classes, with 6,000 images per class.

    ä½¿ç”¨ torch ä¸‹è½½å’ŒåŠ è½½ cifar 10:

    ```py
    import torch
    import torchvision
    import torchvision.transforms as transforms
    import torch.nn as nn
    import torch.optim as optim

    # Step 1: Loading the CIFAR-10 dataset
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]
    ])

    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                            download=True, transform=transform)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,
                                              shuffle=True, num_workers=2)

    testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                           download=True, transform=transform)
    testloader = torch.utils.data.DataLoader(testset, batch_size=4,
                                             shuffle=False, num_workers=2)

    classes = ('plane', 'car', 'bird', 'cat',
               'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

    for data in trainloader:
        input_data: torch.Tensor
        gt: torch.Tensor
        input_data, gt = data
        print('input_data:')
        print(input_data)
        print('input_data shape: {}'.format(input_data.shape))
        print('gt:')
        print(gt)
        print('gt shape: {}'.format(gt.shape))
        break
    ```

    output:

    ```
    Files already downloaded and verified
    Files already downloaded and verified
    input_data:
    tensor([[[[-0.5843, -0.5765, -0.5608,  ..., -0.6314, -0.6784, -0.8118],
              [-0.6392, -0.5843, -0.5765,  ..., -0.6706, -0.6941, -0.7804],
              [-0.6471, -0.6078, -0.6392,  ..., -0.7020, -0.7176, -0.7725],
              ...,
              [-0.4431, -0.4196, -0.3725,  ..., -0.6000, -0.6392, -0.6157],
              [-0.4118, -0.3804, -0.3647,  ..., -0.5216, -0.4980, -0.6235],
              [-0.3333, -0.3333, -0.3255,  ..., -0.5216, -0.4980, -0.6157]],

             [[-0.4902, -0.5059, -0.5294,  ..., -0.6000, -0.6471, -0.7804],
              [-0.5373, -0.5137, -0.5373,  ..., -0.6392, -0.6627, -0.7490],
              [-0.5373, -0.5294, -0.5922,  ..., -0.6706, -0.6863, -0.7412],
              ...,
              [-0.3490, -0.3490, -0.3333,  ..., -0.5765, -0.6157, -0.6078],
              [-0.3569, -0.3333, -0.3333,  ..., -0.4902, -0.4745, -0.6078],
              [-0.3490, -0.3412, -0.3255,  ..., -0.4902, -0.4745, -0.6078]],

             [[-0.5843, -0.5922, -0.6078,  ..., -0.6078, -0.6549, -0.7882],
              [-0.6784, -0.6471, -0.6549,  ..., -0.6471, -0.6706, -0.7569],
              [-0.7020, -0.6784, -0.7333,  ..., -0.6784, -0.6941, -0.7490],
              ...,
              [-0.4824, -0.4824, -0.4745,  ..., -0.7412, -0.7333, -0.6784],
              [-0.4745, -0.4588, -0.4745,  ..., -0.6784, -0.6235, -0.6784],
              [-0.4431, -0.4431, -0.4510,  ..., -0.6941, -0.6392, -0.6784]]],


            [[[-0.4353, -0.4431, -0.4275,  ..., -0.4510, -0.4275, -0.3569],
              [-0.7490, -0.7882, -0.8196,  ..., -0.8039, -0.7569, -0.7098],
              [-0.6941, -0.7804, -0.8118,  ..., -0.8431, -0.8039, -0.7804],
              ...,
              [-0.6706, -0.7725, -0.7176,  ..., -0.7333, -0.7412, -0.7412],
              [-0.6549, -0.7882, -0.7569,  ..., -0.7804, -0.7882, -0.7569],
              [-0.5451, -0.6392, -0.6706,  ..., -0.6941, -0.7098, -0.5843]],

             [[-0.6235, -0.5843, -0.5922,  ..., -0.6157, -0.5765, -0.6392],
              [-0.7333, -0.7098, -0.7255,  ..., -0.7569, -0.6863, -0.7882],
              [-0.7647, -0.7490, -0.7647,  ..., -0.7647, -0.7020, -0.7804],
              ...,
              [-0.6627, -0.6549, -0.6392,  ..., -0.7725, -0.7647, -0.7647],
              [-0.6471, -0.6392, -0.6000,  ..., -0.7882, -0.7882, -0.8039],
              [-0.6000, -0.5922, -0.5765,  ..., -0.7490, -0.7569, -0.7804]],

             [[-0.6941, -0.6549, -0.7020,  ..., -0.6549, -0.6000, -0.7725],
              [-0.5922, -0.4353, -0.5137,  ..., -0.4667, -0.3725, -0.5922],
              [-0.5529, -0.3020, -0.3882,  ..., -0.4118, -0.3255, -0.5059],
              ...,
              [-0.4667, -0.3098, -0.3725,  ..., -0.5294, -0.5451, -0.5608],
              [-0.4667, -0.2863, -0.3176,  ..., -0.4980, -0.4980, -0.5843],
              [-0.5216, -0.4039, -0.4353,  ..., -0.5608, -0.5451, -0.7098]]],


            [[[-0.7098, -0.6627, -0.7725,  ..., -0.5765, -0.5529, -0.5294],
              [-0.7098, -0.6549, -0.7412,  ..., -0.5686, -0.5373, -0.4980],
              [-0.7098, -0.6549, -0.7412,  ..., -0.5608, -0.5451, -0.5059],
              ...,
              [-0.4275, -0.3255,  0.4431,  ..., -0.1922, -0.5765, -0.6157],
              [-0.5137, -0.3255,  0.4588,  ..., -0.0980, -0.5843, -0.6314],
              [-0.6706, -0.3412,  0.4431,  ...,  0.0118, -0.5843, -0.6627]],

             [[-0.7725, -0.7490, -0.8118,  ..., -0.7176, -0.6941, -0.6863],
              [-0.7725, -0.7490, -0.8039,  ..., -0.7176, -0.6941, -0.6863],
              [-0.7647, -0.7490, -0.8118,  ..., -0.7098, -0.7020, -0.6863],
              ...,
              [-0.5686, -0.4745,  0.3176,  ..., -0.3647, -0.7098, -0.7176],
              [-0.6392, -0.4510,  0.3333,  ..., -0.2863, -0.7176, -0.7333],
              [-0.7569, -0.4510,  0.3176,  ..., -0.1765, -0.7176, -0.7490]],

             [[-0.8196, -0.8039, -0.8588,  ..., -0.7961, -0.7882, -0.7804],
              [-0.8196, -0.8118, -0.8431,  ..., -0.8039, -0.7961, -0.7882],
              [-0.8118, -0.7961, -0.8431,  ..., -0.7882, -0.7882, -0.7804],
              ...,
              [-0.6706, -0.5922,  0.1922,  ..., -0.5216, -0.7882, -0.7804],
              [-0.7098, -0.5529,  0.2078,  ..., -0.4510, -0.7961, -0.7961],
              [-0.8039, -0.5294,  0.1922,  ..., -0.3490, -0.8039, -0.8118]]],


            [[[-0.0353,  0.0118,  0.0667,  ..., -0.2627, -0.3098, -0.2863],
              [ 0.0196,  0.0588,  0.1137,  ..., -0.3098, -0.1765, -0.1373],
              [ 0.0353,  0.0745,  0.1216,  ..., -0.1216, -0.0510, -0.1451],
              ...,
              [-0.7490, -0.7255, -0.6471,  ..., -0.8118, -0.8431, -0.8510],
              [-0.7647, -0.7176, -0.6784,  ..., -0.8275, -0.8431, -0.8667],
              [-0.7804, -0.7412, -0.7176,  ..., -0.8275, -0.8431, -0.8667]],

             [[ 0.3490,  0.3961,  0.4510,  ..., -0.1686, -0.2000, -0.1608],
              [ 0.3882,  0.4353,  0.4824,  ..., -0.2392, -0.0902, -0.0353],
              [ 0.3961,  0.4275,  0.4745,  ..., -0.0588,  0.0275, -0.0510],
              ...,
              [-0.7255, -0.7412, -0.7098,  ..., -0.8118, -0.8431, -0.8510],
              [-0.7412, -0.7255, -0.7176,  ..., -0.8275, -0.8431, -0.8667],
              [-0.7569, -0.7412, -0.7333,  ..., -0.8275, -0.8431, -0.8667]],

             [[ 0.7647,  0.8039,  0.8667,  ..., -0.0431, -0.0667, -0.0353],
              [ 0.8196,  0.8588,  0.9137,  ..., -0.1137,  0.0431,  0.0980],
              [ 0.8196,  0.8588,  0.9059,  ...,  0.0745,  0.1686,  0.0824],
              ...,
              [-0.7020, -0.7020, -0.6549,  ..., -0.8118, -0.8431, -0.8510],
              [-0.7176, -0.6863, -0.6627,  ..., -0.8275, -0.8431, -0.8667],
              [-0.7333, -0.7098, -0.6941,  ..., -0.8275, -0.8431, -0.8667]]]])
    input_data shape: torch.Size([4, 3, 32, 32])
    gt:
    tensor([4, 5, 5, 9])
    gt shape: torch.Size([4])
    ```

    æ•°æ®ä¼šè¢«ä¸‹è½½åˆ°å½“å‰æ–‡ä»¶å¤¹çš„`./data`ç›®å½•é‡Œã€‚

    ```
    cifar-10-batches-py  cifar-10-python.tar.gz
    ```

### loss

* Cross Entropy Loss

    ç”¨äºŽè®¡ç®—ä¸¤ä¸ªæ¦‚çŽ‡åˆ†å¸ƒä¹‹é—´çš„å·®å€¼ã€‚

    $$\mathrm{CrossEntropyLoss}(x, \mathrm{target}) = - \frac 1 N \sum_i (\mathrm{target}_i \cdot \log x_i)$$

    * x represents the predicted values,

    * target represents the ground truth or target values.

    æ³¨ï¼š

    1. è¿™ä¸ªæ•°å­¦å…¬å¼ä¸­çš„ $\mathrm{target}_i$ æ˜¯å‘é‡ä¸­çš„å…ƒç´ ï¼Œä¸Žä¸‹é¢ torch å®žçŽ°çš„æ ‡ç­¾ç¼–ç ä¸ä¸€æ ·ã€‚

        åœ¨å®žé™…ä»»åŠ¡ä¸­ï¼Œ$\mathrm{target}_i$ å¤§éƒ¨åˆ†ä¸º 0ï¼Œåªæœ‰ä¸€ä¸ªä¸º 1ï¼Œå…¶å®žç›¸å½“äºŽä¸€ä¸ª indicatorã€‚

    1. è¿™é‡Œçš„ $N$ æŒ‡çš„å¹¶ä¸æ˜¯ batch sizeï¼Œè€Œæ˜¯ä¸€ä¸ªå‘é‡ä¸­çš„ N ä¸ªå…ƒç´ ï¼Œç›¸å½“äºŽä¸‹é¢çš„`N_class`ã€‚

    syntax:

    ```py
    torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean', label_smoothing=0.0)
    ```

    example:

    ```py
    from hlc_utils import *

    ce_loss = nn.CrossEntropyLoss()

    batch_size = 2
    N_class = 4

    input = torch.randn(batch_size, N_class)
    print('input, shape: {}, data:\n{}\n'.format(input.shape, input))

    target = torch.randint(0, N_class, (batch_size,))
    print('target, shape: {}, data:\n{}\n'.format(target.shape, target))

    output = ce_loss(input, target)
    print('output, shape: {}. data:\n{}'.format(output.shape, output))
    ```

    output:

    ```
    input, shape: torch.Size([2, 4]), data:
    tensor([[ 1.0211,  2.0191, -0.9489, -1.2573],
            [ 1.2270,  1.9557, -0.6735, -0.9454]])

    target, shape: torch.Size([2]), data:
    tensor([3, 2])

    output, shape: torch.Size([]). data:
    3.379208564758301
    ```

    æ³¨ï¼š

    1. `input` æ˜¯**æœªç»è¿‡**â€œæ¦‚çŽ‡åŒ–â€çš„å‘é‡ï¼Œæ‰€è°“æ¦‚çŽ‡åŒ–æŒ‡çš„æ˜¯ä¸€ä¸ªå‘é‡ä¸­çš„ `N_class` ä¸ªå€¼åŠ èµ·æ¥å’Œä¸º 1. `CrossEntropyLoss` å†…ç½®äº†å¯¹è¾“å…¥å€¼è¿›è¡Œ softmax é¢„å¤„ç†çš„æ“ä½œã€‚

    1. `target` çš„å€¼æ˜¯æ ‡ç­¾ç¼–ç ï¼ˆLabel Encodingï¼Œä¸Ž one-hot ç¼–ç ç›¸å¯¹åº”ï¼‰

    1. å¦‚æžœ batch size å¤§äºŽ 1ï¼Œé‚£ä¹ˆ CrossEntropyLoss æ±‚çš„æ˜¯ batch çš„å‡å€¼ã€‚

        åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œbatch size çš„å€¼ä¸º 2.

    1. `input` å¿…é¡»æ˜¯äºŒç»´çš„ï¼Œå¦‚æžœæ˜¯ä¸€ç»´çš„ï¼Œä¼šæŠ¥é”™

    Advantages:

    * Invariant to scaling and shifting of the predicted probabilities.

    Disadvantages:

    * Sensitive to outliers and imbalanced data (can be biased towards majority class).

    * It does not provide a similarity between classes which can be required in some cases.

* L1 loss

    The L1 loss function also called Mean Absolute Error (MAE) computes the average of the sum of absolute differences between the predicted and the actual values.

    Formula: 

    $\mathcal L_{L1} (y, \hat y) = \frac 1 n \sum_{i=1}^n \lvert y_i - \hat y_i\rvert$

    Here,

    * $n$ represents the total number of observations or samples

    * $y_i$ represents the actual or observed value for the i-th sample,

    * $\hat y_i$ represents the predicted or estimated value for the i-th sample.

    L1 loss is mostly used for regression problems and is more robust to outliers.

    syntax:

    ```py
    torch.nn.L1Loss(size_average=None, reduce=None, reduction='mean')
    ```

    example:

    ```py
    import torch
    from torch import nn

    #initialising the loss function
    loss = nn.L1Loss()
    #randomly initialising the input and the target value...input is considered as predicted value here.
    input = torch.randn(2, 4, requires_grad=True)
    target = torch.randn(2, 4)
    #passing both the values inside the loss function.
    output = loss(input, target)
    #backpropagation
    output.backward()
    print(output)
    ```

    output:

    ```
    tensor(1.1041, grad_fn=<MeanBackward0>)
    ```

    Advantage:

    * MAE is more robust to outliers compared to Mean Squared Error (MSE) because it takes the absolute difference, reducing the impact of extremely large errors.

    * The MAE loss is straightforward to interpret as it represents the average magnitude of errors, making it easier to communicate the model's performance to stakeholders.

    Disadvantage:

    * MAE treats all errors equally, regardless of their magnitude. This can be a disadvantage in cases where distinguishing between small and large errors is important.

    * The gradient of MAE is a constant value, which can slow down convergence during optimization, especially in comparison to MSE, where the gradient decreases as the error decreases.

* Mean Square Error (L2 loss)

    L2 computes the average of the squared differences between the predicted and actual values.

    The main idea behind squaring is to penalise the model for large difference so that the model avoid larger differences. 

    $$MSE = \frac 1 n \sum_{i=1}^n (y_i - \hat y_i)^2$$

    Here,

    * $n$ represents the total number of observations or samples,

    * $y_i$ represents the actual or observed value for the ith sample,

    * $\hat y_i$ represents the predicted or estimated value for the ith sample.

    syntax:

    ```py
    torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')
    ```

    example:

    ```py
    import torch
    from torch import nn
    #initialising the loss function
    loss = nn.MSELoss()
    #randomly initialising the input and the target value...input is considered as predicted value here.
    input = torch.randn(2, 4, requires_grad=True)
    target = torch.randn(2, 4)
    #passing both the values inside the loss function.
    output = loss(input, target)
    #backpropagation
    output.backward()
    print(output)
    ```

    output:

    ```
    tensor(1.6697, grad_fn=<MseLossBackward0>)
    ```

    example 2:

    ```py
    #import nn module
    import torch.nn as nn
    mse_loss_fn = nn.MSELoss()

    loss = mse_loss_fn(predicted_value, target)
    #predicted value is what the model is predicting 
    #target is the actual value
    ```

    Disadvantages:

    Sensitive to outliers due to the squaring operation, which deviates the results in the optimization process.

* Huber Loss

    This loss is used while tackling regression problems especially when dealing with outliers.

    $$\mathrm{HuberLoss}(x, \mathrm{target}, \delta) = 
    \frac 1 N \sum_i
    \left\{
    \begin{aligned}
        &\frac 1 2 (x_i - \mathrm{target}_i)^2 \quad \text{if } \lvert x_i - \mathrm{target}_i \rvert \leq \delta \\
        &\delta \left( \lvert x_i - \mathrm{target}_i \rvert - \frac 1 2 \delta \right) \quad \text{otherwise}
    \end{aligned}    
    \right.$$

    Here,

    * x represents the predicted values,target represents the ground truth or target values,

    * Î´ is a parameter controlling the threshold for switching between quadratic and linear loss

    It combines both MAE( Mean Absolute Error ) and MSE( Mean Squared Error) and which loss will be used depends upon the delta value.

    syntax:

    ```py
    torch.nn.HuberLoss(reduction='mean', delta=1.0)
    ```

    Advantage:

    * Less sensitive to outliers than MSE but still provide a more balanced approach to evaluating the performance of regression models compared to MAE.

    Disadvantage:

    * Introduces a new hyper parameter and the optimization of that leads to more complexity in the model.

    MAE, MSE and Huber loss are used in regression problems but, which one should we use. MSE can be used when you want to penalize larger errors more heavily. It's useful when the data does not have significant outliers and you assume that the errors are normally distributed. MAE can be used when you want robust loss function that is less affected by outliers. And Huber loss can be used when you want to compromise the benefits of both MAE and MSE. 

### ç½‘ç»œå‚æ•°

* `net.named_parameters()`

    éåŽ†ç¥žç»ç½‘ç»œä¸­çš„æ‰€æœ‰å¯å­¦ä¹ å‚æ•°ï¼ˆæƒé‡å’Œåç½®ï¼‰ï¼Œå¹¶è¿”å›žå‚æ•°åç§°å’Œå‚æ•°å€¼æœ¬èº«çš„è¿­ä»£å™¨ã€‚

    example:

    ```py
    from hlc_utils import *

    class MyModel(Module):
        def __init__(self):
            super().__init__()
            self.fc1 = Linear(784, 64)
            self.fc2 = Linear(64, 10)
        
        def forward(self, x):
            x = self.fc1(x)
            x = F.sigmoid(x)
            x = self.fc2(x)
            x = F.softmax(x)

    net = MyModel()

    for name, param in net.named_parameters():
        param: Parameter
        print('param: {}'.format(param))
        print("name: {}".format(name))
        print('shape: {}'.format(param.shape))
        print('data: {}'.format(param.data))
        print('grad: {}'.format(param.grad))
        break
    ```

    output:

    ```
    param: Parameter containing:
    tensor([[ 0.0224, -0.0285, -0.0134,  ...,  0.0081,  0.0048, -0.0166],
            [ 0.0114, -0.0229, -0.0186,  ...,  0.0354, -0.0218, -0.0119],
            [ 0.0211, -0.0086,  0.0258,  ..., -0.0265,  0.0103, -0.0192],
            ...,
            [ 0.0037,  0.0333, -0.0095,  ...,  0.0202, -0.0237, -0.0126],
            [-0.0068, -0.0324, -0.0191,  ...,  0.0220,  0.0154,  0.0047],
            [ 0.0280,  0.0258, -0.0333,  ...,  0.0143, -0.0299,  0.0020]],
           requires_grad=True)
    name: fc1.weight
    shape: torch.Size([64, 784])
    data: tensor([[ 0.0224, -0.0285, -0.0134,  ...,  0.0081,  0.0048, -0.0166],
            [ 0.0114, -0.0229, -0.0186,  ...,  0.0354, -0.0218, -0.0119],
            [ 0.0211, -0.0086,  0.0258,  ..., -0.0265,  0.0103, -0.0192],
            ...,
            [ 0.0037,  0.0333, -0.0095,  ...,  0.0202, -0.0237, -0.0126],
            [-0.0068, -0.0324, -0.0191,  ...,  0.0220,  0.0154,  0.0047],
            [ 0.0280,  0.0258, -0.0333,  ...,  0.0143, -0.0299,  0.0020]])
    grad: None
    ```

    å¯ä»¥çœ‹åˆ°`Parameter`ç»§æ‰¿è‡ª Tensorï¼Œå¯ä»¥ä½¿ç”¨`param.data`èŽ·å–åˆ° tensorã€‚å¹¶ä¸” parameter æœ¬èº«æ²¡æœ‰ name å±žæ€§ã€‚

    å·²çŸ¥ä¸€ä¸ª paramï¼Œæ— æ³•å¿«é€Ÿæ‰¾åˆ°å®ƒå¯¹åº”çš„ layerï¼Œå¿…é¡»é€šè¿‡ name åŽ»åŒ¹é…ã€‚

    è®¾ç½®ä¸åŒçš„å­¦ä¹ çŽ‡:

    ```py
    optimizer_params = []
    for name, param in net.named_parameters():
        if 'bias' in name:
            # åç½®é¡¹ä½¿ç”¨åŒå€å­¦ä¹ çŽ‡
            optimizer_params.append({'params': param, 'lr': 0.02})
        else:
            optimizer_params.append({'params': param, 'lr': 0.01})

    optimizer = torch.optim.SGD(optimizer_params)
    ```

    å‚æ•°å†»ç»“:

    ```py
    # å†»ç»“å‰å‡ å±‚çš„å‚æ•°
    for name, param in net.named_parameters():
        if 'fc1' in name:
            param.requires_grad = False  # å†»ç»“è¯¥å‚æ•°
    ```

    å‚æ•°ç»Ÿè®¡:

    ```py
    total_params = 0
    for name, param in net.named_parameters():
        if param.requires_grad:
            total_params += param.numel()
    print(f"å¯è®­ç»ƒå‚æ•°æ€»æ•°: {total_params}")
    ```

    ç›¸å…³æ–¹æ³•å¯¹æ¯”

    * parameters(): åªè¿”å›žå‚æ•°å€¼ï¼Œä¸åŒ…å«åç§°

    * state_dict(): è¿”å›žåŒ…å«å‚æ•°åç§°å’Œå€¼çš„å­—å…¸ï¼Œç”¨äºŽæ¨¡åž‹ä¿å­˜

    * named_parameters(): è¿”å›žåŒ…å«åç§°å’Œå‚æ•°çš„è¿­ä»£å™¨ï¼Œé€‚åˆéåŽ†æ“ä½œ

* nn.Parameter()

    ä¸»è¦åšä¸¤ä»¶äº‹æƒ…ï¼š

    1. ä¸º tensor å¢žåŠ  grad

    2. å°† tensor æ³¨å†Œåˆ° model çš„å‚æ•°åˆ—è¡¨ä¸­

    example:

    * add grad

        ```py
        # è‡ªåŠ¨è®¾ç½® requires_grad=True
        param = nn.Parameter(torch.tensor([1.0, 2.0, 3.0]))
        print(param.requires_grad)  # è¾“å‡º: True
        ```

    * register as model parameter

        ```py
        class MyModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.weight = nn.Parameter(torch.randn(10, 5))
                self.bias = nn.Parameter(torch.zeros(5))
            
            def forward(self, x):
                return x @ self.weight + self.bias

        model = MyModel()
        # è‡ªåŠ¨åŒ…å«åœ¨æ¨¡åž‹å‚æ•°ä¸­
        for name, param in model.named_parameters():
            print(f"{name}: {param.shape}")
        ```

* `nn.Parameter()`

    nn.Parameter() æ˜¯ä¸€ä¸ªç”¨äºŽå°†å¼ é‡åŒ…è£…ä¸ºæ¨¡åž‹å‚æ•°çš„ç±»ï¼Œå®ƒæ˜¯ torch.Tensor çš„å­ç±»ã€‚

    syntax:

    ```py
    torch.nn.Parameter(data=None, requires_grad=True)
    ```

    params:

    * `data` (Tensor): è¦åŒ…è£…ä¸ºå‚æ•°çš„å¼ é‡

    * `requires_grad` (bool, å¯é€‰): æ˜¯å¦éœ€è¦åœ¨åå‘ä¼ æ’­ä¸­è®¡ç®—æ¢¯åº¦ï¼Œé»˜è®¤ä¸º True

### tensor åˆ›å»ºä¸Žè½¬æ¢

* `np.linspace()`

    syntax:

    ```py
    np.linspace(start, stop, num=50, endpoint=True, dtype=None, retstep=False)
    ```

    retstepï¼šå¦‚æžœä¸ºTrueï¼Œè¿”å›žï¼ˆæ•°ç»„ï¼Œæ­¥é•¿ï¼‰ï¼›å¦‚æžœä¸ºFalseï¼ˆé»˜è®¤ï¼‰ï¼Œåªè¿”å›žæ•°ç»„

    example:

    ```py
    import numpy as np

    lin_1, step_1 = np.linspace(0, 2, 5, endpoint=True, retstep=True)
    lin_2, step_2 = np.linspace(0, 2, 5, endpoint=False, retstep=True)

    print("{}, step: {}".format(lin_1, step_1))
    print("{}, step: {}".format(lin_2, step_2))
    ```

    output:

    ```
    [0.  0.5 1.  1.5 2. ], step: 0.5
    [0.  0.4 0.8 1.2 1.6], step: 0.4
    ```

    å¯ä»¥çœ‹åˆ°ï¼Œå½“åŒ…å« endpoint æ—¶ï¼Œ`step = (end - start) / (num - 1)`ï¼›å½“ä¸åŒ…å« endpoint æ—¶ï¼Œ`step = (end - start) / num`ã€‚

    å…¶ä»–å¸¸è§çš„åˆ›å»ºæ•°ç»„çš„æ–¹æ³•ï¼š

    ```py
    # np.zeros() - å…¨é›¶æ•°ç»„
    np.zeros(5)                    # [0., 0., 0., 0., 0.]
    np.zeros((2, 3))               # 2x3çš„å…¨é›¶çŸ©é˜µ

    # np.ones() - å…¨1æ•°ç»„
    np.ones(4)                     # [1., 1., 1., 1.]
    np.ones((2, 2))                # 2x2çš„å…¨1çŸ©é˜µ

    # np.full() - å¡«å……æŒ‡å®šå€¼
    np.full(3, 7)                  # [7, 7, 7]
    np.full((2, 2), 5)             # 2x2çš„å¡«å……5çš„çŸ©é˜µ

    # np.eye() - å•ä½çŸ©é˜µ
    np.eye(3)                      # 3x3å•ä½çŸ©é˜µ

    # np.arange() - ç±»ä¼¼rangeï¼Œä½†è¿”å›žæ•°ç»„
    np.arange(5)                   # [0, 1, 2, 3, 4]
    np.arange(0, 10, 2)            # [0, 2, 4, 6, 8]

    # np.logspace() - å¯¹æ•°ç­‰é—´è·
    np.logspace(0, 2, 5)           # [1., 3.16, 10., 31.62, 100.]

    # np.random.rand() - å‡åŒ€åˆ†å¸ƒ
    np.random.rand(3)              # 3ä¸ª[0,1)çš„éšæœºæ•°
    np.random.rand(2, 2)           # 2x2çš„éšæœºçŸ©é˜µ

    # np.random.randn() - æ ‡å‡†æ­£æ€åˆ†å¸ƒ
    np.random.randn(3)             # 3ä¸ªæ ‡å‡†æ­£æ€åˆ†å¸ƒéšæœºæ•°

    # np.random.randint() - æ•´æ•°éšæœºæ•°
    np.random.randint(0, 10, 5)    # 5ä¸ª[0,10)çš„éšæœºæ•´æ•°

    # np.array() - ä»Žåˆ—è¡¨/å…ƒç»„åˆ›å»º
    np.array([1, 2, 3])            # ä»Žåˆ—è¡¨åˆ›å»º
    np.array([[1, 2], [3, 4]])     # äºŒç»´æ•°ç»„

    # np.asarray() - è½¬æ¢ä¸ºæ•°ç»„
    np.asarray(existing_list)      # å°†çŽ°æœ‰åºåˆ—è½¬ä¸ºæ•°ç»„

    # np.empty() - æœªåˆå§‹åŒ–æ•°ç»„ï¼ˆé€Ÿåº¦å¿«ï¼‰
    np.empty(3)                    # å†…å®¹éšæœºï¼Œä¸åˆå§‹åŒ–

    # np.copy() - åˆ›å»ºå‰¯æœ¬
    arr_copy = np.copy(original_arr)

    # np.meshgrid() - åæ ‡çŸ©é˜µ
    x = np.linspace(0, 1, 3)
    y = np.linspace(0, 1, 3)
    X, Y = np.meshgrid(x, y)       # åˆ›å»ºç½‘æ ¼åæ ‡
    ```

* `np.meshgrid()`

    np.meshgrid() çš„ä¸»è¦ä½œç”¨æ˜¯ ä»Žä¸€ç»´åæ ‡å‘é‡ç”Ÿæˆç½‘æ ¼åæ ‡çŸ©é˜µã€‚å®ƒæŽ¥å—å¤šä¸ªï¼ˆé€šå¸¸æ˜¯ä¸¤ä¸ªï¼‰ä¸€ç»´æ•°ç»„ï¼Œè¿™äº›æ•°ç»„åˆ†åˆ«ä»£è¡¨ä¸åŒåæ ‡è½´ä¸Šçš„ç‚¹ã€‚ç„¶åŽï¼Œå®ƒä¼šç”Ÿæˆä¸€ä¸ªç½‘æ ¼ï¼Œå¹¶è¿”å›žè¿™ä¸ªç½‘æ ¼ä¸­ æ¯ä¸€ä¸ªç‚¹ çš„æ¨ªåæ ‡å’Œçºµåæ ‡ã€‚

    syntax:

    ```py
    numpy.meshgrid(*xi, copy=True, sparse=False, indexing='xy')
    ```

    å‚æ•°è§£é‡Šï¼š

    * `*xi`ï¼š ä¸€ä¸ªæˆ–å¤šä¸ªä¸€ç»´æ•°ç»„ï¼Œä»£è¡¨ç½‘æ ¼çš„åæ ‡ã€‚é€šå¸¸æ˜¯ç­‰é—´è·çš„æ•°å€¼åºåˆ—ï¼ˆä¾‹å¦‚ï¼Œç”± np.linspace æˆ– np.arange ç”Ÿæˆï¼‰ã€‚

    * `copy`ï¼š å¸ƒå°”å€¼ï¼Œé»˜è®¤ä¸º Trueã€‚å¦‚æžœä¸º Falseï¼Œåˆ™è¿”å›žåŽŸå§‹æ•°ç»„çš„è§†å›¾ä»¥èŠ‚çœå†…å­˜ã€‚é€šå¸¸ä¿æŒé»˜è®¤å³å¯ã€‚

    * `sparse`ï¼š å¸ƒå°”å€¼ï¼Œé»˜è®¤ä¸º Falseã€‚å¦‚æžœä¸º Trueï¼Œåˆ™è¿”å›žç¨€ç–ç½‘æ ¼ä»¥èŠ‚çœå†…å­˜å’Œè®¡ç®—æ—¶é—´ã€‚åœ¨æ•°ç»„å¾ˆå¤§æ—¶æœ‰ç”¨ã€‚

    * `indexing`ï¼š å­—ç¬¦ä¸²ï¼Œ'xy' æˆ– 'ij'ï¼Œé»˜è®¤ä¸º 'xy'ã€‚è¿™æ˜¯ä¸€ä¸ªéžå¸¸å…³é”®çš„å‚æ•°ï¼Œå†³å®šäº†è¾“å‡ºçš„é¡ºåºã€‚

        indexing='xy'ï¼š è¿”å›žçš„ç¬¬ä¸€ä¸ªæ•°ç»„æ˜¯ çºµåæ ‡ï¼ˆYï¼‰ çš„çŸ©é˜µï¼Œç¬¬äºŒä¸ªæ•°ç»„æ˜¯ æ¨ªåæ ‡ï¼ˆXï¼‰ çš„çŸ©é˜µã€‚è¿™ä¸Žæˆ‘ä»¬é€šå¸¸çš„æ•°å­¦å’Œå›¾åƒå¤„ç†ä¹ æƒ¯ï¼ˆè¡Œå¯¹åº”Yï¼Œåˆ—å¯¹åº”Xï¼‰ä¸€è‡´ã€‚

        indexing='ij'ï¼š è¿”å›žçš„ç¬¬ä¸€ä¸ªæ•°ç»„æ˜¯ æ¨ªåæ ‡ï¼ˆXï¼‰ çš„çŸ©é˜µï¼Œç¬¬äºŒä¸ªæ•°ç»„æ˜¯ çºµåæ ‡ï¼ˆYï¼‰ çš„çŸ©é˜µã€‚è¿™ä¸ŽçŸ©é˜µç´¢å¼•ä¸€è‡´ã€‚

    è¿”å›žå€¼ï¼š

    è¿”å›žä¸€ä¸ª `list` of ndarrayï¼ˆNumpyæ•°ç»„çš„åˆ—è¡¨ï¼‰ã€‚å¯¹äºŽäºŒç»´ç½‘æ ¼ï¼Œè¿”å›žä¸¤ä¸ªäºŒç»´æ•°ç»„ï¼›å¯¹äºŽä¸‰ç»´ç½‘æ ¼ï¼Œè¿”å›žä¸‰ä¸ªä¸‰ç»´æ•°ç»„ï¼Œä¾æ­¤ç±»æŽ¨ã€‚

    example:

    ```py
    import numpy as np

    x = np.array([1, 2, 3])
    y = np.array([4, 5])

    # ä½¿ç”¨é»˜è®¤çš„ indexing='xy'
    X, Y = np.meshgrid(x, y)

    print("X (åæ ‡çŸ©é˜µ):")
    print(X)
    print("\nY (åæ ‡çŸ©é˜µ):")
    print(Y)
    ```

    output:

    ```
    X (åæ ‡çŸ©é˜µ):
    [[1 2 3]
     [1 2 3]]

    Y (åæ ‡çŸ©é˜µ):
    [[4 4 4]
     [5 5 5]]
    ```

    ç»“æžœåˆ†æžï¼š

        X çŸ©é˜µï¼šæ¯ä¸€ è¡Œ éƒ½æ˜¯ç›¸åŒçš„ï¼Œæ˜¯ x æ•°ç»„çš„å¤åˆ¶ã€‚å®ƒä»£è¡¨äº†ç½‘æ ¼ä¸­æ¯ä¸ªç‚¹çš„ æ¨ªåæ ‡ã€‚

        Y çŸ©é˜µï¼šæ¯ä¸€ åˆ— éƒ½æ˜¯ç›¸åŒçš„ï¼Œæ˜¯ y æ•°ç»„çš„å¤åˆ¶ã€‚å®ƒä»£è¡¨äº†ç½‘æ ¼ä¸­æ¯ä¸ªç‚¹çš„ çºµåæ ‡ã€‚

    è¿™æ ·ï¼Œç½‘æ ¼ä¸­çš„ç‚¹ (X[i, j], Y[i, j]) å°±æ˜¯æ‰€æœ‰ (x[j], y[i]) çš„ç»„åˆã€‚ä¾‹å¦‚ï¼š

        (X[0,0], Y[0,0]) = (1, 4)

        (X[0,1], Y[0,1]) = (2, 4)

        (X[1,0], Y[1,0]) = (1, 5)

        ...ä»¥æ­¤ç±»æŽ¨

    example:

    ```py
    import numpy as np
    import matplotlib.pyplot as plt

    # åˆ›å»ºä¸€ç»´åæ ‡å‘é‡
    x = np.linspace(-5, 5, 50)
    y = np.linspace(-5, 5, 50)

    # ç”Ÿæˆç½‘æ ¼åæ ‡çŸ©é˜µ
    X, Y = np.meshgrid(x, y)

    # å®šä¹‰äºŒç»´å‡½æ•°ï¼Œä¾‹å¦‚ R = sqrt(X^2 + Y^2)
    R = np.sqrt(X**2 + Y**2)
    # è®¡ç®—æ¯ä¸ªç½‘æ ¼ç‚¹çš„Zå€¼ï¼Œä¾‹å¦‚ Z = sin(R)
    Z = np.sin(R)

    # ç»˜åˆ¶ä¸‰ç»´å›¾å½¢
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    ax.plot_surface(X, Y, Z, cmap='viridis')
    plt.show()
    ```

* å°† tensor ä»Ž numpy è½¬æ¢åˆ° torch

    * `torch.from_numpy()`

        è¿™ç§æ–¹æ¡ˆä¼šå…±äº«å†…å­˜ã€‚

        ```py
        import torch
        import numpy as np

        # åˆ›å»º NumPy æ•°ç»„
        numpy_array = np.array([1, 2, 3, 4, 5])

        # è½¬æ¢ä¸º Torch Tensor
        torch_tensor = torch.from_numpy(numpy_array)

        print("NumPy æ•°ç»„:", numpy_array)
        print("Torch Tensor:", torch_tensor)
        print("Tensor ç±»åž‹:", torch_tensor.dtype)
        ```

    * `torch.as_tensor()`

        è¿™ç§æ–¹æ¡ˆä¼šå°½å¯èƒ½å…±äº«å†…å­˜ï¼Œä½†ä¸ä¿è¯ã€‚

        ```py
        torch_tensor = torch.as_tensor(numpy_array)
        ```

    * `torch.tensor()`

        è¿™ç§æ–¹æ¡ˆä¼šåˆ›å»ºæ•°æ®çš„å‰¯æœ¬ã€‚

        ```py
        torch_tensor = torch.tensor(numpy_array)
        ```

* torch åˆ›å»º tensor çš„å¸¸è§æ–¹æ³•

    ```py
    import torch

    tensor_1d = torch.tensor([1, 2, 3])
    print("1D Tensor (Vector):")
    print(tensor_1d)
    print()

    tensor_2d = torch.tensor([[1, 2], [3, 4]])
    print("2D Tensor (Matrix):")
    print(tensor_2d)
    print()

    random_tensor = torch.rand(2, 3)
    print("Random Tensor (2x3):")
    print(random_tensor)
    print()

    zeros_tensor = torch.zeros(2, 3)
    print("Zeros Tensor (2x3):")
    print(zeros_tensor)
    print()

    ones_tensor = torch.ones(2, 3)
    print("Ones Tensor (2x3):")
    print(ones_tensor)
    ```

    output:

    ```
    1D Tensor (Vector):
    tensor([1, 2, 3])

    2D Tensor (Matrix):
    tensor([[1, 2],
            [3, 4]])

    Random Tensor (2x3):
    tensor([[0.9134, 0.1796, 0.5852],
            [0.8830, 0.9940, 0.2796]])

    Zeros Tensor (2x3):
    tensor([[0., 0., 0.],
            [0., 0., 0.]])

    Ones Tensor (2x3):
    tensor([[1., 1., 1.],
            [1., 1., 1.]])
    ```

* å¯ä»¥åœ¨åˆ›å»º tensor æ—¶ä½¿ç”¨`device=`å‚æ•°æ¥æŒ‡å®šæ˜¯å¦ä½¿ç”¨ gpu

    ```py
    import torch

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f'Using device: {device}')

    tensor_size = (10000, 10000)  
    a = torch.randn(tensor_size, device=device)  
    b = torch.randn(tensor_size, device=device)  

    c = a + b  

    print("Result shape (moved to CPU for printing):", c.cpu().shape)

    print("Current GPU memory usage:")
    print(f"Allocated: {torch.cuda.memory_allocated(device) / (1024 ** 2):.2f} MB")
    print(f"Cached: {torch.cuda.memory_reserved(device) / (1024 ** 2):.2f} MB")
    ```

    output:

    ```
    Using device: cpu
    Result shape (moved to CPU for printing): torch.Size([10000, 10000])
    Current GPU memory usage:
    Allocated: 0.00 MB
    Cached: 0.00 MB
    ```

* å°† numpy ndarray è½¬æ¢ä¸º torch tensor

    * ä½¿ç”¨ torch.from_numpy()

        ```py
        import torch
        import numpy as np

        # åˆ›å»º NumPy æ•°ç»„
        numpy_array = np.array([1, 2, 3, 4, 5])

        # è½¬æ¢ä¸º Torch Tensor
        torch_tensor = torch.from_numpy(numpy_array)

        print("NumPy æ•°ç»„:", numpy_array)
        print("Torch Tensor:", torch_tensor)
        print("Tensor ç±»åž‹:", torch_tensor.dtype)
        ```

    * ä½¿ç”¨ torch.as_tensor()

        ```py
        torch_tensor = torch.as_tensor(numpy_array)
        ```

    * ä½¿ç”¨ torch.tensor()

        è¿™ä¸ªæ–¹æ³•ä¼šåˆ›å»ºæ•°æ®çš„å‰¯æœ¬

        ```py
        torch_tensor = torch.tensor(numpy_array)
        ```

    å…³äºŽå†…å­˜çš„å…±äº«æ€§ï¼š

    * `torch.from_numpy()`: å…±äº«å†…å­˜

    * `torch.as_tensor()`: å¦‚æžœå¯èƒ½çš„è¯ï¼Œå…±äº«å†…å­˜

    * `torch.tensor()`: ä¸å…±äº«å†…å­˜ï¼Œä¼šåˆ›å»ºå‰¯æœ¬

    example:

    ```py
    import numpy as np
    import torch

    # åˆ›å»º NumPy æ•°ç»„
    numpy_array = np.array([1, 2, 3])

    # ä½¿ç”¨ from_numpyï¼ˆå…±äº«å†…å­˜ï¼‰
    torch_tensor = torch.from_numpy(numpy_array)

    # ä¿®æ”¹ NumPy æ•°ç»„
    numpy_array[0] = 100

    print("ä¿®æ”¹åŽçš„ NumPy æ•°ç»„:", numpy_array)
    print("Torch Tensorï¼ˆä¹Ÿæ”¹å˜äº†ï¼‰:", torch_tensor)  # ä¹Ÿä¼šæ˜¾ç¤º 100

    # ä½¿ç”¨ torch.tensorï¼ˆä¸å…±äº«å†…å­˜ï¼‰
    torch_tensor_copy = torch.tensor(numpy_array)
    numpy_array[1] = 200
    print("Torch Tensor å‰¯æœ¬ï¼ˆæœªæ”¹å˜ï¼‰:", torch_tensor_copy)  # ä¸ä¼šæ”¹å˜
    ```

    output:

    ```
    ä¿®æ”¹åŽçš„ NumPy æ•°ç»„: [100   2   3]
    Torch Tensorï¼ˆä¹Ÿæ”¹å˜äº†ï¼‰: tensor([100,   2,   3])
    Torch Tensor å‰¯æœ¬ï¼ˆæœªæ”¹å˜ï¼‰: tensor([100,   2,   3])
    ```

* å…³äºŽ torch tensor åˆ›å»ºæ•°æ®å‰¯æœ¬çš„å‡ ç§æƒ…å†µ

    * torch.tensor(ä»»ä½•Pythonæ•°æ®) â†’ æ€»æ˜¯åˆ›å»ºå‰¯æœ¬

    * torch.from_numpy(np_array) â†’ å…±äº«å†…å­˜ï¼ˆä»…å¯¹NumPyæ•°ç»„ï¼‰

    * torch.as_tensor() â†’ å°½å¯èƒ½å…±äº«å†…å­˜ï¼ˆæ™ºèƒ½é€‰æ‹©ï¼‰

* å°† numpy è½¬æ¢ä¸º tensor æ—¶æŒ‡å®šç±»åž‹

    ```py
    # è½¬æ¢ä¸º float32
    torch_tensor_float = torch.from_numpy(numpy_array).float()

    # æˆ–è€…åœ¨è½¬æ¢æ—¶æŒ‡å®š
    torch_tensor_float = torch.from_numpy(numpy_array.astype(np.float32))

    # ä½¿ç”¨ dtype å‚æ•°
    torch_tensor = torch.tensor(numpy_array, dtype=torch.float32)
    ```

    æœ€ä½³å®žè·µ

        æŽ¨èä½¿ç”¨ torch.from_numpy() - æ•ˆçŽ‡é«˜ï¼Œå†…å­˜å…±äº«

        å¦‚æžœéœ€è¦ç‹¬ç«‹å‰¯æœ¬ - ä½¿ç”¨ torch.tensor()

        æ³¨æ„æ•°æ®ç±»åž‹ - ç¡®ä¿ä½¿ç”¨é€‚åˆæ·±åº¦å­¦ä¹ çš„æ•°æ®ç±»åž‹ï¼ˆé€šå¸¸æ˜¯ float32ï¼‰

        æ£€æŸ¥è®¾å¤‡ - ç¡®ä¿ tensor åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šï¼ˆCPU/GPUï¼‰

    æ³¨ï¼š

    1. `.float()`ä¼šåˆ›å»ºå‰¯æœ¬

        ```py
        import torch
        import numpy as np

        # åˆ›å»º NumPy æ•°ç»„
        numpy_array = np.array([1, 2, 3], dtype=np.int32)

        # è½¬æ¢è¿‡ç¨‹
        torch_tensor_int = torch.from_numpy(numpy_array)  # å…±äº«å†…å­˜ï¼Œdtype=int32
        torch_tensor_float = torch.from_numpy(numpy_array).float()  # åˆ›å»ºæ–°å‰¯æœ¬ï¼Œdtype=float32
        ```

    1. åªæœ‰æå‰æŠŠ numpy ndarray çš„æ•°æ®ç±»åž‹è½¬æ¢è¿‡æ¥ï¼Œæ‰èƒ½å…±äº«æ•°æ®

        ```py
        # æ–¹æ³•1ï¼šå…ˆè½¬æ¢ NumPy æ•°ç»„çš„æ•°æ®ç±»åž‹
        numpy_array_float = numpy_array.astype(np.float32)
        torch_tensor = torch.from_numpy(numpy_array_float)  # å…±äº«å†…å­˜ï¼Œfloat32

        # æ–¹æ³•2ï¼šä½¿ç”¨ astype å¹¶ä¿æŒå…±äº«
        torch_tensor = torch.from_numpy(numpy_array.astype(np.float32, copy=False))
        ```

### metric

* f measure å»¶ä¼¸

    è¿™é‡Œçš„ "F" é€šå¸¸è¢«è®¤ä¸ºæ˜¯ä»£è¡¨ F-measureï¼ˆF åº¦é‡ï¼‰ï¼Œæºè‡ªç»Ÿè®¡å­¦ä¸­çš„ F-test æ¦‚å¿µã€‚

    f1-score æœ‰æ—¶ä¹Ÿè¢«è§£é‡Šä¸ºå¹³è¡¡ Precision å’Œ Recall çš„ Harmonic Meanï¼ˆè°ƒå’Œå¹³å‡ï¼‰ã€‚

    $\beta$ å‚æ•°çš„æ„ä¹‰ï¼š

    * $\beta$ å‚æ•°æŽ§åˆ¶ç€ Precision å’Œ Recall çš„ç›¸å¯¹é‡è¦æ€§

    * $\beta = 1$ï¼šPrecision å’Œ Recall åŒç­‰é‡è¦ â†’ F1-score

    * $\beta > 1$ï¼šæ›´é‡è§† Recallï¼ˆå¦‚ $\beta = 2$ æ—¶ï¼ŒRecall çš„æƒé‡æ˜¯ Precision çš„ 4 å€ï¼‰

    * $\beta < 1$ï¼šæ›´é‡è§† Precisionï¼ˆå¦‚ $\beta = 0.5$ æ—¶ï¼ŒPrecision çš„æƒé‡æ˜¯ Recall çš„ 4 å€ï¼‰

* F1-score

    F1 æŒ‡çš„æ˜¯ F-score æˆ– F-measure å®¶æ—ä¸­çš„ç¬¬ä¸€ä¸ªæˆå‘˜ï¼Œå…·ä½“æ¥è¯´æ˜¯å½“å‚æ•° Î² = 1 æ—¶çš„ç‰¹æ®Šæƒ…å†µã€‚

    F-score çš„é€šç”¨å…¬å¼æ˜¯ï¼š

    $$F_\beta = (1 + \beta^2) \cdot \frac{\mathrm{Precision} \cdot \mathrm{Recall}}{(\beta^2 \cdot \mathrm{Precision}) + \mathrm{Recall}}$$

    å½“ $\beta = 1$ æ—¶ï¼Œå…¬å¼ç®€åŒ–ä¸ºï¼š

    $$F_1 = 2 \cdot \frac{\mathrm{Precision} \cdot \mathrm{Recall}}{\mathrm{Precision} + \mathrm{Recall}}$$

    è¿™å°±æ˜¯ F1-score çš„ç”±æ¥ - å®ƒæ˜¯ F-measure with $\beta = 1$ã€‚

* ä¸ºä»€ä¹ˆ macro ä¸ä½¿ç”¨è°ƒå’Œå¹³å‡å€¼ï¼Ÿ

    F1-score å·²ç»æ˜¯è€ƒè™‘è¿‡ç±»åˆ«å¹³è¡¡çš„æ•°æ®äº†ï¼Œç›´æŽ¥å¯¹ F1-socre ä½¿ç”¨ macro å°±å¯ä»¥ã€‚F1-socre å¯¹ Precision å’Œ Recall ä½¿ç”¨è°ƒå’Œå¹³å‡æ˜¯å› ä¸º Precision å’Œ Recall æ˜¯åŒä¸€ç±»åˆ«ä¸åŒç»´åº¦çš„æŒ‡æ ‡ã€‚è€Œ macro æ˜¯å¯¹åŒä¸€ä¸ªç»´åº¦çš„æŒ‡æ ‡è¿›è¡Œè°ƒå’Œå¹³å‡ï¼Œæ²¡æœ‰å¿…è¦ã€‚

    å¯¹å·²ç»å¹³è¡¡è¿‡çš„æŒ‡æ ‡ï¼ˆF1ï¼‰å†è¿›è¡Œä¸€æ¬¡å¹³è¡¡ï¼Œè¿™å¯èƒ½å¯¼è‡´è¿‡åº¦æƒ©ç½šã€‚

    å¦‚æžœæˆ‘ä»¬è€ƒè™‘åˆ°ä¸åŒç±»åˆ«çš„å¹³è¡¡ï¼Œå¯ä»¥ä½¿ç”¨

    1. åŠ æƒF1ï¼ˆWeighted-F1ï¼‰

        `Weighted-F1 = Î£(weight_i Ã— F1_i)`

        å…¶ä¸­ weight_i é€šå¸¸æ˜¯è¯¥ç±»åˆ«çš„æ ·æœ¬æ¯”ä¾‹

    2. å‡ ä½•å¹³å‡ï¼ˆGeometric Meanï¼‰

        å¯¹æžç«¯å€¼æ¯”ç®—æœ¯å¹³å‡æ›´æ•æ„Ÿï¼Œä½†æ¯”è°ƒå’Œå¹³å‡æ¸©å’Œï¼š

        `G-Mean = (F1_1 Ã— F1_2 Ã— ... Ã— F1_N)^(1/N)`

    3. ä½¿ç”¨ä¸“é—¨çš„ä¸å¹³è¡¡å­¦ä¹ æŒ‡æ ‡

        å¦‚ G-Meanï¼ˆå‡ ä½•å¹³å‡ï¼‰æˆ– Balanced Accuracyã€‚

    å¦‚æžœç‰¹åˆ«å…³æ³¨æœ€å·®ç±»åˆ«ï¼Œè€ƒè™‘æŠ¥å‘Š æœ€å°F1ï¼ˆMin-F1ï¼‰

* å¦‚ä½•é€‰æ‹© micro ä¸Ž macro

    é€‰æ‹©å“ªç§å¹³å‡æ–¹å¼å®Œå…¨å–å†³äºŽä½ çš„ä¸šåŠ¡ç›®æ ‡å’Œæ•°æ®é›†ç‰¹ç‚¹ã€‚

    é€‰æ‹© 'micro' å½“ï¼š

        ä½ å…³å¿ƒæ¨¡åž‹çš„æ•´ä½“æ€§èƒ½ï¼Œå¹¶ä¸”æ¯ä¸ªæ ·æœ¬çš„é”™è¯¯ä»£ä»·æ˜¯ç›¸åŒçš„ã€‚

        æ•°æ®å­˜åœ¨ä¸å¹³è¡¡ï¼Œä½†å¤§ç±»çš„æ€§èƒ½æ›´é‡è¦ã€‚ä¾‹å¦‚ï¼Œåœ¨ç”µå•†äº§å“åˆ†ç±»ä¸­ï¼Œçƒ­é”€å•†å“çš„å‡†ç¡®çŽ‡è¿œæ¯”å†·é—¨å•†å“é‡è¦ã€‚

        ä½ å¸Œæœ›å¾—åˆ°ä¸€ä¸ªå•ä¸€çš„ã€æ¦‚æ‹¬æ€§çš„æ€§èƒ½æŒ‡æ ‡ï¼Œå¹¶ä¸”è¿™ä¸ªæŒ‡æ ‡ä¸Žå‡†ç¡®çŽ‡ç­‰ä»·ã€‚

    é€‰æ‹© 'macro' å½“ï¼š

        æ‰€æœ‰ç±»åˆ«éƒ½åŒç­‰é‡è¦ï¼Œæ— è®ºå…¶æ ·æœ¬æ•°é‡å¤šå°‘ã€‚

        ä½ ç‰¹åˆ«å…³å¿ƒæ¨¡åž‹åœ¨å°ç±»/ç¨€æœ‰ç±»ä¸Šçš„è¡¨çŽ°ã€‚è¿™åœ¨å¾ˆå¤šå…³é”®é¢†åŸŸè‡³å…³é‡è¦ï¼š

            åŒ»ç–—ï¼š è¯Šæ–­ä¸€ä¸ªç¨€æœ‰ç—…ã€‚

            é‡‘èžé£ŽæŽ§ï¼š æ£€æµ‹æžå°‘æ•°ä½†å±å®³å·¨å¤§çš„æ¬ºè¯ˆäº¤æ˜“ã€‚

            å·¥ä¸šï¼š é¢„æµ‹ç½•è§çš„è®¾å¤‡æ•…éšœã€‚

        ä½ çš„æ•°æ®é›†ç±»åˆ«ç›¸å¯¹å¹³è¡¡ã€‚

        ä½ æƒ³è¯„ä¼°æ¨¡åž‹çš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œçœ‹å®ƒæ˜¯å¦åœ¨æ‰€æœ‰ç±»åˆ«ä¸Šéƒ½â€œå­¦å¾—ä¸é”™â€ã€‚


    æœ€ä½³å®žè·µ

    ä¸è¦åªçœ‹ä¸€ä¸ªæ•°å­—ï¼ ä¸€ä¸ªè´Ÿè´£ä»»çš„å®žè·µæ˜¯ï¼š

        åŒæ—¶æŠ¥å‘Š Micro å’Œ Macro å€¼ï¼Œä»¥æä¾›æ›´å…¨é¢çš„è§†å›¾ã€‚

        æŸ¥çœ‹æ¯ä¸ªç±»åˆ«çš„å•ç‹¬æŒ‡æ ‡ï¼ˆå³ä¸å¹³å‡ï¼‰ï¼Œè¿™èƒ½æœ€ç›´æŽ¥åœ°å‘çŽ°é—®é¢˜æ‰€åœ¨ã€‚

        åˆ†æžæ··æ·†çŸ©é˜µï¼Œç›´è§‚åœ°çœ‹åˆ°å“ªäº›ç±»åˆ«è¢«æ··æ·†äº†ã€‚

* precision çš„ä¸‰ç§æ¨¡å¼ micro, macro ä¸Ž none

    * `'micro'`ï¼š å…¨å±€è§†è§’ã€‚å…ˆæ±‡æ€»æ‰€æœ‰ç±»åˆ«ï¼ˆæˆ–æ‰€æœ‰æ ·æœ¬ï¼‰çš„ TP, FP, FNï¼Œå†ç”¨æ±‡æ€»åŽçš„æ€»æ•°è®¡ç®—ä¸€ä¸ªå…¨å±€æŒ‡æ ‡ã€‚

    * `'macro'`ï¼š å¹³å‡è§†è§’ã€‚å…ˆç‹¬ç«‹è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡ï¼Œç„¶åŽå¯¹æ‰€æœ‰ç±»åˆ«çš„æŒ‡æ ‡å€¼æ±‚ç®—æœ¯å¹³å‡ã€‚

    * `'none'`: è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ precisionï¼Œä¸è¿›è¡Œå¹³å‡

    example:

    ```py
    from torchmetrics import Precision

    pred = t.tensor([0, 1, 2, 3])
    gt = t.tensor([0, 1, 0, 0])

    pre = Precision('multiclass', num_classes=10, average='micro')
    pre.update(pred, gt)
    pre_score = pre.compute()
    print('micro pre: {}'.format(pre_score))

    pre = Precision('multiclass', num_classes=10, average='macro')
    pre.update(pred, gt)
    pre_score = pre.compute()
    print('macro pre: {}'.format(pre_score))

    pre = Precision('multiclass', num_classes=10, average='none')
    pre.update(pred, gt)
    pre_score = pre.compute()
    print('none pre: {}'.format(pre_score))
    ``` 

    output:

    ```
    micro pre: 0.5
    macro pre: 0.5
    none pre: tensor([1., 1., 0., 0., 0., 0., 0., 0., 0., 0.])
    ```

    * `'micro'`æ¨¡å¼è¯¦è§£

        å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå¤šç±»åˆ†ç±»é—®é¢˜ï¼Œæœ‰ C ä¸ªç±»åˆ«ã€‚

        1. é€ç±»ç»Ÿè®¡ï¼š

            å¯¹äºŽæ¯ä¸ªç±»åˆ« iï¼Œè®¡ç®—å…¶çœŸæ­£ä¾‹ï¼ˆTP_iï¼‰å’Œå‡æ­£ä¾‹ï¼ˆFP_iï¼‰ã€‚

            * çœŸæ­£ä¾‹ï¼ˆTP_iï¼‰ï¼š çœŸå®žæ ‡ç­¾ä¸º i ä¸”è¢«é¢„æµ‹ä¸º i çš„æ ·æœ¬æ•°ã€‚

            * å‡æ­£ä¾‹ï¼ˆFP_iï¼‰ï¼š çœŸå®žæ ‡ç­¾ä¸æ˜¯ i ä½†è¢«é¢„æµ‹ä¸º i çš„æ ·æœ¬æ•°ã€‚

        2. å…¨å±€æ±‡æ€»ï¼š

            * è®¡ç®—æ‰€æœ‰ç±»åˆ«çš„ TP ä¹‹å’Œï¼š total_TP = TP_1 + TP_2 + ... + TP_C

            * è®¡ç®—æ‰€æœ‰ç±»åˆ«çš„ FP ä¹‹å’Œï¼š total_FP = FP_1 + FP_2 + ... + FP_C

        3. è®¡ç®— Micro Precisionï¼š

            ä½¿ç”¨æ±‡æ€»åŽçš„ total_TP å’Œ total_FP æ¥è®¡ç®— Precisionï¼Œå…¬å¼å’Œæ ‡å‡†çš„äºŒåˆ†ç±» Precision ä¸€æ¨¡ä¸€æ ·ã€‚

            $\mathrm{Precision_{micro}} = \frac{\mathrm{total\_TP}}{\mathrm{total\_TP + total\_FP}}$

        é‡è¦ç‰¹æ€§ä¸Žæ³¨æ„äº‹é¡¹

        * ä¸Ž Accuracy çš„å…³ç³»ï¼š åœ¨å¤šç±»åˆ†ç±»ä¸­ï¼ŒMicro Precision çš„å€¼ç­‰äºŽ å‡†ç¡®çŽ‡ï¼ˆAccuracyï¼‰ã€‚è¿™æ˜¯å› ä¸ºï¼š

            * total_TP å°±æ˜¯æ‰€æœ‰è¢«æ­£ç¡®åˆ†ç±»çš„æ ·æœ¬æ€»æ•°ã€‚

            * total_TP + total_FP å°±æ˜¯æ‰€æœ‰è¢«é¢„æµ‹ä¸ºæ­£ä¾‹çš„æ ·æœ¬æ€»æ•°ï¼Œåœ¨å¤šç±»åˆ†ç±»ä¸­ï¼Œè¿™ç­‰äºŽæ€»æ ·æœ¬æ•°ï¼ˆå› ä¸ºæ¯ä¸ªæ ·æœ¬å¿…é¡»è¢«åˆ†åˆ°ä¸€ä¸ªç±»åˆ«ï¼‰ã€‚

            * æ‰€ä»¥ï¼ŒMicro Precision = total_TP / N = Accuracyã€‚

        * æ ·æœ¬ä¸å¹³è¡¡ï¼š Micro å¹³å‡å¯¹æ¯ä¸ªæ ·æœ¬â€œä¸€è§†åŒä»â€ï¼Œå› æ­¤å®ƒæ›´é€‚åˆæ ·æœ¬ä¸å¹³è¡¡çš„æ•°æ®é›†ï¼Œå› ä¸ºå¤§ç±»çš„æ€§èƒ½ä¼šä¸»å¯¼æœ€ç»ˆç»“æžœã€‚å¦‚æžœä½ å…³å¿ƒå°ç±»çš„æ€§èƒ½ï¼Œåº”è¯¥ä½¿ç”¨ 'macro' å¹³å‡ã€‚

        * å¤šæ ‡ç­¾ä»»åŠ¡ï¼š åœ¨å¤šæ ‡ç­¾ä»»åŠ¡ä¸­ï¼ˆä¸€ä¸ªæ ·æœ¬å¯ä»¥æœ‰å¤šä¸ªæ ‡ç­¾ï¼‰ï¼ŒMicro Precision çš„è®¡ç®—é€»è¾‘å®Œå…¨ç›¸åŒï¼ˆæ±‡æ€»æ‰€æœ‰æ ‡ç­¾çš„ TP å’Œ FPï¼‰ï¼Œä½†æ­¤æ—¶å®ƒä¸ç­‰äºŽ Accuracyï¼Œå› ä¸ºä¸€ä¸ªæ ·æœ¬å¯ä»¥æœ‰å¤šä¸ªé¢„æµ‹å’Œå¤šä¸ªçœŸå®žæ ‡ç­¾ã€‚

    * `'micro'`ä¸Ž`'macro'`æ¨¡å¼çš„å¯¹æ¯”

        * å¯¹ç±»åˆ«ä¸å¹³è¡¡çš„æ•æ„Ÿæ€§

            * `micro`

                ä¸æ•æ„Ÿï¼ˆé»˜è®¤åå‘å¤§ç±»ï¼‰
                å¤§ç±»çš„æ€§èƒ½ä¸»å¯¼äº†æœ€ç»ˆç»“æžœã€‚å› ä¸ºå¤§ç±»çš„ TP/FP æ•°é‡è¿œå¤šäºŽå°ç±»ï¼Œåœ¨æ±‡æ€»æ—¶è´¡çŒ®æœ€å¤§ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼ŒMicro Precision (0.786) æ›´æŽ¥è¿‘å¤§ç±» A çš„ Precision (0.947)ã€‚

            * `macro`

                æ•æ„Ÿï¼ˆå¹³ç­‰å¯¹å¾…æ¯ä¸ªç±»ï¼‰
                å°†æ‰€æœ‰ç±»åˆ«è§†ä¸ºåŒç­‰é‡è¦ï¼Œæ— è®ºå…¶æ ·æœ¬å¤šå°‘ã€‚å°ç±»çš„å·®åŠ²æ€§èƒ½ä¼šç›´æŽ¥æ‹‰ä½Žå¹³å‡å€¼ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼ŒMacro Precision (0.831) è¢«å°ç±» C çš„ Precision (0.714) æ‹‰ä½Žäº†ã€‚
        * ä¼˜ç‚¹

            * micro

                1. ç»¼åˆæ€§èƒ½ï¼š å¾ˆå¥½åœ°è¡¡é‡äº†æ¨¡åž‹åœ¨æ•´ä½“æ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚
                2. ç­‰äºŽAccuracyï¼š åœ¨å¤šç±»åˆ†ç±»ä¸­ï¼ŒMicro-Precision/Recall/F1 ç­‰äºŽå‡†ç¡®çŽ‡ï¼Œæ˜“äºŽç†è§£ã€‚
                3. é€‚ç”¨äºŽæ ·æœ¬ä¸å¹³è¡¡ä½†å…³å¿ƒæ•´ä½“æ€§èƒ½çš„åœºæ™¯ã€‚

            * macro

                1. å…¬å¹³æ€§ï¼š ç»™äºˆæ‰€æœ‰ç±»åˆ«åŒç­‰æƒé‡ï¼Œèƒ½æ­ç¤ºæ¨¡åž‹åœ¨å°ç±»ä¸Šçš„çŸ­æ¿ã€‚
                2. ç¨³å®šæ€§ï¼š ä¸å—ç±»åˆ«åˆ†å¸ƒå½±å“ï¼Œé€‚åˆæ¯”è¾ƒä¸åŒæ•°æ®é›†æˆ–ä¸åŒé‡‡æ ·ç­–ç•¥ä¸‹çš„æ¨¡åž‹ã€‚
                3. é€‚ç”¨äºŽéœ€è¦å…³æ³¨å°ç±»çš„åœºæ™¯ï¼ˆå¦‚åŒ»ç–—è¯Šæ–­ã€æ•…éšœæ£€æµ‹ï¼‰ã€‚

        * ç¼ºç‚¹

            * micro

                1. æŽ©ç›–å°ç±»é—®é¢˜ï¼š å¦‚æžœæ¨¡åž‹å®Œå…¨å¿½ç•¥å°ç±»ï¼Œä½†åªè¦å¤§ç±»è¡¨çŽ°å¥½ï¼ŒMicroæŒ‡æ ‡ä¾ç„¶ä¼šå¾ˆé«˜ï¼Œä»Žè€Œè¯¯å¯¼ä½ è®¤ä¸ºæ¨¡åž‹å¾ˆå¥½ã€‚
                2. å¯¹æ•°æ®åˆ†å¸ƒæ•æ„Ÿï¼š ç»“æžœä¸¥é‡ä¾èµ–äºŽæ•°æ®é›†çš„ç±»åˆ«åˆ†å¸ƒã€‚

            * macro

                1. å¯èƒ½ä½Žä¼°æ€§èƒ½ï¼š å¦‚æžœä¸€ä¸ªæ¨¡åž‹åœ¨å¤§ç±»ä¸Šè¡¨çŽ°æžå¥½ï¼Œä½†åœ¨ä¸€ä¸ªæ ·æœ¬æžå°‘çš„å°ç±»ä¸Šè¡¨çŽ°ç¨å·®ï¼ŒMacroæŒ‡æ ‡å¯èƒ½ä¼šç»™å‡ºä¸€ä¸ªç›¸å¯¹è¾ƒä½Žçš„è¯„ä»·ï¼Œè¿™å¯èƒ½ä¸å®Œå…¨ç¬¦åˆä¸šåŠ¡ç›´è§‰ã€‚
                2. å¯¹å™ªå£°æ•æ„Ÿï¼š ä¸€ä¸ªåœ¨æŸä¸ªå°ç±»ä¸Šçš„æžç«¯å·®å€¼ï¼ˆå¦‚ Precision=0ï¼‰ä¼šä¸¥é‡æ‹‰ä½Žæ•´ä½“å¹³å‡å€¼ã€‚

* torchmetrics

    `acc.update(pred, gt)`

    åœ¨`.update()`å‡½æ•°ä¸­ï¼Œç¬¬ä¸€ä¸ªå‚æ•°å¿…é¡»æ˜¯ predï¼Œç¬¬äºŒä¸ªå‚æ•°å¿…é¡»æ˜¯ gtã€‚

    pred å’Œ gt å¿…é¡»æ˜¯ torch çš„ tensor ç±»åž‹ï¼Œä¸èƒ½æ˜¯ numpy çš„ ndarrayã€‚

    å¦‚æžœ pred æ˜¯ä¸€ç»´çš„ï¼Œé‚£ä¹ˆå…¶ç¼–ç æ–¹å¼ä¸ºæ ‡ç­¾ç¼–ç ï¼Œå³é¢„æµ‹çš„ç±»åˆ«çš„ç´¢å¼•ï¼Œè€Œä¸æ˜¯æ¦‚çŽ‡ã€‚
    
    å¦‚æžœ pred æ˜¯äºŒç»´çš„ï¼Œé‚£ä¹ˆ pred çš„ç±»åž‹å¿…é¡»æ˜¯ floatï¼Œä¸èƒ½æ˜¯ intï¼Œå…¶ä»£è¡¨çš„å«ä¹‰ä¸ºè¾“å‡ºçš„æ¦‚çŽ‡ã€‚é—®é¢˜ï¼šæ˜¯å¦éœ€è¦ç»è¿‡ softmaxï¼Ÿé—®é¢˜ï¼šå¦‚æžœä½¿ç”¨ max() å–æ¦‚çŽ‡æœ€å¤§å€¼ï¼Œé‚£ä¹ˆ threshold = 0.5 æœ‰ä»€ä¹ˆæ„ä¹‰ï¼Ÿ

* Accuracyï¼ˆå‡†ç¡®çŽ‡ï¼‰, Precisionï¼ˆç²¾ç¡®çŽ‡/æŸ¥å‡†çŽ‡ï¼‰, Recallï¼ˆå¬å›žçŽ‡/æŸ¥å…¨çŽ‡ï¼‰

    * accuracy

        å«ä¹‰ï¼šæ‰€æœ‰é¢„æµ‹ç»“æžœä¸­ï¼Œé¢„æµ‹æ­£ç¡®çš„æ¯”ä¾‹ã€‚

        å…¬å¼ï¼š

        `Accuracy = (TP + TN) / (TP + TN + FP + FN)`

        æ„ä¹‰ï¼šè¡¡é‡æ¨¡åž‹æ•´ä½“çš„æ­£ç¡®çŽ‡ã€‚å®ƒæ˜¯ä¸€ä¸ªéžå¸¸ç›´è§‚çš„æŒ‡æ ‡ã€‚

        ä¼˜ç¼ºç‚¹ï¼š

        * ä¼˜ç‚¹ï¼šå®¹æ˜“ç†è§£ã€‚

        * ç¼ºç‚¹ï¼šåœ¨æ•°æ®ä¸å¹³è¡¡çš„æ•°æ®é›†ä¸Šï¼Œå‡†ç¡®çŽ‡ä¼šä¸¥é‡å¤±çœŸã€‚

            ä¾‹å­ï¼šåœ¨ä¸€ä¸ªæœ‰1000ä¸ªæ ·æœ¬çš„æ•°æ®é›†ä¸­ï¼Œæœ‰990ä¸ªè´Ÿæ ·æœ¬ï¼ˆ0ï¼‰ï¼Œåªæœ‰10ä¸ªæ­£æ ·æœ¬ï¼ˆ1ï¼‰ã€‚å¦‚æžœä¸€ä¸ªæ¨¡åž‹ç®€å•åœ°å°†æ‰€æœ‰æ ·æœ¬éƒ½é¢„æµ‹ä¸ºè´Ÿï¼Œé‚£ä¹ˆå®ƒçš„å‡†ç¡®çŽ‡æ˜¯ (0 + 990) / 1000 = 99%ã€‚è™½ç„¶å‡†ç¡®çŽ‡å¾ˆé«˜ï¼Œä½†è¿™ä¸ªæ¨¡åž‹å®Œå…¨æ²¡æœ‰è¯†åˆ«æ­£ä¾‹çš„èƒ½åŠ›ï¼Œæ˜¯ä¸€ä¸ªæ— ç”¨çš„æ¨¡åž‹ã€‚

    * precision

        å«ä¹‰ï¼šåœ¨æ‰€æœ‰è¢«æ¨¡åž‹é¢„æµ‹ä¸ºæ­£ä¾‹çš„æ ·æœ¬ä¸­ï¼ŒçœŸæ­£çš„æ­£ä¾‹æœ‰å¤šå°‘ã€‚

        å…¬å¼ï¼š

        `Precision = TP / (TP + FP)`

        æ„ä¹‰ï¼šè¡¡é‡æ¨¡åž‹çš„â€œç²¾å‡†åº¦â€æˆ–â€œå®ç¼ºæ¯‹æ»¥â€çš„ç¨‹åº¦ã€‚å®ƒå…³æ³¨çš„æ˜¯é¢„æµ‹ç»“æžœã€‚

        æ ¸å¿ƒé—®é¢˜ï¼šå½“æ¨¡åž‹è¯´æŸä¸ªä¸œè¥¿æ˜¯â€œæ­£ä¾‹â€æ—¶ï¼Œå®ƒæœ‰å¤šå¯ä¿¡ï¼Ÿ

        åº”ç”¨åœºæ™¯ï¼šæ³¨é‡å‡å°‘è¯¯æŠ¥ï¼ˆFPï¼‰çš„åœºæ™¯ã€‚

        * åžƒåœ¾é‚®ä»¶æ£€æµ‹ï¼šæˆ‘ä»¬éžå¸¸ä¸å¸Œæœ›æŠŠæ­£å¸¸é‚®ä»¶è¯¯åˆ¤ä¸ºåžƒåœ¾é‚®ä»¶ï¼ˆFPï¼‰ã€‚å®å¯æ”¾è¿‡ä¸€äº›åžƒåœ¾é‚®ä»¶ï¼ˆFNï¼‰ï¼Œä¹Ÿä¸èƒ½è¯¯æ€æ­£å¸¸é‚®ä»¶ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦é«˜ç²¾ç¡®çŽ‡ã€‚

        * æŽ¨èç³»ç»Ÿï¼šç»™ç”¨æˆ·æŽ¨é€çš„å†…å®¹ï¼Œå¸Œæœ›å°½é‡éƒ½æ˜¯ä»–æ„Ÿå…´è¶£çš„ã€‚å¦‚æžœæŽ¨é€äº†ä¸æ„Ÿå…´è¶£çš„å†…å®¹ï¼ˆFPï¼‰ï¼Œä¼šå½±å“ç”¨æˆ·ä½“éªŒã€‚

    * recall

        å«ä¹‰ï¼šåœ¨æ‰€æœ‰å®žé™…ä¸ºæ­£ä¾‹çš„æ ·æœ¬ä¸­ï¼Œæ¨¡åž‹æˆåŠŸé¢„æµ‹å‡ºæ¥çš„æœ‰å¤šå°‘ã€‚

        å…¬å¼ï¼š

        `Recall = TP / (TP + FN)`

        æ„ä¹‰ï¼šè¡¡é‡æ¨¡åž‹çš„â€œè¦†ç›–çŽ‡â€æˆ–â€œå®é”™æ€ä¸æ¼æ”¾â€çš„ç¨‹åº¦ã€‚å®ƒå…³æ³¨çš„æ˜¯çœŸå®žæƒ…å†µã€‚

        æ ¸å¿ƒé—®é¢˜ï¼šåœ¨æ‰€æœ‰çœŸæ­£çš„æ­£ä¾‹ä¸­ï¼Œæ¨¡åž‹æ‰¾å‡ºäº†å¤šå°‘ï¼Ÿ

        åº”ç”¨åœºæ™¯ï¼šæ³¨é‡å‡å°‘æ¼æŠ¥ï¼ˆFNï¼‰çš„åœºæ™¯ã€‚

        * ç–¾ç—…æ£€æµ‹ï¼šæˆ‘ä»¬éžå¸¸ä¸å¸Œæœ›æŠŠä¸€ä¸ªæ‚£ç—…çš„äººè¯¯åˆ¤ä¸ºå¥åº·ï¼ˆFNï¼‰ã€‚å®å¯è®©ä¸€äº›å¥åº·çš„äººåšè¿›ä¸€æ­¥æ£€æŸ¥ï¼ˆFPï¼‰ï¼Œä¹Ÿä¸èƒ½æ¼æŽ‰ä¸€ä¸ªç—…äººã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦é«˜å¬å›žçŽ‡ã€‚

        * é€ƒçŠ¯è¯†åˆ«ï¼šåœ¨å®‰æ£€ç³»ç»Ÿä¸­ï¼Œç»å¯¹ä¸èƒ½æ¼æŽ‰ä¸€ä¸ªé€ƒçŠ¯ï¼ˆFNï¼‰ã€‚å³ä½¿éœ€è¦è¯¯è­¦ä¸€äº›æ™®é€šäººï¼ˆFPï¼‰è¿›è¡ŒäºŒæ¬¡æ£€æŸ¥ï¼Œä¹Ÿè¦ç¡®ä¿é«˜å¬å›žçŽ‡ã€‚

    * Precisionå’ŒRecallçš„â€œè··è··æ¿â€å…³ç³»

        åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œç²¾ç¡®çŽ‡ï¼ˆPrecisionï¼‰å’Œå¬å›žçŽ‡ï¼ˆRecallï¼‰æ˜¯ç›¸äº’çŸ›ç›¾çš„ã€‚æé«˜ä¸€ä¸ªï¼Œé€šå¸¸ä¼šå¯¼è‡´å¦ä¸€ä¸ªçš„é™ä½Žã€‚

        * å¦‚æžœä½ æƒ³æé«˜Precisionï¼ˆå‡å°‘FPï¼‰ï¼š

            ä½ éœ€è¦æé«˜é¢„æµ‹æ­£ä¾‹çš„é—¨æ§›ã€‚ä¾‹å¦‚ï¼Œåªæœ‰æ¨¡åž‹æœ‰99%çš„æŠŠæ¡æ—¶æ‰é¢„æµ‹ä¸ºæ­£ã€‚è¿™æ ·ï¼Œè¢«é¢„æµ‹ä¸ºæ­£çš„æ ·æœ¬ç¡®å®žå¾ˆå¯èƒ½æ˜¯æ­£çš„ï¼ˆPrecisioné«˜ï¼‰ï¼Œä½†å¾ˆå¤šâ€œæ²¡é‚£ä¹ˆç¡®å®šâ€çš„æ­£ä¾‹ä¼šè¢«åˆ¤ä¸ºè´Ÿä¾‹ï¼Œä»Žè€Œå¯¼è‡´æ¼æŠ¥å¢žåŠ ï¼ˆFNå¢žåŠ ï¼‰ï¼ŒRecallé™ä½Žã€‚

        * å¦‚æžœä½ æƒ³æé«˜Recallï¼ˆå‡å°‘FNï¼‰ï¼š

            ä½ éœ€è¦é™ä½Žé¢„æµ‹æ­£ä¾‹çš„é—¨æ§›ã€‚ä¾‹å¦‚ï¼Œåªè¦æ¨¡åž‹æœ‰50%çš„æŠŠæ¡å°±é¢„æµ‹ä¸ºæ­£ã€‚è¿™æ ·ï¼Œä½ èƒ½æŠ“ä½å‡ ä¹Žæ‰€æœ‰çš„æ­£ä¾‹ï¼ˆRecallé«˜ï¼‰ï¼Œä½†ä¹Ÿä¼šæ··å…¥å¾ˆå¤šå…¶å®žæ˜¯è´Ÿä¾‹çš„æ ·æœ¬ï¼Œå¯¼è‡´è¯¯æŠ¥å¢žåŠ ï¼ˆFPå¢žåŠ ï¼‰ï¼ŒPrecisioné™ä½Žã€‚

    * ä¸ŽAccuracyçš„å…³ç³»

        Accuracyæä¾›äº†ä¸€ä¸ªå®è§‚çš„ã€æ•´ä½“çš„æ€§èƒ½è§†å›¾ã€‚

        Precisionå’ŒRecallæä¾›äº†æ›´ç»†ç²’åº¦çš„ã€é’ˆå¯¹ç‰¹å®šç±»åˆ«ï¼ˆæ­£ä¾‹ï¼‰çš„æ€§èƒ½è§†å›¾ã€‚

        åœ¨æ•°æ®å¹³è¡¡ä¸”FPå’ŒFNçš„æˆæœ¬ç›¸ä¼¼çš„é—®é¢˜ä¸­ï¼ŒAccuracyæ˜¯ä¸€ä¸ªä¸é”™çš„æŒ‡æ ‡ã€‚

        åœ¨æ•°æ®ä¸å¹³è¡¡æˆ–FPä¸ŽFNçš„æˆæœ¬æ˜Žæ˜¾ä¸åŒçš„é—®é¢˜ä¸­ï¼Œå¿…é¡»ç»“åˆPrecisionå’ŒRecallï¼ˆä»¥åŠF1-Scoreï¼‰æ¥åˆ†æžã€‚

* F1-Scoreï¼šè°ƒå’Œå¹³å‡æ•°

    ä¸ºäº†åŒæ—¶è€ƒè™‘Precisionå’ŒRecallï¼Œæˆ‘ä»¬å¼•å…¥äº† F1-Scoreã€‚

    å…¬å¼ï¼š

    `F1-Score = 2 * (Precision * Recall) / (Precision + Recall)`

    æ„ä¹‰ï¼šF1-Score æ˜¯ Precision å’Œ Recall çš„è°ƒå’Œå¹³å‡æ•°ã€‚å®ƒåªæœ‰åœ¨ Precision å’Œ Recall éƒ½è¾ƒé«˜æ—¶æ‰ä¼šé«˜ã€‚å› æ­¤ï¼Œå®ƒæ˜¯ä¸€ä¸ªç»¼åˆæ€§çš„æŒ‡æ ‡ï¼Œç‰¹åˆ«é€‚ç”¨äºŽä¸å¹³è¡¡æ•°æ®é›†çš„è¯„ä»·ã€‚

    * ä¸ºä»€ä¹ˆå–è°ƒå’Œå¹³å‡æ•°ï¼Œè€Œä¸æ˜¯ä»£æ•°å¹³å‡æ•°ï¼Œæˆ–è€…å‡ ä½•å¹³å‡æ•°ï¼Ÿ

        å› ä¸ºè°ƒå’Œå¹³å‡æ•°å¯¹è¾ƒä½Žå€¼æ–½åŠ äº†æ›´ä¸¥åŽ‰çš„æƒ©ç½šã€‚

        ä¸‰ç§å¹³å‡æ•°ï¼š

        å‡è®¾æˆ‘ä»¬æœ‰ Precision (P) å’Œ Recall (R) ä¸¤ä¸ªå€¼ã€‚

        * ç®—æœ¯å¹³å‡æ•°ï¼š(P + R) / 2

            ç‰¹ç‚¹ï¼šå¯¹æ‰€æœ‰å€¼ä¸€è§†åŒä»ï¼Œæ˜¯æ™®é€šçš„â€œå¹³å‡å€¼â€ã€‚

        * å‡ ä½•å¹³å‡æ•°ï¼šsqrt(P * R)

            ç‰¹ç‚¹ï¼šå—æžç«¯å€¼å½±å“è¾ƒå°ï¼Œæ›´é€‚åˆè¡¡é‡æ¯”ä¾‹æˆ–å¢žé•¿çŽ‡ã€‚

        * è°ƒå’Œå¹³å‡æ•°ï¼š2 * P * R / (P + R)

            ç‰¹ç‚¹ï¼šå¼ºçƒˆæƒ©ç½šä¸å¹³è¡¡çš„æ•°å€¼ã€‚å½“På’ŒRä¸­æœ‰ä¸€ä¸ªéžå¸¸ä½Žæ—¶ï¼Œè°ƒå’Œå¹³å‡æ•°ä¼šæŽ¥è¿‘è¿™ä¸ªä½Žå€¼ã€‚

        æˆ‘ä»¬å¸Œæœ›ä¸€ä¸ªæ¨¡åž‹åœ¨Precisionå’ŒRecallä¸Šéƒ½è¡¨çŽ°è‰¯å¥½ï¼Œè€Œä¸æ˜¯ç”¨å…¶ä¸­ä¸€ä¸ªçš„é«˜åˆ†æ¥â€œæŽ©ç›–â€å¦ä¸€ä¸ªçš„ä½Žåˆ†ã€‚

        example:

        åœºæ™¯ï¼šæˆ‘ä»¬æœ‰ä¸€ä¸ªç–¾ç—…æ£€æµ‹æ¨¡åž‹ã€‚

            æ¨¡åž‹Aï¼š Precision = 1.0ï¼Œ Recall = 0.1

                å®ƒé¢„æµ‹æœ‰ç—…çš„äººï¼Œ100%ç¡®å®žæœ‰ç—…ï¼ˆéžå¸¸å‡†ï¼Œç»ä¸è¯¯è¯Šï¼‰ã€‚

                ä½†å®žé™…æœ‰ç—…çš„äººï¼Œå®ƒåªæ‰¾å‡ºäº†10%ï¼ˆæ¼æŽ‰äº†90%çš„ç—…äººï¼Œéžå¸¸å±é™©ï¼‰ã€‚

            æ¨¡åž‹Bï¼š Precision = 0.5ï¼Œ Recall = 0.5

                å®ƒé¢„æµ‹æœ‰ç—…çš„äººï¼Œä¸€åŠç¡®å®žæœ‰ç—…ã€‚

                å®žé™…æœ‰ç—…çš„äººï¼Œå®ƒæ‰¾å‡ºäº†ä¸€åŠã€‚

        é—®é¢˜ï¼šå“ªä¸ªæ¨¡åž‹æ›´å¥½ï¼Ÿ

        è®¡ç®—å®ƒä»¬çš„å¹³å‡æ•°ï¼š

        | æ¨¡åž‹ | A | B |
        | - | - | - |
        |ã€€Precision (P) | 1.0 | 0.5 |
        | Recall (R) | 0.1 | 0.5 |
        | ç®—æœ¯å¹³å‡ | (1.0 + 0.1) / 2 = 0.55 | (0.5+0.5)/2 = 0.50 |
        | å‡ ä½•å¹³å‡ sqrt(1.0 * 0.1) â‰ˆ 0.32 | sqrt(0.5 * 0.5) = 0.50 |
        | è°ƒå’Œå¹³å‡ | (F1) 2(1.0 * 0.1)/(1.0+0.1) â‰ˆ 0.18 | 2(0.5* 0.5)/(0.5+0.5) = 0.50 |

        åˆ†æžç»“æžœï¼š

        * ä»Žç®—æœ¯å¹³å‡æ•°çœ‹ï¼šæ¨¡åž‹A (0.55) > æ¨¡åž‹B (0.50)ã€‚è¿™æ˜¾ç„¶æ˜¯ä¸åˆç†çš„ã€‚æ¨¡åž‹Aæ˜¯ä¸€ä¸ªâ€œæ‡’æƒ°â€çš„æ¨¡åž‹ï¼Œå®ƒä¸ºäº†ä¿æŒ100%çš„å‡†ç¡®çŽ‡ï¼Œåªæ•¢å¯¹æžå°‘æ•°éžå¸¸æœ‰æŠŠæ¡çš„ç—…ä¾‹åšå‡ºé˜³æ€§é¢„æµ‹ï¼Œå¯¼è‡´å¤§é‡ç—…äººè¢«æ¼è¯Šã€‚åœ¨åŒ»å­¦ä¸Šï¼Œè¿™æ˜¯ä¸€ä¸ªç¾éš¾æ€§çš„æ¨¡åž‹ã€‚ç„¶è€Œï¼Œç®—æœ¯å¹³å‡æ•°å´è¢«å®ƒæžé«˜çš„Precisionæ‰€â€œæ¬ºéª—â€ï¼Œç»™å‡ºäº†æ›´é«˜çš„åˆ†æ•°ã€‚

        * ä»Žå‡ ä½•å¹³å‡æ•°çœ‹ï¼šæ¨¡åž‹A (0.32) < æ¨¡åž‹B (0.50)ã€‚è¿™ä¸ªç»“æžœå·²ç»æ¯”ç®—æœ¯å¹³å‡æ•°åˆç†äº†ï¼Œå®ƒè¯†åˆ«å‡ºäº†æ¨¡åž‹Açš„ä¸å¹³è¡¡æ€§ã€‚

        * ä»Žè°ƒå’Œå¹³å‡æ•° (F1-Score) çœ‹ï¼šæ¨¡åž‹A (0.18) << æ¨¡åž‹B (0.50)ã€‚è°ƒå’Œå¹³å‡æ•°å¯¹æ¨¡åž‹Açš„â€œåç§‘â€è¡Œä¸ºæ–½åŠ äº†æœ€ä¸¥åŽ‰çš„æƒ©ç½šï¼Œç»™å‡ºäº†ä¸€ä¸ªæžä½Žçš„åˆ†æ•°ï¼Œæ¸…æ™°åœ°è¡¨æ˜Žæ¨¡åž‹Bçš„ç»¼åˆæ€§èƒ½è¿œä¼˜äºŽæ¨¡åž‹Aã€‚

        ç»“è®ºä¸Žæ€»ç»“:

        å¹³å‡æ•°ç±»åž‹	å¯¹ä¸å¹³è¡¡çš„æƒ©ç½šåŠ›åº¦	åœ¨è¯„ä¼°æ¨¡åž‹ä¸­çš„é€‚ç”¨æ€§
        ç®—æœ¯å¹³å‡	æœ€å¼±	ä¸é€‚ç”¨ã€‚å®¹æ˜“è¢«ä¸€ä¸ªé«˜æŒ‡æ ‡å’Œå¦ä¸€ä¸ªä½ŽæŒ‡æ ‡çš„æ¨¡åž‹æ‰€è¯¯å¯¼ã€‚
        å‡ ä½•å¹³å‡	ä¸­ç­‰	æ¯”ç®—æœ¯å¹³å‡å¥½ï¼Œåœ¨æŸäº›åœºæ™¯ä¸‹ï¼ˆå¦‚FÎ²-Scoreçš„å˜ä½“ï¼‰æœ‰åº”ç”¨ã€‚
        è°ƒå’Œå¹³å‡ (F1)	æœ€å¼º	æœ€å¸¸ç”¨ã€‚èƒ½æœ‰æ•ˆæƒ©ç½šâ€œåç§‘â€çš„æ¨¡åž‹ï¼Œç¡®ä¿æ¨¡åž‹åœ¨På’ŒRä¹‹é—´å–å¾—æœ‰æ„ä¹‰çš„å¹³è¡¡ã€‚

    F1-Scoreç‰¹åˆ«é€‚åˆäºŽç±»åˆ«ä¸å¹³è¡¡çš„æ•°æ®é›†ï¼Œä»¥åŠé‚£äº›æ²¡æœ‰æ˜Žç¡®å€¾å‘æ˜¯æ›´éœ€è¦Precisionè¿˜æ˜¯Recallçš„åœºæ™¯ï¼Œå®ƒæä¾›äº†ä¸€ä¸ªç¨³å¥çš„ã€å•ä¸€çš„ç»¼åˆæ€§è¯„ä¼°æŒ‡æ ‡ã€‚

    å½“ä½ æœ‰æ˜Žç¡®å€¾å‘æ—¶ï¼Œå¯ä»¥ä½¿ç”¨FÎ²-Scoreã€‚

    FÎ² = (1 + Î²Â²) * (Precision * Recall) / (Î²Â² * Precision + Recall)

    å½“Î²=1æ—¶ï¼Œå°±æ˜¯F1ã€‚

    å½“Î²>1æ—¶ï¼ŒRecallçš„æƒé‡æ›´é«˜ï¼ˆæ›´çœ‹é‡æŸ¥å…¨ï¼‰ã€‚

    å½“Î²<1æ—¶ï¼ŒPrecisionçš„æƒé‡æ›´é«˜ï¼ˆæ›´çœ‹é‡æŸ¥å‡†ï¼‰ã€‚

* æ··æ·†çŸ©é˜µï¼ˆConfusion Matrixï¼‰

    æœ‰æ—¶ä¹Ÿè¢«ç§°ä¸º Error Matrixï¼ˆé”™è¯¯çŸ©é˜µï¼‰ï¼Œå®ƒæ˜¯ä¸€ä¸ª2x2çš„è¡¨æ ¼ï¼Œæ€»ç»“äº†åˆ†ç±»æ¨¡åž‹å¯¹äºŒåˆ†ç±»é—®é¢˜çš„é¢„æµ‹ç»“æžœã€‚


    | | å®žé™…ä¸ºæ­£ä¾‹ | å®žé™…ä¸ºè´Ÿä¾‹ |
    | - | :-: | :-: |
    | é¢„æµ‹ä¸ºæ­£ä¾‹ | TP (True Positive) | FP (False Positive) |
    | é¢„æµ‹ä¸ºè´Ÿä¾‹ | FN (False Negative) | TN (True Negative) |

    * TPï¼ˆçœŸé˜³æ€§ï¼‰ï¼šæ¨¡åž‹é¢„æµ‹ä¸ºæ­£ï¼Œå®žé™…ä¹Ÿæ˜¯æ­£ã€‚é¢„æµ‹æ­£ç¡®ã€‚

    * FPï¼ˆå‡é˜³æ€§ï¼‰ï¼šæ¨¡åž‹é¢„æµ‹ä¸ºæ­£ï¼Œä½†å®žé™…æ˜¯è´Ÿã€‚è¯¯æŠ¥ã€‚

    * FNï¼ˆå‡é˜´æ€§ï¼‰ï¼šæ¨¡åž‹é¢„æµ‹ä¸ºè´Ÿï¼Œä½†å®žé™…æ˜¯æ­£ã€‚æ¼æŠ¥ã€‚

    * TNï¼ˆçœŸé˜´æ€§ï¼‰ï¼šæ¨¡åž‹é¢„æµ‹ä¸ºè´Ÿï¼Œå®žé™…ä¹Ÿæ˜¯è´Ÿã€‚é¢„æµ‹æ­£ç¡®ã€‚

* torchmetrics

    install: `pip install torchmetrics`

    ```py
    import torch
    from torchmetrics import Accuracy, Precision

    # accuracy
    accuracy = Accuracy(task="multiclass", num_classes=10)
    accuracy.reset()

    batch1_preds = torch.tensor([0, 1, 2, 3]) # æ¨¡åž‹é¢„æµ‹çš„ç±»åˆ«ç´¢å¼•
    batch1_target = torch.tensor([0, 1, 1, 3]) # çœŸå®žçš„ç±»åˆ«ç´¢å¼•

    batch2_preds = torch.tensor([1, 0, 2])
    batch2_target = torch.tensor([1, 0, 1])

    accuracy.update(batch1_preds, batch1_target)
    accuracy.update(batch2_preds, batch2_target)

    final_accuracy = accuracy.compute()
    print(f"æœ€ç»ˆå‡†ç¡®çŽ‡: {final_accuracy}") # ä¾‹å¦‚ï¼štensor(0.7143)


    # precision
    pre = Precision('multiclass', num_classes=10, average='macro')
    pre.reset()
    pre.update(batch1_preds, batch1_target)
    pre.update(batch2_preds, batch2_target)
    final_pre = pre.compute()
    print('final pre: {}'.format(final_pre))
    ```

    output:

    ```
    æœ€ç»ˆå‡†ç¡®çŽ‡: 0.7142857313156128
    final pre: 0.75
    ```

    æ³¨ï¼š

    1. å¦‚æžœ precision çš„ average è®¾ç½®ä¸º`micro`ï¼Œé‚£ä¹ˆæœ€åŽå¾—åˆ°çš„ç»“æžœå’Œ accuracy ç›¸åŒã€‚
